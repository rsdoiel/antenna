---
title: libraries
updated: 2025-01-10 07:05:40
---

# libraries

(date: 2025-01-10 07:05:40)

---

## Good Good Boy Alert: The Dogs of 2024

date: 2025-01-10, from: Scholarly Kitchen

<p>Before the chaos of 2025 really kicks in, luxuriate in the dogs of 2024.</p>
<p>The post <a href="https://scholarlykitchen.sspnet.org/2025/01/10/good-good-boy-alert-the-dogs-of-2024/">Good Good Boy Alert: The Dogs of 2024</a> appeared first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.</p>
 

<br> 

<https://scholarlykitchen.sspnet.org/2025/01/10/good-good-boy-alert-the-dogs-of-2024/>

---

## Point Counter Point, by Aldous Huxley

date: 2025-01-09, from: Standard Ebooks, new releaases

Several connected stories are interwoven to create a satirical look at English society. 

<br> 

<https://standardebooks.org/ebooks/aldous-huxley/point-counter-point>

---

## Metadata Matching: Beyond Correctness

date: 2025-01-09, from: ROR Research ID Blog

The fifth and final blog post about metadata matching by ROR’s Adam Buttrick and Crossref’s Dominika Tkaczyk outlines a set of pragmatic criteria for making decisions about metadata matching. 

<br> 

<https://ror.org/blog/2025-01-09-metadata-matching-beyond-correctness/>

---

## Insights from Arizona State University: Advancing Research Support Through Open Science, Data Sovereignty, and Machine-Actionable Plans

date: 2025-01-09, from: Association of Research Libraries News

<p>Last Updated on January 9, 2025, 9:04 am ET On the final site visit for the Machine Actionable Plans (MAP) Pilot project, the Association of Research Libraries (ARL) and the...</p>
<p>The post <a href="https://www.arl.org/blog/insights-from-arizona-state-university-advancing-research-support-through-open-science-data-sovereignty-and-machine-actionable-plans/">Insights from Arizona State University: Advancing Research Support Through Open Science, Data Sovereignty, and Machine-Actionable Plans</a> appeared first on <a href="https://www.arl.org">Association of Research Libraries</a>.</p>
 

<br> 

<https://www.arl.org/blog/insights-from-arizona-state-university-advancing-research-support-through-open-science-data-sovereignty-and-machine-actionable-plans/>

---

## AI and Content — The 2024 Trend that Wasn’t and the Related Opportunity that Exists

date: 2025-01-09, from: Scholarly Kitchen

<p>As a result of EU law and other factors, rights holders are reserving their AI rights. This material is available for AI training/licensing.</p>
<p>The post <a href="https://scholarlykitchen.sspnet.org/2025/01/09/ai-and-content-the-2024-trend-that-wasnt-and-the-related-opportunity-that-exists/">AI and Content &#8212; The 2024 Trend that Wasn’t and the Related Opportunity that Exists</a> appeared first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.</p>
 

<br> 

<https://scholarlykitchen.sspnet.org/2025/01/09/ai-and-content-the-2024-trend-that-wasnt-and-the-related-opportunity-that-exists/>

---

## Antigone, by Sophocles

date: 2025-01-08, from: Standard Ebooks, new releaases

A young woman defies the king’s decree to honor her brother with a proper burial. 

<br> 

<https://standardebooks.org/ebooks/sophocles/antigone/francis-storr>

---

## UC Riverside Machine Actionable Plans (MAP) Team Builds Promising Communication Prototype

date: 2025-01-08, from: Association of Research Libraries News

<p>Last Updated on January 9, 2025, 8:57 am ET The Machine Actionable Plans (MAP) Pilot project team from the Association of Research Libraries (ARL) and the California Digital Library (CDL)...</p>
<p>The post <a href="https://www.arl.org/blog/uc-riverside-machine-actionable-plans-map-team-builds-promising-communication-prototype/">UC Riverside Machine Actionable Plans (MAP) Team Builds Promising Communication Prototype</a> appeared first on <a href="https://www.arl.org">Association of Research Libraries</a>.</p>
 

<br> 

<https://www.arl.org/blog/uc-riverside-machine-actionable-plans-map-team-builds-promising-communication-prototype/>

---

## We’re Seeking A Deputy Editor for The Scholarly Kitchen

date: 2025-01-08, from: Scholarly Kitchen

<p>Come join us in The Scholarly Kitchen! We are seeking a Deputy Editor.</p>
<p>The post <a href="https://scholarlykitchen.sspnet.org/2025/01/08/were-seeking-a-deputy-editor-for-the-scholarly-kitchen/">We&#8217;re Seeking A Deputy Editor for The Scholarly Kitchen</a> appeared first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.</p>
 

<br> 

<https://scholarlykitchen.sspnet.org/2025/01/08/were-seeking-a-deputy-editor-for-the-scholarly-kitchen/>

---

## Enroll in LIB 296A, The Information Ecosystem

date: 2025-01-08, from: CSUN Library Blog, Cited

<div><img width="300" height="200" src="https://library.csun.edu/blogs/cited/wp-content/uploads/sites/4/2025/01/LIB296A-900-x-600-px-300x200.png" class="attachment-medium size-medium wp-post-image" alt="" style="margin-bottom: 15px;" decoding="async" fetchpriority="high" srcset="https://library.csun.edu/blogs/cited/wp-content/uploads/sites/4/2025/01/LIB296A-900-x-600-px-300x200.png 300w, https://library.csun.edu/blogs/cited/wp-content/uploads/sites/4/2025/01/LIB296A-900-x-600-px-768x512.png 768w, https://library.csun.edu/blogs/cited/wp-content/uploads/sites/4/2025/01/LIB296A-900-x-600-px.png 900w" sizes="(max-width: 300px) 100vw, 300px" /></div>Are you interested in learning how information is created and consumed? Do you have questions about fake new or misinformation? Do you need a 1-unit,&#8230; 

<br> 

<https://library.csun.edu/blogs/cited/2025/01/08/enroll-in-lib-296a/>

---

## Metadata matching: beyond correctness

date: 2025-01-08, from: Crossref Blog

<p><img src="https://www.crossref.org/img/crossref-logo-icon-only.svg" alt="Crossref logo icon" height="15" style="display: inline;" /> <a href="https://doi.org/10.13003/axeer1ee" target="_blank">https://doi.org/10.13003/axeer1ee</a></p>
<p>In our <a href="https://www.crossref.org/blog/how-good-is-your-matching">previous entry</a>, we explained that thorough evaluation is key to understanding a matching strategy&rsquo;s performance. While evaluation is what allows us to assess the correctness of matching, choosing the best matching strategy is, unfortunately, not as simple as selecting the one that yields the best matches. Instead, these decisions usually depend on weighing multiple factors based on your particular circumstances. This is true not only for metadata matching, but for <a href="https://www.wired.com/2012/04/netflix-prize-costs/" target="_blank">many technical choices</a> that require navigating trade-offs. In this blog post, the last one in the metadata matching series, we outline a subjective set of criteria we would recommend you consider when making decisions about matching.</p>
<h2 id="openness">Openness</h2>
<p>Matching tools come in many different shapes and sizes: web applications, APIs, command-line tools, sometimes even <a href="https://adambuttrick.github.io/mysterious-crystal-ball-matching/" target="_blank">enchanted crystal balls showing matched identifiers emerging from a mysterious mist</a>! No matter what form they take, an important consideration is whether the source code and all the related resources for the matching are openly available.</p>
<p>Matching strategies that are either closed-source, or rely on closed-source services for their matching logic, make it difficult to fully understand and explain matching processes. This lack of transparency also makes it impossible to adjust or improve the matching logic, since we cannot understand or improve code we cannot see.</p>
<p>Users are similarly impeded from identifying flaws or suggesting improvements to processes they are unable to examine. By blocking this community participation, we also lose the proven cycle of real-world testing, refinement, and validation that has strengthened myriad of open source projects. The cumulative impact of both minor and major community-driven refinements over time is incredibly valuable and should not be underestimated.</p>
<p>Using open source matching will also help build trust in the matching workflows and results. This is one reason why open source is one of the tenets of the <a href="https://openscholarlyinfrastructure.org" target="_blank">Principles of Open Scholarly Infrastructure</a>, adopted by Crossref, DataCite, ROR, and other organizations who build and maintain open scholarly infrastructure.</p>
<p>When evaluating matching strategies, we strongly recommend prioritizing those that are fully open source. This not only ensures their transparency and trustworthiness, but also allows for the kind of continuous improvement that results from this visibility and community engagement.</p>
<h2 id="explainability">Explainability</h2>
<p>In terms of our ability to understand and improve a matching strategy, using an open source model is only the first step. What typically matters most in the context of building and maintaining matching services is that we are able to understand their underlying code and have a clear model of how matches are derived from their corresponding inputs. Even if the matching code itself and all of the resources used in the matching are open, if they are poorly documented, lack reproducibility or tests, or are otherwise opaque, there is no guarantee that it will be possible to understand or improve the strategy. Striving for a high level of interpretability in our matching plays a determinative role in how well we can understand and modify our strategies in the future.</p>
<p>Being able to explain the behaviour of the matching will also help you to respond to and incorporate user feedback. When users encounter errors, you will be able to do things like advise them on how to modify or clean their inputs so that the results are better. Conversely, examining the behaviour of the strategy relative to user inputs and feedback can provide you with ideas for improving the matching.</p>
<p>Typically, heuristic-based strategies, such as those that use forms of search or string similarity measures, like <a href="https://en.wikipedia.org/wiki/Edit_distance" target="_blank">edit distance</a>, are easier to explain than, say, machine learning models. If a strategy uses machine learning, at least some internal decisions might be made by passing data through a complex network of algebraic equations. Those can be mysterious, non-deterministic, and are famous for being <a href="https://xkcd.com/1838/" target="_blank">hard to interpret</a>. This doesn&rsquo;t mean they should be avoided entirely - we have built and use many machine-learning based tools ourselves! Instead, it is a good idea to weigh how their inherent lack of explainability could affect your ability to continue work on the strategy and respond to user needs, relative to all the available options.</p>
<h2 id="complexity">Complexity</h2>
<p>Complexity is another aspect that can greatly affect how easy it is to maintain the strategy. Complexity is related to how many different components the strategy has and how difficult they are to use and maintain. When a strategy has multiple interconnected parts, each component becomes a potential failure point that requires discrete assessment and maintenance.</p>
<p>Consider, for example, two different approaches to a matching strategy: one that uses a single machine learning model versus another that uses an ensemble of models. A single model requires maintaining one set of training data, a single training pipeline, and one deployment process. If the model&rsquo;s performance unexpectedly deteriorates, whether because of an issue with the training data, a configuration error, or the need for additional input sanitization, the source of the problem is easier to isolate and fix.</p>
<p>The ensemble, by contrast, combines multiple, specialized models, each requiring its own training data, tests, updates, and deployments. If one model in the ensemble is found to reduce the performance of the strategy, the interdependence between models can cause this degradation to cascade through the entire system and undermine its overall reliability. Correcting for these errors becomes more challenging. If fixing one model&rsquo;s performance requires retraining or adjusting its outputs, this could require recalibrating the entire ensemble to maintain the balance between models, identify regressions, and prevent new errors from emerging.</p>
<p>In general, preferring simpler strategies not only reduces operational overhead, but also makes it easier to diagnose issues, test changes, and iterate on user feedback. When problems arise, having fewer moving parts means less places to look for the root cause and fewer components that could be affected by any fixes.</p>
<h2 id="flexibility">Flexibility</h2>
<p>The metadata to which we match grows and changes over time. New records are created, existing ones are updated, with schemas changing and evolving alongside. The resources that underlie our matching are also not static. The libraries we depend on may deprecate features between versions or the taxonomies we used to categorize results might undergo significant revisions. We thus rarely have the luxury of deploying a matching strategy once and using it forever without any changes. A good strategy has to be flexible enough to adapt to such changes, with this adaptation also being both technically feasible and practical to implement.</p>
<p>Much of this flexibility is also determined by a matching strategy&rsquo;s ability to incorporate new data. Strategies that use continuously updated databases or indices can immediately match against new metadata as it appears in the system. By contrast, some machine learning-based approaches require training on target matches and can thus be limited in flexibility and face more constraints. While some models can be incrementally updated to recognize new matches, others require retraining from scratch to incorporate these changes - a process that can be both time-consuming and resource-intensive.</p>
<p>Paying close attention to a strategy&rsquo;s flexibility and favoring this aspect, when possible, can significantly impact its long-term viability. When comparing different matching strategies, flexibility should thus be a primary concern in your decision-making process.</p>
<h2 id="resources">Resources</h2>
<p>Matching strategies can vary significantly in their resource requirements, including things like CPU and GPU utilization, memory consumption, storage capacity, and network bandwidth. These requirements are directly related to infrastructure costs and energy consumption, so when evaluating a matching strategy, it is necessary to assess its resource demands across all phases of the matching lifecycle. This includes things like initial model training, re-training, index construction, updates and management for all aspects of the strategy, as well as the real-world processing of matching requests. It is a good idea to measure and monitor resource usage carefully in considering which strategies to use, as the best performing strategy may also be too resource intensive to run as a service or might grow to this state over time with additional utilization.</p>
<h2 id="speed">Speed</h2>
<p>Matching strategies can operate at a wide range of speeds, from milliseconds to minutes per match. Since the overall response time of a strategy can affect both system scalability and user experience, we should always assess the strategy&rsquo;s performance for different usage scenarios and scales of data. While some strategies might perform adequately with small datasets, they can also exhibit exponential slowdowns as data volume and complexity increases or as concurrent requests grow in number. We should therefore consider carefully how requirements for matching speed might evolve with increased usage, data complexity, and total anticipated growth. The fastest matching strategy might not always be the best choice if it comes at the cost of reduced accuracy or requires large amounts of resources, but unacceptable latency can make an otherwise excellent strategy unusable in practice for many use cases.</p>
<h2 id="putting-it-all-together">Putting it all together</h2>
<p>The typical life cycle of developing a metadata matching strategy is as follows:</p>
<ol>
<li><strong>Scoping</strong>: we define the matching task, along with its inputs and outputs.</li>
<li><strong>Research</strong>: we research what existing strategies are available for our task and/or we develop our own.</li>
<li><strong>Evaluation</strong>: we evaluate all available strategies, internally or externally-developed, exploring all of the aspects described above.</li>
<li><strong>Decision</strong>: we choose which strategy (if any) we want to use in our production system.</li>
<li><strong>Production setup</strong>: we prepare the production models, indexes, and other resources needed for the matching.</li>
<li><strong>Maintenance</strong>: we monitor and adapt the strategy relative to changing data, user feedback, and new resource requirements.</li>
</ol>
<p>In practice, these phases do not happen all at once, nor in this strict order. Often we need to proceed through multiple iterations of them to arrive at the best strategy. For example, if initial evaluation of a strategy yields poor results, we might return to the research phase to investigate other strategies or refine our understanding of the task. Often, during the maintenance phase, we receive feedback from users that indicates potential areas of improvement and then pursue them with a new round of research and evaluation.</p>
<p>As we cycle through these phases, ideally all the aspects described in this entry, along with the results of the evaluation, would be taken into account. Of course, this means that these decisions have to be based on multiple criteria and by making trade-offs between their performance and all other considerations. In making these complex and difficult choices, it is useful to consider two primary questions:</p>
<ol>
<li>Are any of the considered matching strategies good enough for our use case?</li>
<li>Out of all the considered strategies that are sufficient for our use case, which would be the best?</li>
</ol>
<p>The first question requires us to create clear and quantifiable criteria that allow for eliminating some of the potential strategies. As we have indicated, these could include things like the strategy being open source, minimum performance baselines using measures like precision or recall, and operational thresholds, like the strategy being able to return results quickly, relative to user expectations or the volume of data to be processed. It should be fairly easy to test these requirements and eliminate any strategies that fall short of them. If the strategies are difficult to assess, that is likely a mark against them.</p>
<p>If no strategies meet these criteria, we have two options: either to abandon matching entirely or to reassess and relax our criteria to align with the available options. While the former is always an option, adopting a more pragmatic lens, framing in terms of potential value (or harm) to the users, might be beneficial. Sometimes we approach matching tasks with too high expectations and a dose of realism helps us to re-center our perspectives. After more consideration, you might decide that your criteria were too stringent or realize that you need to better define and decompose the tasks to fit the available options.</p>
<p>When multiple strategies appear viable, the selection process becomes more nuanced. When evaluating strategies across these various dimensions, we should try to avoid placing undue weight on minor performance differences. Evaluation metrics are useful estimates of performance, but do not always translate to real-world applications and changing data. In cases where a more complex strategy offers only marginal improvements over a simpler alternative, the maintenance and operational benefits of the simpler solution often outweigh small performance gains.</p>
<p>This concludes our series on metadata matching, where we described the conceptual, product, and technical aspects of matching and its applications. We hope this overview was instructive and helps you to make better decisions about the use of matching in your own tools and services!</p> 

<br> 

<https://www.crossref.org/blog/metadata-matching-beyond-correctness/>

---

## Jungle Tales of Tarzan, by Edgar Rice Burroughs

date: 2025-01-07, from: Standard Ebooks, new releaases

A collection of short stories about Tarzan growing up in the jungle. 

<br> 

<https://standardebooks.org/ebooks/edgar-rice-burroughs/jungle-tales-of-tarzan>

---

## Oedipus at Colonus, by Sophocles

date: 2025-01-07, from: Standard Ebooks, new releaases

A prophecy is fulfilled when an exiled king dies, providing protection to Athens. 

<br> 

<https://standardebooks.org/ebooks/sophocles/oedipus-at-colonus/francis-storr>

---

## Victory, by Joseph Conrad

date: 2025-01-07, from: Standard Ebooks, new releaases

A reclusive man rescues a mistreated young woman, but their romantic life on a remote island is soon disturbed. 

<br> 

<https://standardebooks.org/ebooks/joseph-conrad/victory>

---

## Guest Post — College Mergers and the Implications for Libraries and Vendors

date: 2025-01-07, from: Scholarly Kitchen

<p>While mergers can save struggling institutions and foster stronger student experiences in the long run, they are complex and their implications for scholarly content and services must be considered thoughtfully.</p>
<p>The post <a href="https://scholarlykitchen.sspnet.org/2025/01/07/guest-post-college-mergers-and-the-implications-for-libraries-and-vendors/">Guest Post &#8212; College Mergers and the Implications for Libraries and Vendors</a> appeared first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.</p>
 

<br> 

<https://scholarlykitchen.sspnet.org/2025/01/07/guest-post-college-mergers-and-the-implications-for-libraries-and-vendors/>

---

## The Well at the World’s End, by William Morris

date: 2025-01-06, from: Standard Ebooks, new releaases

A king’s son embarks on an adventure to find a legendary well that grants long life. 

<br> 

<https://standardebooks.org/ebooks/william-morris/the-well-at-the-worlds-end>

---

## Day in Review (January 6–8)

date: 2025-01-06, from: Association of Research Libraries News

<p>Last Updated on January 9, 2025, 7:48 am ET Sign up to receive the Day in Review by email. Jump to: Tuesday, January 7 &#124; Wednesday, January 8 &#124; Note:...</p>
<p>The post <a href="https://www.arl.org/day-in-review/day-in-review-january-6-9/">Day in Review (January 6–8)</a> appeared first on <a href="https://www.arl.org">Association of Research Libraries</a>.</p>
 

<br> 

<https://www.arl.org/day-in-review/day-in-review-january-6-9/>

---

## The Year in Review: 2024 in The Scholarly Kitchen

date: 2025-01-06, from: Scholarly Kitchen

<p>Before we plunge into 2025, a look back at 2024, a year of uncertainty in The Scholarly Kitchen.</p>
<p>The post <a href="https://scholarlykitchen.sspnet.org/2025/01/06/the-year-in-review-2024-in-the-scholarly-kitchen-2/">The Year in Review: 2024 in The Scholarly Kitchen</a> appeared first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.</p>
 

<br> 

<https://scholarlykitchen.sspnet.org/2025/01/06/the-year-in-review-2024-in-the-scholarly-kitchen-2/>

---

## Hester, by Margaret Oliphant

date: 2025-01-05, from: Standard Ebooks, new releaases

The daughter of a failed banker is drawn into conflict with her father’s cousin who saved the bank. 

<br> 

<https://standardebooks.org/ebooks/margaret-oliphant/hester>

---

## Trace metal evolution of the Late Cretaceous Ocean

date: 2025-01-05, from: ETH Zurich, recently added

Sun, Mingzhao; Archer, Corey; Scholz, Florian; Sweere, Tim; Vance, Derek 

<br> 

<http://hdl.handle.net/20.500.11850/705683>

