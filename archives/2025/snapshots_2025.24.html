<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8" >
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" >
  <title>snapshots</title>
<!--  <link rel="stylesheet" type="text/css"  href="../webfonts/fonts.css" media="screen" > -->
  <link rel="stylesheet" type="text/css"  href="../css/site.css" media="screen" >
</head>
<body>
<header>
	<img class="logo" 
	src="https://upload.wikimedia.org/wikipedia/commons/9/9c/Antenna_1_-_The_Noun_Project.svg"
	alt="line art showing an antenna"
	height="80" width="60" >
	<h1>The Antenna</h1> 
	<h2>finding signal in the noise</h2>
</header>
<nav>
<ul>
	<li><a href="../../">The Antenna</a></li>
	<li><a href="../">Archives</a></li>
	<li><a href="../../about.html">About</a></li>
</ul>
</nav>
<section>
<div class="description-for-items">
<h2>snapshots</h2>
An experiment in personal news aggregation.
</div>
<h1 id="snapshots">snapshots</h1>
<p>(date: 2025-06-16 06:08:33)</p>
<hr />
<h2
id="blog-carnival-23-editors-outro-digital-circulation-in-rhetoric-and-writing-studies">Blog
Carnival 23: Editor’s Outro: “Digital Circulation in Rhetoric and
Writing Studies</h2>
<p>date: 2025-06-16, from: Digital Humanities Quarterly News</p>
Bathroom graffiti. Podcasts. Skibibi brain rot. Social media activism.
Deepfakes. Collages. J.D. Vance Photoshop memes. In this blog carnival,
the contributing authors used these ideas to explore the role of
circulation in rhetoric and writing studies.  Some authors used the
framework of circulation to explore how specific artifacts or ideas
circulate through different systems. For […]
<p>
<a href="https://www.digitalrhetoriccollaborative.org/2025/06/16/blog-carnival-23-editors-outro-digital-circulation-in-rhetoric-and-writing-studies/" rel="nofollow">Source</a>
</p>
<p><br></p>
<p><a
href="https://www.digitalrhetoriccollaborative.org/2025/06/16/blog-carnival-23-editors-outro-digital-circulation-in-rhetoric-and-writing-studies/"
class="uri">https://www.digitalrhetoriccollaborative.org/2025/06/16/blog-carnival-23-editors-outro-digital-circulation-in-rhetoric-and-writing-studies/</a></p>
<hr />
<h2
id="meta-users-feel-less-safe-since-it-weakened-hateful-conduct-policy-survey-finds">Meta
Users Feel Less Safe Since It Weakened ‘Hateful Conduct’ Policy, Survey
Finds</h2>
<p>date: 2025-06-16, from: 404 Media Group</p>
<p>A survey of 7,000 active users on Instagram, Facebook and Threads
shows people feel grossed out and unsafe since Mark Zuckerberg’s
decision to scale back moderation after Trump’s election.</p>
<p><br></p>
<p><a
href="https://www.404media.co/meta-users-feel-less-safe-since-it-weakened-hateful-conduct-policy-survey-finds/"
class="uri">https://www.404media.co/meta-users-feel-less-safe-since-it-weakened-hateful-conduct-policy-survey-finds/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-16, from: Dave Winer’s linkblog)</p>
<p>Removing documents and restoring monuments won't change America's
history.</p>
<p><br></p>
<p><a
href="https://missouriindependent.com/2025/06/16/removing-documents-and-restoring-monuments-wont-change-americas-history/"
class="uri">https://missouriindependent.com/2025/06/16/removing-documents-and-restoring-monuments-wont-change-americas-history/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-16, from: Dave Winer’s linkblog)</p>
<p>The NYT is so scared of itself it can’t say what’s obvious, Cuomo
should be the next mayor of NYC.</p>
<p><br></p>
<p><a
href="https://www.nytimes.com/2025/06/16/opinion/new-york-mayor-election-advice.html?smid=nytcore-ios-share&amp;referringSource=articleShare"
class="uri">https://www.nytimes.com/2025/06/16/opinion/new-york-mayor-election-advice.html?smid=nytcore-ios-share&amp;referringSource=articleShare</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-16, from: Dave Winer’s linkblog)</p>
<p>Scripting News: Democratic resurrection plan. The first step is to
ignore CNN, the NYT (esp Ezra Klein), MSNBC, Washington Post et al. When
they’re complaining, you’re on the right track. They need to be utterly
disempowered. We can’t win so stop trying.</p>
<p><br></p>
<p><a href="http://scripting.com/2025/06/13/162709.html"
class="uri">http://scripting.com/2025/06/13/162709.html</a></p>
<hr />
<h2 id="trumps-parade-flopped.-no-kings-day-was-a-hit.">Trump’s parade
flopped. No Kings Day was a hit.</h2>
<p>date: 2025-06-16, from: Paul Krugman</p>
<p>Right now, images largely determine the outcome</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/trumps-parade-flopped-no-kings-day"
class="uri">https://paulkrugman.substack.com/p/trumps-parade-flopped-no-kings-day</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-16, from: Dave Winer’s linkblog)</p>
<p>Some free commonsense advice for Democrats about winning back men.
(Exactly right. Men are people, and we vote.)</p>
<p><br></p>
<p><a
href="https://thehill.com/opinion/campaign/5350036-some-free-commonsense-advice-for-democrats-about-winning-back-men/"
class="uri">https://thehill.com/opinion/campaign/5350036-some-free-commonsense-advice-for-democrats-about-winning-back-men/</a></p>
<hr />
<h2 id="ccc-fordert-von-dobrindt-klare-kante-gegen-chatkontrolle">CCC
fordert von Dobrindt klare Kante gegen Chatkontrolle</h2>
<p>date: 2025-06-16, updated: 2025-06-16, from: Chaos Computer Club
Updates</p>
<p>Ein zivilgesellschaftliches Bündnis fordert die Bundesregierung auf,
sich ihren Vorgängern anzuschließen und der auf EU-Ebene geplanten
anlasslosen Chatkontrolle ein klares Nein entgegenzusetzen. Die
absehbaren Gefahren, die von dem Vorhaben ausgehen, übersteigen den
ohnehin fragwürdigen Nutzen bei weitem.</p>
<p><br></p>
<p><a
href="https://www.ccc.de/de/updates/2025/ccc-fordert-von-dobrindt-klare-kante-gegen-chatkontrolle"
class="uri">https://www.ccc.de/de/updates/2025/ccc-fordert-von-dobrindt-klare-kante-gegen-chatkontrolle</a></p>
<hr />
<h2 id="episode-159---the-intel-286-a-legacy-trap">Episode 159 - The
Intel 286: A Legacy Trap</h2>
<p>date: 2025-06-15, from: Advent of Computing</p>
<p>
In 1982 Intel released the iAPX 286. It’s was the first heir to the
smash-hit 8086. But the 286 was developed before the IBM PC put an Intel
chip on every desk. It’s design isn’t influence by the PC. Rather, it
reaches further into the past. Today we are looking at the strange
melding of old technology, new ideas, and compatibility that lead to the
286.
</p>
<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://traffic.libsyn.com/secure/adventofcomputing/ep159_286.mp3?dest-id=1206722">
</source>
</audio>
<p><a href="https://traffic.libsyn.com/secure/adventofcomputing/ep159_286.mp3?dest-id=1206722" target="_blank">download
audio/mpeg</a><br></p>
<p><a
href="https://adventofcomputing.libsyn.com/episode-159-the-intel-286-a-legacy-trap"
class="uri">https://adventofcomputing.libsyn.com/episode-159-the-intel-286-a-legacy-trap</a></p>
<hr />
<h2 id="the-medici-curse-of-the-mad-monk-part-3">574. The Medici: Curse
of the Mad Monk (Part 3)</h2>
<p>date: 2025-06-15, from: This is history podcast</p>
<p>
Did Lorenzo de’Medici’s rule in Florence incur prosperity, or was it a
corrupt and autocratic regime, rife with torture, that would spell the
doom of the former Republic? While building an edifice of power, wealth
and luxury, how was he secretly bankrupting his famous family and city?
Was he really the perfect Renaissance Prince, and […]
</p>
<p>
The post
<a href="https://therestishistory.com/574-the-medici-curse-of-the-mad-monk-part-3/">574.
The Medici: Curse of the Mad Monk (Part 3)</a> appeared first on
<a href="https://therestishistory.com">The Rest is History</a>.
</p>
<p><br></p>
<p><a
href="https://therestishistory.com/574-the-medici-curse-of-the-mad-monk-part-3/"
class="uri">https://therestishistory.com/574-the-medici-curse-of-the-mad-monk-part-3/</a></p>
<hr />
<h2 id="quoting-joshua-barretto">Quoting Joshua Barretto</h2>
<p>date: 2025-06-15, updated: 2025-06-15, from: Simon Willison’s
Weblog</p>
<blockquote cite="https://www.jsbarretto.com/blog/software-is-joy/">
<p>
I am a huge fan of Richard Feyman’s famous quote:
</p>
<p>
<strong>“What I cannot create, I do not understand”</strong>
</p>
<p>
I think it’s brilliant, and it remains true across many fields (if
you’re willing to be a little creative with the definition of ‘create’).
It is to this principle that I believe I owe everything I’m truly good
at. Some will tell you should avoid reinventing the wheel, but they’re
wrong: you <em>should</em> build your own wheel, because it’ll teach you
more about how they work than reading a thousand books on them ever
will.
</p>
</blockquote>
<p class="cite">
— <a href="https://www.jsbarretto.com/blog/software-is-joy/">Joshua
Barretto</a>, Writing Toy Software is a Joy
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/careers&quot;&gt;careers&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/programming&quot;&gt;programming&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Jun/15/joshua-barretto/#atom-everything"
class="uri">https://simonwillison.net/2025/Jun/15/joshua-barretto/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-15, from: Dave Winer’s linkblog)</p>
<p>Do parents prefer sons over daughters? Not so much anymore.</p>
<p><br></p>
<p><a
href="https://www.vox.com/future-perfect/416809/sexism-girl-preference-sex-ratios-discrimination-ivf"
class="uri">https://www.vox.com/future-perfect/416809/sexism-girl-preference-sex-ratios-discrimination-ivf</a></p>
<hr />
<h2 id="ayaneo-ag01-starship-graphics-dock-review">AYANEO AG01 Starship
Graphics Dock Review</h2>
<p>date: 2025-06-15, from: Liliputing</p>
<p>
The AYANEO AG01 Starship is an external graphics dock that brings a
discrete AMD Radeon RX 7600M XT mobile GPU to mini, handheld, laptop or
desktop computers. With both USB4 and OCuLink connectors, it’s a
versatile external GPU (eGPU) that should work with a wide range of
products. It also has a distinctive design that AYANEO says […]
</p>
<p>
The post
<a href="https://liliputing.com/ayaneo-ag01-starship-graphics-dock-review/">AYANEO
AG01 Starship Graphics Dock Review</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/ayaneo-ag01-starship-graphics-dock-review/"
class="uri">https://liliputing.com/ayaneo-ag01-starship-graphics-dock-review/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-15, from: Dave Winer’s linkblog)</p>
<p>The shootings have deeply unnerved members of Congress, who feel that
any one of them could be the subject of an unanticipated attack —
particularly at home in their districts and while in transit.</p>
<p><br></p>
<p><a
href="https://www.axios.com/2025/06/15/hortman-hoffman-congress-security-minnesota"
class="uri">https://www.axios.com/2025/06/15/hortman-hoffman-congress-security-minnesota</a></p>
<hr />
<h2 id="collage-as-socialist-circulation">Collage as Socialist
Circulation</h2>
<p>date: 2025-06-15, from: Digital Humanities Quarterly News</p>
“The distinguishing characteristic of the modern author . . . is that he
is a proprietor, that he is conceived as the originator and therefore
the owner of a special kind of commodity, the ‘work.’” —Mark Rose, “The
Author as Proprietor” “The idea that a text belongs naturally and
uniquely to the person who wrote […]
<p>
<a href="https://www.digitalrhetoriccollaborative.org/2025/06/15/collage-as-socialist-circulation/" rel="nofollow">Source</a>
</p>
<p><br></p>
<p><a
href="https://www.digitalrhetoriccollaborative.org/2025/06/15/collage-as-socialist-circulation/"
class="uri">https://www.digitalrhetoriccollaborative.org/2025/06/15/collage-as-socialist-circulation/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-15, from: Dave Winer’s linkblog)</p>
<p>Inside Trump’s Extraordinary Turnaround on Immigration Raids.</p>
<p><br></p>
<p><a
href="https://www.nytimes.com/2025/06/14/us/politics/trump-immigration-raids-workers.html"
class="uri">https://www.nytimes.com/2025/06/14/us/politics/trump-immigration-raids-workers.html</a></p>
<hr />
<h2 id="understanding-inequality-part-iii-tariffs">Understanding
Inequality, Part III: Tariffs</h2>
<p>date: 2025-06-15, from: Paul Krugman</p>
<p>A Trumpian diversion</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/understanding-inequality-part-iii"
class="uri">https://paulkrugman.substack.com/p/understanding-inequality-part-iii</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-15, from: Dave Winer’s linkblog)</p>
<p>This is where AOC loses me. She’s doing what Repubs do, dividing us,
instead of uniting in our common cause. Let’s<span
style="letter-spacing: 0.01rem; -webkit-text-size-adjust: 100%;"> work
together.</span></p>
<p><br></p>
<p><a
href="https://www.politico.com/news/2025/06/14/aoc-rallies-against-cuomo-gerontocracy-00406346"
class="uri">https://www.politico.com/news/2025/06/14/aoc-rallies-against-cuomo-gerontocracy-00406346</a></p>
<hr />
<h2
id="seven-replies-to-the-viral-apple-reasoning-paper-and-why-they-fall-short">Seven
replies to the viral Apple reasoning paper – and why they fall
short</h2>
<p>date: 2025-06-15, updated: 2025-06-15, from: Simon Willison’s
Weblog</p>
<p>
<strong><a href="https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple">Seven
replies to the viral Apple reasoning paper – and why they fall
short</a></strong>
</p>
A few weeks ago Apple Research released a new paper
<a href="https://machinelearning.apple.com/research/illusion-of-thinking">The
Illusion of Thinking: Understanding the Strengths and Limitations of
Reasoning Models via the Lens of Problem Complexity</a>.
</p>
<blockquote>
<p>
Through extensive experimentation across diverse puzzles, we show that
frontier LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counter-intuitive scaling limit:
their reasoning effort increases with problem complexity up to a point,
then declines despite having an adequate token budget.
</p>
</blockquote>
<p>
I skimmed the paper and it struck me as a more thorough example of the
many other trick questions that expose failings in LLMs - this time
involving puzzles such as the Tower of Hanoi that can have their
difficulty level increased to the point that even “reasoning” LLMs run
out of output tokens and fail to complete them.
</p>
<p>
I thought this paper got <em>way</em> more attention than it warranted -
the title “The Illusion of Thinking” captured the attention of the “LLMs
are over-hyped junk” crowd. I saw enough well-reasoned rebuttals that I
didn’t feel it worth digging into.
</p>
<p>
And now, notable LLM skeptic Gary Marcus has saved me some time by
aggregating the best of those rebuttals
<a href="https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple">together
in one place</a>!
</p>
<p>
Gary rebuts those rebuttals, but given that his previous headline
concerning this paper was
<a href="https://garymarcus.substack.com/p/a-knockout-blow-for-llms">a
knockout blow for LLMs?</a> it’s not surprising that he finds those
arguments unconvincing. From that previous piece:
</p>
<blockquote>
<p>
The vision of AGI I have always had is one that <em>combines</em> the
strengths of humans with the strength of machines, overcoming the
weaknesses of humans. I am not interested in a “AGI” that can’t do
arithmetic, and I certainly wouldn’t want to entrust global
infrastructure or the future of humanity to such a system.
</p>
</blockquote>
<p>
Then from his new post:
</p>
<blockquote>
<p>
<strong>The paper is not news; we already knew these models generalize
poorly.</strong> True! (I personally have been trying to tell people
this for almost thirty years; Subbarao Rao Kambhampati has been trying
his best, too). But then why do we think these models are the royal road
to AGI?
</p>
</blockquote>
<p>
And therein lies my disagreement. I’m not interested in whether or not
LLMs are the “road to AGI”. I continue to care only about whether they
have useful applications today, once you’ve understood their
limitations.
</p>
<p>
Reasoning LLMs are a relatively new and interesting twist on the genre.
They are demonstrably able to solve a whole bunch of problems that
previous LLMs were unable to handle, hence why we’ve seen
<a href="https://simonwillison.net/tags/llm-reasoning/">a rush of new
models</a> from OpenAI and Anthropic and Gemini and DeepSeek and Qwen
and Mistral.
</p>
<p>
They get even more interesting when you
<a href="https://simonwillison.net/2025/Jun/6/six-months-in-llms/#ai-worlds-fair-2025-43.jpeg">combine
them with tools</a>.
</p>
<p>
<p>They’re already useful to me today, whether or not they can reliably
solve the Tower of Hanoi or River Crossing puzzles.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://news.ycombinator.com/item?id=44278403&quot;&gt;Hacker News&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/llm-reasoning&quot;&gt;llm-reasoning&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/apple&quot;&gt;apple&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Jun/15/viral-apple-reasoning-paper/#atom-everything"
class="uri">https://simonwillison.net/2025/Jun/15/viral-apple-reasoning-paper/#atom-everything</a></p>
<hr />
<h2 id="an-introduction-to-googles-approach-to-ai-agent-security">An
Introduction to Google’s Approach to AI Agent Security</h2>
<p>date: 2025-06-15, updated: 2025-06-15, from: Simon Willison’s
Weblog</p>
<p>
Here’s another new paper on AI agent security:
<strong><a href="https://research.google/pubs/an-introduction-to-googles-approach-for-secure-ai-agents/">An
Introduction to Google’s Approach to AI Agent Security</a></strong>, by
Santiago Díaz, Christoph Kern, and Kara Olive.
</p>
<p>
(I wrote about a different recent paper,
<a href="https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/">Design
Patterns for Securing LLM Agents against Prompt Injections</a> just a
few days ago.)
</p>
<p>
This Google paper describes itself as “our aspirational framework for
secure AI agents”. It’s a very interesting read.
</p>
<p>
Because I collect
<a href="https://simonwillison.net/tags/agent-definitions/">definitions
of “AI agents”</a>, here’s the one they use:
</p>
<blockquote>
<p>
AI systems designed to perceive their environment, make decisions, and
take autonomous actions to achieve user-defined goals.
</p>
</blockquote>
<h4 id="the-two-key-risks">
The two key risks
</h4>
<p>
The paper describes two key risks involved in deploying these systems. I
like their clear and concise framing here:
</p>
<blockquote>
<p>
The primary concerns demanding strategic focus are <strong>rogue
actions</strong> (unintended, harmful, or policy-violating actions) and
<strong>sensitive data disclosure</strong> (unauthorized revelation of
private information). A fundamental tension exists: increased agent
autonomy and power, which drive utility, correlate directly with
increased risk.
</p>
</blockquote>
<p>
The paper takes a less strident approach than the
<a href="https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/">design
patterns paper</a> from last week. That paper clearly emphasized that
“once an LLM agent has ingested untrusted input, it must be constrained
so that it is impossible for that input to trigger any consequential
actions”. This Google paper skirts around that issue, saying things like
this:
</p>
<blockquote>
<p>
<em>Security implication</em>: A critical challenge here is reliably
distinguishing trusted user commands from potentially untrusted
contextual data and inputs from other sources (for example, content
within an email or webpage). Failure to do so opens the door to prompt
injection attacks, where malicious instructions hidden in data can
hijack the agent. Secure agents must carefully parse and separate these
input streams.
</p>
<p>
Questions to consider:
</p>
<ul>
<li>
What types of inputs does the agent process, and can it clearly
distinguish trusted user inputs from potentially untrusted contextual
inputs?
</li>
</ul>
</blockquote>
<p>
Then when talking about system instructions:
</p>
<blockquote>
<p>
<em>Security implication</em>: A crucial security measure involves
clearly delimiting and separating these different elements within the
prompt. Maintaining an unambiguous distinction between trusted system
instructions and potentially untrusted user data or external content is
important for mitigating prompt injection attacks.
</p>
</blockquote>
<p>
Here’s my problem: in both of these examples the only correct answer is
that <strong>unambiguous separation is not possible</strong>! The way
the above questions are worded implies a solution that does not exist.
</p>
<p>
Shortly afterwards they do acknowledge exactly that (emphasis mine):
</p>
<blockquote>
<p>
Furthermore, <strong>current LLM architectures do not provide rigorous
separation between constituent parts of a prompt</strong> (in
particular, system and user instructions versus external, untrustworthy
inputs), making them susceptible to manipulation like prompt injection.
The common practice of iterative planning (in a “reasoning loop”)
exacerbates this risk: each cycle introduces opportunities for flawed
logic, divergence from intent, or hijacking by malicious data,
potentially compounding issues. Consequently, agents with high autonomy
undertaking complex, multi-step iterative planning present a
significantly higher risk, demanding robust security controls.
</p>
</blockquote>
<p>
This note about memory is excellent:
</p>
<blockquote>
<p>
Memory can become a vector for persistent attacks. If malicious data
containing a prompt injection is processed and stored in memory (for
example, as a “fact” summarized from a malicious document), it could
influence the agent’s behavior in future, unrelated interactions.
</p>
</blockquote>
<p>
And this section about the risk involved in rendering agent output:
</p>
<blockquote>
<p>
If the application renders agent output without proper sanitization or
escaping based on content type, vulnerabilities like Cross-Site
Scripting (XSS) or data exfiltration (from maliciously crafted URLs in
image tags, for example) can occur. Robust sanitization by the rendering
component is crucial.
</p>
<p>
Questions to consider: […]
</p>
<ul>
<li>
What sanitization and escaping processes are applied when rendering
agent-generated output to prevent execution vulnerabilities (such as
XSS)?
</li>
<li>
How is rendered agent output, especially generated URLs or embedded
content, validated to prevent sensitive data disclosure?
</li>
</ul>
</blockquote>
<p>
The paper then extends on the two key risks mentioned earlier, rogue
actions and sensitive data disclosure.
</p>
<h4 id="rogue-actions">
Rogue actions
</h4>
<p>
Here they include a cromulent definition of prompt injection:
</p>
<blockquote>
<p>
Rogue actions—unintended, harmful, or policy-violating agent
behaviors—represent a primary security risk for AI agents.
</p>
<p>
A key cause is <strong>prompt injection</strong>: malicious instructions
hidden within processed data (like files, emails, or websites) can trick
the agent’s core AI model, hijacking its planning or reasoning phases.
The model misinterprets this embedded data as instructions, causing it
to execute attacker commands using the user’s authority.
</p>
</blockquote>
<p>
Plus the related risk of <strong>misinterpretation</strong> of user
commands that could lead to unintended actions:
</p>
<blockquote>
<p>
The agent might misunderstand ambiguous instructions or context. For
instance, an ambiguous request like “email Mike about the project
update” could lead the agent to select the wrong contact, inadvertently
sharing sensitive information.
</p>
</blockquote>
<h4 id="sensitive-data-disclosure">
Sensitive data disclosure
</h4>
<p>
This is the most common form of prompt injection risk I’ve seen
demonstrated so far. I’ve written about this at length in my
<a href="https://simonwillison.net/tags/exfiltration-attacks/">exfiltration-attacks
tag</a>.
</p>
<blockquote>
<p>
A primary method for achieving sensitive data disclosure is data
exfiltration. This involves tricking the agent into making sensitive
information visible to an attacker. Attackers often achieve this by
<strong>exploiting agent actions and their side effects</strong>,
typically driven by prompt injection. […] They might trick the agent
into retrieving sensitive data and then leaking it through actions, such
as embedding data in a URL the agent is prompted to visit, or hiding
secrets in code commit messages.
</p>
</blockquote>
<h4 id="three-core-principles-for-agent-security">
Three core principles for agent security
</h4>
<p>
The next section of the paper describes Google’s three core principles
for agent security:
</p>
<p>
Principle 1 is that <strong>Agents must have well-defined human
controllers</strong>.
</p>
<blockquote>
<p>
[…] it is essential for security and accountability that agents operate
under clear human oversight. Every agent must have a well-defined set of
controlling human user(s).
</p>
<p>
This principle mandates that systems must be able to reliably
distinguish instructions originating from an authorized controlling user
versus any other input, especially potentially untrusted data processed
by the agent. For actions deemed critical or irreversible—such as
deleting large amounts of data, authorizing significant financial
transactions, or changing security settings—the system should require
explicit human confirmation before proceeding, ensuring the user remains
in the loop. […]
</p>
<p>
Agents acting on behalf of teams or groups need distinct identities and
clear authorization models to prevent unauthorized cross-user data
access or one user inadvertently triggering actions impacting another.
</p>
</blockquote>
<p>
There are two parts to this then: tracking <em>which</em> user is
controlling the agent, and adding a human-in-the-loop confirmation step
for critical actions.
</p>
<p>
Principle 2 is <strong>Agent powers must have limitations</strong>.
</p>
<blockquote>
<p>
An agent’s powers—the actions it can take and the resources it can
access—must be carefully limited in alignment with its intended purpose
and its controlling user’s risk tolerance. For example, an agent
designed for research should not possess the power to modify financial
accounts. General-purpose agents need mechanisms to dynamically confine
their capabilities at runtime, ensuring only relevant permissions are
active for any given query (for example, disallowing file deletion
actions when the task is creative writing).
</p>
</blockquote>
<p>
This represents a more sophisticated approach to agent permissions than
I’ve seen before. The idea that an agent’s permisisons should
dynamically change based on the task is certainly intriguing, though I
find it hard to imagine how it can work well in practice. The only
implementation approach I can think of would involve adding more layers
of AI that dynamically adjust permissions based on the percieved task,
and that feels inherently risky to me since prompt injection attacks
could influence those decisions.
</p>
<p>
Principle 3 is that <strong>Agent actions and planning must be
observable</strong>. I <em>love</em> this principle - emphasis mine:
</p>
<blockquote>
<p>
We cannot ensure an agent is acting faithfully or diagnose problems if
its operations are entirely opaque. Therefore, <strong>agent
actions</strong>, and where feasible, their planning processes,
<strong>must be observable and auditable</strong>. […]
</p>
<p>
Effective observability also means that the properties of the actions an
agent can take—such as whether an action is read-only versus
state-changing, or if it handles sensitive data—must be clearly
characterized. This metadata is crucial for automated security
mechanisms and human reviewers. Finally, <strong>user interfaces should
be designed to promote transparency</strong>, providing users with
insights into the agent’s “thought process,” the data sources it
consulted, or the actions it intends to take, especially for complex or
high-risk operations.
</p>
</blockquote>
<p>
<strong>Yes. Yes. Yes.</strong> LLM systems that hide what they are
doing from me are inherently frustrating - they make it much harder for
me to evaluate if they are doing a good job and spot when they make
mistakes. This paper has convinced me that there’s a very strong
security argument to be made too: the more opaque the system, the less
chance I have to identify when it’s going rogue and being subverted by
prompt injection attacks.
</p>
<h4 id="google-s-hybrid-defence-in-depth-strategy">
Google’s hybrid defence-in-depth strategy
</h4>
<p>
<img src="https://static.simonwillison.net/static/2025/google-hybrid.jpg" alt="Architecture diagram showing AI agent safety framework with runtime policy enforcement connecting to reasoning-based defenses (highlighted in purple), which along with regression testing, variant analysis, and red teams &amp; human reviewers provide dependable constraints on agent privileges and hardening of the base model, classifiers, and safety fine-tuning, plus testing for regressions, variants, and new vulnerabilities, all feeding into an AI Agent system containing Application, Perception, Rendering, Reasoning core, and Orchestration components with bidirectional arrows showing data flow between components." style="max-width: 100%;" />
</p>
<p>
All of which leads us to the discussion of Google’s current hybrid
defence-in-depth strategy. They optimistically describe this as
combining “traditional, deterministic security measures with dynamic,
reasoning-based defenses”. I like determinism but I remain <em>deeply
skeptical</em> of “reasoning-based defenses”, aka addressing security
problems with non-deterministic AI models.
</p>
<p>
The way they describe their layer 1 makes complete sense to me:
</p>
<blockquote>
<p>
<strong>Layer 1: Traditional, deterministic measures (runtime policy
enforcement)</strong>
</p>
<p>
When an agent decides to use a tool or perform an action (such as “send
email,” or “purchase item”), the request is intercepted by the policy
engine. The engine evaluates this request against predefined rules based
on factors like the action’s inherent risk (Is it irreversible? Does it
involve money?), the current context, and potentially the chain of
previous actions (Did the agent recently process untrusted data?). For
example, a policy might enforce a spending limit by automatically
blocking any purchase action over $500 or requiring explicit user
confirmation via a prompt for purchases between $100 and $500. Another
policy might prevent an agent from sending emails externally if it has
just processed data from a known suspicious source, unless the user
explicitly approves.
</p>
<p>
Based on this evaluation, the policy engine determines the outcome: it
can <strong>allow</strong> the action, <strong>block</strong> it if it
violates a critical policy, or <strong>require user
confirmation</strong>.
</p>
</blockquote>
<p>
I really like this. Asking for user confirmation for everything quickly
results in “prompt fatigue” where users just click “yes” to everything.
This approach is smarter than that: a policy engine can evaluate the
risk involved, e.g. if the action is irreversible or involves more than
a certain amount of money, and only require confirmation in those cases.
</p>
<p>
I also like the idea that a policy “might prevent an agent from sending
emails externally if it has just processed data from a known suspicious
source, unless the user explicitly approves”. This fits with the data
flow analysis techniques described in
<a href="https://simonwillison.net/2025/Apr/11/camel/">the CaMeL
paper</a>, which can help identify if an action is working with data
that may have been tainted by a prompt injection attack.
</p>
<p>
Layer 2 is where I start to get uncomfortable:
</p>
<blockquote>
<p>
<strong>Layer 2: Reasoning-based defense strategies</strong>
</p>
<p>
To complement the deterministic guardrails and address their limitations
in handling context and novel threats, the second layer leverages
reasoning-based defenses: techniques that use AI models themselves to
evaluate inputs, outputs, or the agent’s internal reasoning for
potential risks.
</p>
</blockquote>
<p>
They talk about <strong>adversarial training</strong> against examples
of prompt injection attacks, attempting to teach the model to recognize
and respect delimiters, and suggest <strong>specialized guard
models</strong> to help classify potential problems.
</p>
<p>
I understand that this is part of defence-in-depth, but I still have
trouble seeing how systems that can’t provide guarantees are a
worthwhile addition to the security strategy here.
</p>
<p>
They do at least acknowlede these limitations:
</p>
<blockquote>
<p>
However, these strategies are non-deterministic and cannot provide
absolute guarantees. Models can still be fooled by novel attacks, and
their failure modes can be unpredictable. This makes them inadequate, on
their own, for scenarios demanding absolute safety guarantees,
especially involving critical or irreversible actions. They must work in
concert with deterministic controls.
</p>
</blockquote>
<p>
I’m much more interested in their layer 1 defences then the approaches
they are taking in layer 2.
</p>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai-agents&quot;&gt;ai-agents&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/prompt-injection&quot;&gt;prompt-injection&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/security&quot;&gt;security&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/google&quot;&gt;google&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/exfiltration-attacks&quot;&gt;exfiltration-attacks&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/paper-review&quot;&gt;paper-review&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/agent-definitions&quot;&gt;agent-definitions&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Jun/15/ai-agent-security/#atom-everything"
class="uri">https://simonwillison.net/2025/Jun/15/ai-agent-security/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2025-06-15, from: Dave Winer’s linkblog)</p>
<p>Nintendo Switch 2 review: everything you need to know.</p>
<p><br></p>
<p><a
href="https://www.npr.org/2025/06/12/nx-s1-5415890/nintendo-switch-2-mario-kart-world-review"
class="uri">https://www.npr.org/2025/06/12/nx-s1-5415890/nintendo-switch-2-mario-kart-world-review</a></p>
<hr />
<h2 id="tag-youre-it">Tag, you’re it</h2>
<p>date: 2025-06-15, updated: 2025-06-15, from: Tink’s blog</p>
<p><br></p>
<p><a href="https://tink.uk/tag-your-it/"
class="uri">https://tink.uk/tag-your-it/</a></p>
<hr />
<h2 id="layers-of-memory-layers-of-compression">Layers of Memory, Layers
of Compression</h2>
<p>date: 2025-06-15, updated: 2025-06-15, from: Tom Kellog blog</p>
<p>AI superpower = strategic amnesia.</p>
<p>Letta caches memory like a CPU, Anthropic spreads it across agent
swarms, Cognition warns of chaos. Curious how forgetting makes machines
smarter? Dive in.</p>
<p><br></p>
<p><a href="http://timkellogg.me/blog/2025/06/15/compression"
class="uri">http://timkellogg.me/blog/2025/06/15/compression</a></p>
</section>
<footer>
Antenna is a personal aggregation of items found around the web.
Curated with <a href="https://rsdoiel.github.io/skimmer">skimmer</a> and <a href="https://sqlite.org">sqlite</a> then rendered with <a href="https://pandoc.org">Pandoc</a>.
</footer>
</body>
</html>
