<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8" >
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" >
  <title>snapshots</title>
<!--  <link rel="stylesheet" type="text/css"  href="../webfonts/fonts.css" media="screen" > -->
  <link rel="stylesheet" type="text/css"  href="../css/site.css" media="screen" >
</head>
<body>
<header>
	<img class="logo" 
	src="https://upload.wikimedia.org/wikipedia/commons/9/9c/Antenna_1_-_The_Noun_Project.svg"
	alt="line art showing an antenna"
	height="80" width="60" >
	<h1>The Antenna</h1> 
	<h2>finding signal in the noise</h2>
</header>
<nav>
<ul>
	<li><a href="../../">The Antenna</a></li>
	<li><a href="../">Archives</a></li>
	<li><a href="../../about.html">About</a></li>
</ul>
</nav>
<section>
<div class="description-for-items">
<h2>snapshots</h2>
An experiment in personal news aggregation.
</div>
<h1 id="snapshots">snapshots</h1>
<p>(date: 2025-03-13 06:11:37)</p>
<hr />
<h2
id="ai-can-rip-you-off.-heres-how-california-lawmakers-want-to-stop-price-discrimination">AI
Can Rip You Off. Here‚Äôs How California Lawmakers Want to Stop Price
Discrimination</h2>
<p>date: 2025-03-13, from: The Markup blog</p>
<p>AI can help businesses charge you more based on how it evaluates your
personal history and desires. California lawmakers want to end that
price discrimination</p>
<p><br></p>
<p><a
href="https://themarkup.org/artificial-intelligence/2025/03/13/ai-can-rip-you-off-heres-how-california-lawmakers-want-to-stop-price-discrimination"
class="uri">https://themarkup.org/artificial-intelligence/2025/03/13/ai-can-rip-you-off-heres-how-california-lawmakers-want-to-stop-price-discrimination</a></p>
<hr />
<h2
id="california-is-considering-30-new-ai-regulations.-trump-wants-none">California
Is Considering 30 New AI Regulations. Trump Wants None</h2>
<p>date: 2025-03-13, from: The Markup blog</p>
<p>President Trump is taking a very different approach to AI than the
Biden administration, and it could affect how California regulates
artificial intelligence</p>
<p><br></p>
<p><a
href="https://themarkup.org/artificial-intelligence/2025/03/13/california-is-considering-30-new-ai-regulations-trump-wants-none"
class="uri">https://themarkup.org/artificial-intelligence/2025/03/13/california-is-considering-30-new-ai-regulations-trump-wants-none</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>As ICE targets a Palestinian activist, some Jews are asking if this
is the fight against antisemitism they signed up for. (Being
pro-Palestinian is not antisemitism.)</p>
<p><br></p>
<p><a
href="https://www.jta.org/2025/03/11/united-states/as-ice-targets-a-pro-palestinian-activist-some-jews-are-asking-if-this-is-the-fight-against-antisemitism-they-signed-up-for"
class="uri">https://www.jta.org/2025/03/11/united-states/as-ice-targets-a-pro-palestinian-activist-some-jews-are-asking-if-this-is-the-fight-against-antisemitism-they-signed-up-for</a></p>
<hr />
<h2 id="making-sweatshops-great-again">Making Sweatshops Great
Again</h2>
<p>date: 2025-03-13, from: Paul Krugman</p>
<p>Does Trump want us to manufacture sneakers, not semiconductors?</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/making-sweatshops-great-again"
class="uri">https://paulkrugman.substack.com/p/making-sweatshops-great-again</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>Taking over the Podcast network: Michelle, WSJ.</p>
<p><br></p>
<p><a
href="https://viehumaniste.wordpress.com/2025/03/11/taking-over-michelle-wsj/"
class="uri">https://viehumaniste.wordpress.com/2025/03/11/taking-over-michelle-wsj/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>No response from either TechCrunch or the company, as before. Bluesky
is doing great, it‚Äôs time they drop the pretense that they‚Äôre
‚Äúbillionaire-proof.‚Äù They obviously don‚Äôt respect this blogger, perhaps
a journalist could ask about their claim.</p>
<p><br></p>
<p><a href="http://scripting.com/2025/03/11/113838.html"
class="uri">http://scripting.com/2025/03/11/113838.html</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>Elon Musk Looks Desperate.</p>
<p><br></p>
<p><a
href="https://www.theatlantic.com/technology/archive/2025/03/elon-musk-human-meme-stock/682023/?gift=f35zZN0v_gDFE8xNwlQAHXwjqMU0i0qxVV4QHoDkshc&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share"
class="uri">https://www.theatlantic.com/technology/archive/2025/03/elon-musk-human-meme-stock/682023/?gift=f35zZN0v_gDFE8xNwlQAHXwjqMU0i0qxVV4QHoDkshc&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>Trump plans "law and order" speech at Justice Department Friday.</p>
<p><br></p>
<p><a
href="https://www.axios.com/2025/03/13/trump-justice-department-speech"
class="uri">https://www.axios.com/2025/03/13/trump-justice-department-speech</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>The Portland crowd enjoyed the Knicks buzzer beater overtime win over
their hometown Trailblazers. Where ever you go the Knicks fans are
there.</p>
<p><br></p>
<p><a
href="https://sports.yahoo.com/article/see-mikal-bridges-overtime-game-061500500.html"
class="uri">https://sports.yahoo.com/article/see-mikal-bridges-overtime-game-061500500.html</a></p>
<hr />
<h2 id="hofstaders-butterfly">288: Hofstader‚Äôs Butterfly</h2>
<p>date: 2025-03-13, from: Fix the News</p>
<p>Fundamental principles across different scales of reality. Plus,
primordial water, super lasers, Right to Repair, and good news on
childcare in Canada, vaccinations in Zimbabwe, de-mining in Cambodia,
and re-wilding in New Zealand.</p>
<p><br></p>
<p><a href="https://fixthenews.com/288-hofstaders-butterfly/"
class="uri">https://fixthenews.com/288-hofstaders-butterfly/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-13, from: Dave Winer‚Äôs linkblog)</p>
<p>Biggest Federal Employee Unions Says Shutdown is Preferable to
Elon/Trump CR.</p>
<p><br></p>
<p><a
href="https://talkingpointsmemo.com/edblog/biggest-federal-employee-unions-says-shutdown-is-preferable-to-elon-trump-cr"
class="uri">https://talkingpointsmemo.com/edblog/biggest-federal-employee-unions-says-shutdown-is-preferable-to-elon-trump-cr</a></p>
<hr />
<h2 id="the-french-revolution-the-execution-of-the-king-part-4">The
French Revolution: The Execution of the King (Part 4)</h2>
<p>date: 2025-03-13, from: This is history podcast</p>
<p>
The second revolution that engulfed France over the course of 1792
reached its climax in December, with an astonishing, world-changing
spectacle, which held all the eyes of Europe spellbound: Louis Capet,
formerly King Louis XVI of France, was on trial for his very life. A
guilty verdict would undermine millennia of thought and tradition,
ripping [‚Ä¶]
</p>
<p>
The post
<a href="https://therestishistory.com/the-french-revolution-the-execution-of-the-king-part-4/">The
French Revolution: The Execution of the King (Part 4)</a> appeared first
on <a href="https://therestishistory.com">The Rest is History</a>.
</p>
<p><br></p>
<p><a
href="https://therestishistory.com/the-french-revolution-the-execution-of-the-king-part-4/"
class="uri">https://therestishistory.com/the-french-revolution-the-execution-of-the-king-part-4/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>Is Trump Taking a ‚ÄòLiquidationist‚Äô Approach to the Economy?</p>
<p><br></p>
<p><a
href="https://www.wsj.com/economy/is-trump-taking-a-liquidationist-approach-to-the-economy-dc06d544?st=m5pNes&amp;reflink=desktopwebshare_permalink"
class="uri">https://www.wsj.com/economy/is-trump-taking-a-liquidationist-approach-to-the-economy-dc06d544?st=m5pNes&amp;reflink=desktopwebshare_permalink</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>Democrats to hold town halls in GOP districts amid controversy.</p>
<p><br></p>
<p><a
href="https://thehill.com/homenews/house/5191810-democrats-host-town-halls-republican-districts/"
class="uri">https://thehill.com/homenews/house/5191810-democrats-host-town-halls-republican-districts/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-12, from: Feed for Alt USDS)</p>
<p>Gov workers aren‚Äôt lazy‚Äîwe‚Äôre the ones fixing broken tax records,
tracking down missing payments, and helping people when they need it
most. The system works because we do.</p>
<p>Read how one IRS worker spent 13 years making things right. üîó
wethebuilders.org/posts/we-are-the-helpers</p>
<p>#WeArePublicService</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk7lejuuai2p"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk7lejuuai2p</a></p>
<hr />
<h2
id="aoostar-g-flip-mini-pc-features-ryzen-ai-9-hx-370-and-a-flip-up-display">AOOSTAR
G-Flip mini PC features Ryzen AI 9 HX 370 and a flip-up display</h2>
<p>date: 2025-03-12, from: Liliputing</p>
<p>
The¬†AOOSTAR G-Flip is an upcoming mini PC with a few unusual features.
For one thing, it has an AMD Strix Point processor, with support for up
to a Ryzen AI 9 HX 370 12-core, 24-thread CPU and 16-core RDNA 3.5
graphics. That‚Äôs uncommon, but not exactly unique though. What¬†is
strange about¬† this little computer though, [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/aoostar-g-flip-mini-pc-features-ryzen-ai-9-hx-370-and-a-flip-up-display/">AOOSTAR
G-Flip mini PC features Ryzen AI 9 HX 370 and a flip-up display</a>
appeared first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/aoostar-g-flip-mini-pc-features-ryzen-ai-9-hx-370-and-a-flip-up-display/"
class="uri">https://liliputing.com/aoostar-g-flip-mini-pc-features-ryzen-ai-9-hx-370-and-a-flip-up-display/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-12, from: Feed for Alt USDS)</p>
<p>Nearly 30 U.S. Digital Services employees have worked with
hardworking civil servants at the Dept of Education over the years -
most notably (and still) on the Free Application for Federal Student Aid
(FAFSA).</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk7caphobc2h"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk7caphobc2h</a></p>
<hr />
<h2
id="kensington-expert-mouse-tb800-eq-is-a-multi-device-trackball-mouse">Kensington
Expert Mouse TB800 EQ is a multi-device trackball mouse</h2>
<p>date: 2025-03-12, from: Liliputing</p>
<p>
Kensington may be planning to launch a new version of its Expert Mouse
wireless trackball that keeps many of the features of the current model,
while adding a few bells and whistles that could make for a more
versatile computer input device. The company recently won an IF Design
Guide award for a previously unannounced [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/kensington-expert-mouse-tb800-eq-is-a-multi-function-trackball-mouse/">Kensington
Expert Mouse TB800 EQ is a multi-device trackball mouse</a> appeared
first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/kensington-expert-mouse-tb800-eq-is-a-multi-function-trackball-mouse/"
class="uri">https://liliputing.com/kensington-expert-mouse-tb800-eq-is-a-multi-function-trackball-mouse/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>COVID keeps evolving and will find new ways to evade immune
systems.</p>
<p><br></p>
<p><a
href="https://www.npr.org/sections/shots-health-news/2025/03/11/nx-s1-5312100/covid-virus-mutation-evolution"
class="uri">https://www.npr.org/sections/shots-health-news/2025/03/11/nx-s1-5312100/covid-virus-mutation-evolution</a></p>
<hr />
<h2 id="chatgpt-now-integrates-directly-with-xcode">ChatGPT Now
Integrates Directly With Xcode</h2>
<p>date: 2025-03-12, from: Michael Tsai</p>
<p>Tim Hardwick: OpenAI has updated the ChatGPT app for macOS with the
ability to directly edit code in popular development environments
including Xcode, VS Code, and JetBrains tools. The new feature allows
the AI chatbot to make changes to code without requiring users to copy
and paste between applications. ChatGPT can now read code from [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/12/chatgpt-now-integrates-directly-with-xcode/"
class="uri">https://mjtsai.com/blog/2025/03/12/chatgpt-now-integrates-directly-with-xcode/</a></p>
<hr />
<h2 id="whither-swift-assist">Whither Swift Assist?</h2>
<p>date: 2025-03-12, from: Michael Tsai</p>
<p>Apple (June 2024): Swift Assist serves as a companion for all of a
developer‚Äôs coding tasks, so they can focus on higher-level problems¬†and
solutions. It‚Äôs seamlessly integrated into Xcode, and knows the latest
software development kits (SDKs) and Swift language features, so
developers will always get the latest code features that blend perfectly
into their [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/12/whither-swift-assist/"
class="uri">https://mjtsai.com/blog/2025/03/12/whither-swift-assist/</a></p>
<hr />
<h2
id="apple-delays-more-personalized-siri-apple-intelligence-features">Apple
Delays ‚ÄúMore Personalized Siri‚Äù Apple Intelligence Features</h2>
<p>date: 2025-03-12, from: Michael Tsai</p>
<p>John Gruber (Mastodon, Dithering, MacRumors, Slashdot): Reading
between the lines, and based on my PhD-level fluency in Cupertino-ese,
what Apple is saying here is that these ‚Äúmore personalized Siri‚Äù
features are being punted from this year‚Äôs OS cycle to next year‚Äôs: to
iOS 19 and MacOS 16. [‚Ä¶] It was already pretty obvious these features
[‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/12/apple-delays-more-personalized-siri-apple-intelligence-features/"
class="uri">https://mjtsai.com/blog/2025/03/12/apple-delays-more-personalized-siri-apple-intelligence-features/</a></p>
<hr />
<h2 id="ai-summaries-of-app-store-reviews">AI Summaries of App Store
Reviews</h2>
<p>date: 2025-03-12, from: Michael Tsai</p>
<p>Filipe Esp√≥sito (October 2024, MacRumors): As seen by 9to5Mac in an
unlisted App Store article, Apple has developed a new system that will
use all user reviews on the App Store to create a summary highlighting
‚Äúthe most common customer feedback‚Äù about each app. According to the
article, the summaries will be updated every time [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/12/ai-summaries-of-app-store-reviews/"
class="uri">https://mjtsai.com/blog/2025/03/12/ai-summaries-of-app-store-reviews/</a></p>
<hr />
<p><strong><span class="citation" data-cites="IIIF">@IIIF</span>
Mastodon feed</strong> (date: 2025-03-12, from: IIIF Mastodon feed)</p>
<p>
üìÜ SAVE THE DATE
</p>
<p>
On Mar.¬†17 @ 11am ET/3pm GMT, at the
<a href="https://glammr.us/tags/IIIF" class="mention hashtag" rel="tag">#<span>IIIF</span></a>
Museums Group, Rodolfo Peraza will present the MUD Foundations's work on
3D &amp; 2D visualization for open-access digital collections for
museums, including integrations w/ IIIF services with MUD Verse
</p>
<p>
Zoom info: iiif.io/community
</p>
<p><br></p>
<p><a href="https://glammr.us/@IIIF/114150924181767622"
class="uri">https://glammr.us/@IIIF/114150924181767622</a></p>
<hr />
<h2
id="clockwork-picocalc-is-a-retro-handheld-powered-by-a-raspberry-pi-pico">Clockwork
PicoCalc is a retro handheld powered by a Raspberry Pi Pico</h2>
<p>date: 2025-03-12, from: Liliputing</p>
<p>
The¬†PicoCalc is a new handheld with a retro design. At first glance it
looks more like a graphing calculator than a modern handheld device. But
it sports a QWERTY keyboard, a 4 inch display, and a mainboard designed
to accommodate a Raspberry Pi Pico. Available for $75 from the Clockwork
Pi store, it‚Äôs also the [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/clockwork-picocalc-is-a-retro-handheld-powered-by-a-raspberry-pi-pico/">Clockwork
PicoCalc is a retro handheld powered by a Raspberry Pi Pico</a> appeared
first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/clockwork-picocalc-is-a-retro-handheld-powered-by-a-raspberry-pi-pico/"
class="uri">https://liliputing.com/clockwork-picocalc-is-a-retro-handheld-powered-by-a-raspberry-pi-pico/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Robert">@Robert</span>‚Äôs
feed at BlueSky</strong> (date: 2025-03-12, from: Robert‚Äôs feed at
BlueSky)</p>
<p>Cool stuff.</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/rsdoiel.bsky.social/post/3lk74cw3sgk25"
class="uri">https://bsky.app/profile/rsdoiel.bsky.social/post/3lk74cw3sgk25</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>"Mail Pages" were a feature of my blog in 1997 and 1998. People could
send email in response to a post, I read them all and published the most
interesting ones, by my subjective criteria, to the current mail page. I
think something like this could work in 2025 and beyond.</p>
<p><br></p>
<p><a href="http://scripting.com/mail/mailThru971027.html"
class="uri">http://scripting.com/mail/mailThru971027.html</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>The Lives of Others (2006), a drama and political thriller set in
East Berlin before the wall came down. How secret police kept watch on
the people. It foretells what's sure to come in the US, and it's also a
fantastic movie.</p>
<p><br></p>
<p><a href="https://www.metacritic.com/movie/the-lives-of-others/"
class="uri">https://www.metacritic.com/movie/the-lives-of-others/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>FTC asks to delay Amazon Prime deceptive practices case, citing
staffing shortfalls. Quid pro quo?</p>
<p><br></p>
<p><a
href="https://www.cnbc.com/2025/03/12/ftc-asks-to-delay-amazon-prime-deceptive-practices-case-citing-staffing-shortfalls.html"
class="uri">https://www.cnbc.com/2025/03/12/ftc-asks-to-delay-amazon-prime-deceptive-practices-case-citing-staffing-shortfalls.html</a></p>
<hr />
<h2
id="saudi-arabia-buys-pok√©mon-go-and-probably-all-of-your-location-data">Saudi
Arabia Buys Pok√©mon Go, and Probably All of Your Location Data</h2>
<p>date: 2025-03-12, from: 404 Media Group</p>
<p>A company owned by the Saudi Arabian Public Investment Fund just
bought the most popular AR video game of all time.</p>
<p><br></p>
<p><a
href="https://www.404media.co/saudi-arabia-buys-pokemon-go-and-probably-all-of-your-location-data/"
class="uri">https://www.404media.co/saudi-arabia-buys-pokemon-go-and-probably-all-of-your-location-data/</a></p>
<hr />
<h2 id="ai-coding-fantasy-meets-pac-man">AI Coding Fantasy meets
Pac-Man</h2>
<p>date: 2025-03-12, from: Gary Marcus blog</p>
<p>Guess who won?</p>
<p><br></p>
<p><a
href="https://garymarcus.substack.com/p/ai-coding-fantasy-meets-pac-man"
class="uri">https://garymarcus.substack.com/p/ai-coding-fantasy-meets-pac-man</a></p>
<hr />
<h2 id="notes-on-googles-gemma-3">Notes on Google‚Äôs Gemma 3</h2>
<p>date: 2025-03-12, updated: 2025-03-12, from: Simon Willison‚Äôs
Weblog</p>
<p>
Google‚Äôs Gemma team released an impressive new model today (under their
not-open-source <a href="https://ai.google.dev/gemma/terms">Gemma
license</a>). Gemma 3 comes in four sizes - 1B, 4B, 12B, and 27B - and
while 1B is text-only the larger three models are all multi-modal for
vision:
</p>
<blockquote>
<p>
Gemma 3 introduces multimodality, supporting vision-language input and
text outputs. It handles context windows up to 128k tokens, understands
over 140 languages, and offers improved math, reasoning, and chat
capabilities, including structured outputs and function calling.
</p>
</blockquote>
<p>
Here‚Äôs the
<a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf">Gemma
3 Technical Report PDF</a>, which makes the big claim that they find
‚ÄúGemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks‚Äù.
</p>
<p>
I tried out the largest model using the latest
<a href="https://ollama.com/">Ollama</a> - this is the second time I‚Äôve
spotted a major model release partnering with Ollama on launch day, the
first being
<a href="https://simonwillison.net/2025/Jan/30/mistral-small-3/">Mistral
Small 3</a>.
</p>
<p>
I ran this (after upgrading Ollama through their menu icon upgrade
option):
</p>
<pre><code>ollama pull gemma3:27b
</code></pre>
<p>
That pulled 17GB of model weights. I‚Äôve been trying it out using
<a href="https://llm.datasette.io/">LLM</a> and
<a href="https://github.com/taketwo/llm-ollama">llm-ollama</a>:
</p>
<pre><code>llm install llm-ollama
llm -m gemma3:27b 'Build a single page HTML+CSS+JavaScript UI that gives me a large textarea for writing in which constantly saves what I have entered to localStorage (restoring when I reload the page) and displays a word counter'
</code></pre>
<p>
That was a replay of
<a href="https://tools.simonwillison.net/colophon#word-counter.html">a
prompt</a> I ran against Claude Artifacts a few months ago. Here‚Äôs
<a href="https://static.simonwillison.net/static/2025/gemma3-word-count.html">what
Gemma built</a>, and the
<a href="https://gist.github.com/simonw/f8a87f2242439d3974405a17f5d8bc23">full
chat transcript</a>. It‚Äôs a simple example but it worked just right.
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/gemma-word-count.gif" alt="Animated demo - a simple textarea, I type text in it and a word count updates above." />
</p>
<p>
Let‚Äôs try out the vision mode, by asking it to describe this photo:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/two-pelicans.jpg" alt="Two pelicans flying against a blue sky " style="max-width: 100%;" />
</p>
<pre><code>llm -m gemma3:27b describe -a https://static.simonwillison.net/static/2025/two-pelicans.jpg
</code></pre>
<p>
Here‚Äôs
<a href="https://gist.github.com/simonw/0e703ba72d0cf5a6dd117981bd03e604">the
full response</a>:
</p>
<blockquote>
<p>
<strong>Overall Impression:</strong>
</p>
<p>
The image features two brown pelicans in flight against a solid, bright
blue background. The composition is dynamic, capturing the birds
mid-wingbeat.
</p>
<p>
<strong>Details:</strong>
</p>
<ul>
<li>
<strong>Pelicans:</strong> Both pelicans are a rich, dark brown color.
They have large, pale yellow-orange beaks and pouches. One pelican is
shown with its wings fully extended, appearing to be gliding or soaring.
The other is captured with its wings in a more dynamic, upward-sweeping
motion.
</li>
<li>
<strong>Background:</strong> The background is a flat, uniform shade of
bright blue, which makes the pelicans stand out.
</li>
<li>
<strong>Composition:</strong> The birds are positioned in opposite
directions, creating a sense of movement and balance. The image is
well-lit, highlighting the texture of the pelicans‚Äô feathers.
</li>
</ul>
<p>
<strong>Overall, the image is a striking and visually appealing
depiction of these majestic birds in flight.</strong>
</p>
</blockquote>
<p>
Is it right to say ‚Äúthe birds are positioned in opposite directions‚Äù? I
thought that was a mistake at first, but I‚Äôll give it a pass since their
wings are positioned in opposite directions to each other.
</p>
<p>
Something I‚Äôve been curious about recently is longer context support:
how well can a local model on my laptop deal with summarization or data
extraction tasks against longer pieces of text?
</p>
<p>
I decided to try
<a href="https://til.simonwillison.net/llms/claude-hacker-news-themes#user-content-adding-a--m-model-option">my
Hacker News summarize script</a> using Gemma, against
<a href="https://news.ycombinator.com/item?id=43340491">the thread</a>
there discussing the Gemma 3 technical paper.
</p>
<p>
First I did a quick token count (using the OpenAI tokenizer but it‚Äôs
usually a similar number to other models):
</p>
<pre><code>curl 'https://hn.algolia.com/api/v1/items/43340491' | ttok
</code></pre>
<p>
This returned 22,260 - well within Gemma‚Äôs documented limits but still a
healthy number considering just last year most models topped out at
4,000 or 8,000.
</p>
<p>
I ran my script like this:
</p>
<pre><code>hn-summary.sh 43340491 -m gemma3:27b
</code></pre>
<p>
It did a pretty good job! Here‚Äôs the
<a href="https://gist.github.com/simonw/ab487ea3d1605e719dc2950cd4565146">full
prompt and response</a>. The one big miss is that it ignored my
instructions to include illustrative quotes - I don‚Äôt know if modifying
the prompt will fix that but it‚Äôs disappointing that it didn‚Äôt handle
that well, given how important direct quotes are for building confidence
in RAG-style responses.
</p>
<p>
Here‚Äôs what I got for
<a href="https://gist.github.com/simonw/f79c4bd2fbe966e1b600cb8b41cae810">Generate
an SVG of a pelican riding a bicycle</a>:
</p>
<pre><code>llm -m gemma3:27b 'Generate an SVG of a pelican riding a bicycle'
</code></pre>
<p style="text-align: center">
<img src="https://static.simonwillison.net/static/2025/gemma-3-pelican.svg" alt="A collection of abstract shapes, definitely not a pelican on a bicycle" />
</p>
<p>
You can also try out the new Gemma
<a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it">in
Google AI Studio</a>, and via their API. I added support for it to
<a href="https://github.com/simonw/llm-gemini/releases/tag/0.15">llm-gemini
0.15</a>, though sadly it appears vision mode doesn‚Äôt work with that API
hosted model yet.
</p>
<pre><code>llm install -U llm-gemini
llm keys set gemini
# paste key here
llm -m gemma-3-27b-it 'five facts about pelicans of interest to skunks'
</code></pre>
<p>
<a href="https://gist.github.com/simonw/bc22062e60e5af3faf458756cb368d0e">Here‚Äôs
what I got</a>. I‚Äôm not sure how pricing works for that hosted model.
</p>
<p>
Gemma 3 is also already available
<a href="https://github.com/Blaizzy/mlx-vlm/pull/235">through
MLX-VLM</a> - here‚Äôs
<a href="https://huggingface.co/collections/mlx-community/gemma-3-67d14a10480a436ad478b0f9">the
MLX model collection</a> - but I haven‚Äôt tried that version yet.
</p>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/google&quot;&gt;google&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gemini&quot;&gt;gemini&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/vision-llms&quot;&gt;vision-llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/mlx&quot;&gt;mlx&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ollama&quot;&gt;ollama&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/pelican-riding-a-bicycle&quot;&gt;pelican-riding-a-bicycle&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gemma&quot;&gt;gemma&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/12/gemma-3/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/12/gemma-3/#atom-everything</a></p>
<hr />
<h2 id="comics-from-197805-creative-computing-mag">Comics from 1978/05
Creative Computing Mag</h2>
<p>date: 2025-03-12, from: Computer ads from the Past</p>
<p>Time for some oldie levity.</p>
<p><br></p>
<p><a
href="https://computeradsfromthepast.substack.com/p/comics-from-197805-creative-computing"
class="uri">https://computeradsfromthepast.substack.com/p/comics-from-197805-creative-computing</a></p>
<hr />
<h2
id="compal-hybrid-aio-concept-pc-is-a-laptop-tablet-and-dual-screen-desktop">Compal
Hybrid AIO concept PC is a laptop, tablet, and dual-screen desktop</h2>
<p>date: 2025-03-12, from: Liliputing</p>
<p>
Plenty of people use their laptop computers like desktops by connecting
an external display, keyboard, and mouse. But Compal‚Äôs Hybrid AIO
concept PC goes a step further than that‚Ä¶ and then a few steps¬†further.
The computer design and manufacturing company recently won an IF Design
Award for this unusual modular computer that can be used [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/compal-hybrid-aio-concept-pc-is-a-laptop-tablet-and-dual-screen-desktop/">Compal
Hybrid AIO concept PC is a laptop, tablet, and dual-screen desktop</a>
appeared first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/compal-hybrid-aio-concept-pc-is-a-laptop-tablet-and-dual-screen-desktop/"
class="uri">https://liliputing.com/compal-hybrid-aio-concept-pc-is-a-laptop-tablet-and-dual-screen-desktop/</a></p>
<hr />
<h2 id="why-you-dont-need-to-be-the-best-to-lead-a-remarkable-life">Why
You Don‚Äôt Need to Be the Best to Lead a Remarkable Life</h2>
<p>date: 2025-03-12, from: Guy Kawasaki blog</p>
<p>The joy of being average.</p>
<p><br></p>
<p><a
href="https://guykawasaki.substack.com/p/why-you-dont-need-to-be-the-best"
class="uri">https://guykawasaki.substack.com/p/why-you-dont-need-to-be-the-best</a></p>
<hr />
<h2
id="pretty-vile-spotify-removes-andrew-tate-pimping-hoes-class-after-employees-complain">‚ÄòPretty
Vile:‚Äô Spotify Removes Andrew Tate ‚ÄòPimping Hoes‚Äô Class After Employees
Complain</h2>
<p>date: 2025-03-12, from: 404 Media Group</p>
<p>Internal Spotify communications viewed by 404 Media show Spotify‚Äôs
own employees complaining about Andrew Tate‚Äôs course being available on
the platform.</p>
<p><br></p>
<p><a
href="https://www.404media.co/pretty-vile-spotify-removes-andrew-tate-pimping-hoes-class-after-employees-and-users-complain-2/"
class="uri">https://www.404media.co/pretty-vile-spotify-removes-andrew-tate-pimping-hoes-class-after-employees-and-users-complain-2/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>‚ÄúSocial media‚Äù is almost 100% bullshit and the rest of it is a con.
Discuss.</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/did:plc:oety7qbfx7x6exn2ytrwikmr/post/3lk63i5bd6s2r"
class="uri">https://bsky.app/profile/did:plc:oety7qbfx7x6exn2ytrwikmr/post/3lk63i5bd6s2r</a></p>
<hr />
<h2 id="the-200-sites-an-ice-surveillance-contractor-is-monitoring">The
200+ Sites an ICE Surveillance Contractor is Monitoring</h2>
<p>date: 2025-03-12, from: 404 Media Group</p>
<p>404 Media has obtained the list of sites and services that ICE
contractor ShadowDragon pulls data from. ShadowDragon sources data from
all over the web and lets government analysts easily search it and draw
connections between people.</p>
<p><br></p>
<p><a
href="https://www.404media.co/the-200-sites-an-ice-surveillance-contractor-is-monitoring/"
class="uri">https://www.404media.co/the-200-sites-an-ice-surveillance-contractor-is-monitoring/</a></p>
<hr />
<h2 id="podcast-were-not-ready-for-chinese-ai-video-generators">Podcast:
We‚Äôre Not Ready for Chinese AI Video Generators</h2>
<p>date: 2025-03-12, from: 404 Media Group</p>
<p>We chat the lack of guardrails on China-based AI video models; how
cops are using AI to summarize evidence from seized mobile phones; and
the AI game that is making $50,000 a month.</p>
<p><br></p>
<p><a
href="https://www.404media.co/podcast-were-not-ready-for-chinese-ai-video-generators/"
class="uri">https://www.404media.co/podcast-were-not-ready-for-chinese-ai-video-generators/</a></p>
<hr />
<h2 id="who-could-resist">Who Could Resist?</h2>
<p>date: 2025-03-12, updated: 2025-03-12, from: One Foot Tsunami</p>
<p><br></p>
<p><a href="https://onefoottsunami.com/2025/03/12/who-could-resist/"
class="uri">https://onefoottsunami.com/2025/03/12/who-could-resist/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-12, from: Feed for Alt USDS)</p>
<p>There are <em>actual</em> laws here, like the Sarbanes-Oxley Act and
following retention schedules outlined in the General Records Schedules
(GRS) and IRS Records Control Schedules. USAID can‚Äôt just shred
government documents.</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk6macazy22s"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk6macazy22s</a></p>
<hr />
<h2 id="a-preview-of-our-15th-presidential-library">A Preview of Our
15th Presidential Library</h2>
<p>date: 2025-03-12, from: National Archives, Pieces of History blog</p>
<p>Today‚Äôs post comes from Matthew Green, an archivist at the Donald J.
Trump Presidential Library in College Park, Maryland. The Library‚Äôs
records are closed to researchers until January 2026 but today we are
previewing some of the records in the Library‚Äôs collection. Established
in 2021, the Donald J. Trump Presidential Library currently administers
records from ‚Ä¶
<a href="https://prologue.blogs.archives.gov/2025/03/12/a-preview-of-our-15th-presidential-library/" class="more-link">Continue
reading <span class="screen-reader-text">A Preview of Our 15th
Presidential Library</span></a></p>
<p><br></p>
<p><a
href="https://prologue.blogs.archives.gov/2025/03/12/a-preview-of-our-15th-presidential-library/"
class="uri">https://prologue.blogs.archives.gov/2025/03/12/a-preview-of-our-15th-presidential-library/</a></p>
<hr />
<h2 id="a-mind-is-a-terrible-thing-to-lose">A Mind is a Terrible Thing
to Lose</h2>
<p>date: 2025-03-12, from: Paul Krugman</p>
<p>Especially when you‚Äôre running a country</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/a-mind-is-a-terrible-thing-to-lose"
class="uri">https://paulkrugman.substack.com/p/a-mind-is-a-terrible-thing-to-lose</a></p>
<hr />
<h2
id="raspberry-pi-wins-2024-europe-tsmc-trophy-for-embedded-computing-innovation">Raspberry
Pi wins 2024 Europe TSMC Trophy for embedded computing innovation</h2>
<p>date: 2025-03-12, from: Raspberry Pi News (.com)</p>
<p>
We‚Äôre proud to win the 2024 Europe TSMC Trophy, recognising our
innovation in embedded computing and focus on high-performance
solutions.
</p>
<p>
The post
<a href="https://www.raspberrypi.com/news/raspberry-pi-wins-2024-europe-tsmc-trophy-for-embedded-computing-innovation/">Raspberry
Pi wins 2024 Europe TSMC Trophy for embedded computing innovation</a>
appeared first on <a href="https://www.raspberrypi.com">Raspberry
Pi</a>.
</p>
<p><br></p>
<p><a
href="https://www.raspberrypi.com/news/raspberry-pi-wins-2024-europe-tsmc-trophy-for-embedded-computing-innovation/"
class="uri">https://www.raspberrypi.com/news/raspberry-pi-wins-2024-europe-tsmc-trophy-for-embedded-computing-innovation/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>How Simon Willison uses<span
style="letter-spacing: 0.01rem; -webkit-text-size-adjust: 100%;"> LLMs
to help</span><span
style="letter-spacing: 0.01rem; -webkit-text-size-adjust: 100%;"> write
code.</span></p>
<p><br></p>
<p><a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/"
class="uri">https://simonwillison.net/2025/Mar/11/using-llms-for-code/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>I am heavily invested in the stock market, and I totally don‚Äôt mind
if it crashes. I also own a Tesla, and ditto.</p>
<p><br></p>
<p><a
href="https://heathercoxrichardson.substack.com/p/march-11-2025?publication_id=20533&amp;post_id=158900485&amp;isFreemail=true&amp;r=w33x&amp;triedRedirect=true"
class="uri">https://heathercoxrichardson.substack.com/p/march-11-2025?publication_id=20533&amp;post_id=158900485&amp;isFreemail=true&amp;r=w33x&amp;triedRedirect=true</a></p>
<hr />
<h2 id="gell-mann-amnesia">Gell-Mann Amnesia</h2>
<p>date: 2025-03-12, from: Status-Q blog</p>
<p>My thanks to Kit Hodsdon, who, responding to yesterday‚Äôs post,
pointed out that there was a name for a phenomenon related to something
I discussed there: the Gell-Mann Amnesia Effect. This term was first
coined by Michael Crichton, and to quote the Wikipedia page linked
above, it describes ‚Äúthe tendency of individuals to critically assess
<a class="more-link excerpt-link" href="https://statusq.org/archives/2025/03/12/13098/">Continue
Reading<span class="glyphicon glyphicon-chevron-right"></span></a></p>
<p><br></p>
<p><a href="https://statusq.org/archives/2025/03/12/13098/"
class="uri">https://statusq.org/archives/2025/03/12/13098/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-12, from: Dave Winer‚Äôs linkblog)</p>
<p>Tulane University Responds to Federal Investigation Over Antisemitism
Complaints.</p>
<p><br></p>
<p><a
href="https://www.wwltv.com/article/news/education/us-department-of-education-threatens-tulane/289-2f714b9f-9302-45f1-970a-5e1681c51a1e"
class="uri">https://www.wwltv.com/article/news/education/us-department-of-education-threatens-tulane/289-2f714b9f-9302-45f1-970a-5e1681c51a1e</a></p>
<hr />
<h2 id="traversal-resistant-file-apis">Traversal-resistant file
APIs</h2>
<p>date: 2025-03-12, updated: 2025-03-12, from: Go language blog</p>
<p>New file access APIs in Go 1.24.</p>
<p><br></p>
<p><a href="https://go.dev/blog/osroot"
class="uri">https://go.dev/blog/osroot</a></p>
<hr />
<h2 id="decisions">Decisions</h2>
<p>date: 2025-03-11, from: Accidentally in Code</p>
<p>Recently, someone asked me for my ‚ÄúLeadership philosophy‚Äù. My initial
reaction was to panic, but after taking a deep breath and a bit of time
to think, I came up with this answer: ‚ÄúMy job is to make it easier for
people to make good decisions.‚Äù What does that mean? Firstly ‚Äì that my
job [‚Ä¶]</p>
<p><br></p>
<p><a href="https://cate.blog/2025/03/11/decisions-3/"
class="uri">https://cate.blog/2025/03/11/decisions-3/</a></p>
<hr />
<h2 id="lilbits-google-pixel-10-could-be-a-triple-camera-phone">Lilbits:
Google Pixel 10 could be a triple-camera phone</h2>
<p>date: 2025-03-11, from: Liliputing</p>
<p>
The folks at Android Headlines have published a series of articles
showing the anticipated design of the upcoming Google Pixel 10, Pixel 10
Pro, and Pixel 10 Pro XL. For the most part the Pro phones look¬†nearly
identical to their Pixel 9 counterparts. But there‚Äôs something different
about the entry-level Pixel 10: instead of two [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/lilbits-google-pixel-10-could-be-a-triple-camera-phone/">Lilbits:
Google Pixel 10 could be a triple-camera phone</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/lilbits-google-pixel-10-could-be-a-triple-camera-phone/"
class="uri">https://liliputing.com/lilbits-google-pixel-10-could-be-a-triple-camera-phone/</a></p>
<hr />
<h2 id="openai-agents-sdk">OpenAI Agents SDK</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://openai.github.io/openai-agents-python/">OpenAI
Agents SDK</a></strong>
</p>
OpenAI‚Äôs other big announcement today
(<a href="https://simonwillison.net/2025/Mar/11/responses-vs-chat-completions/">see
also</a>) - a Python library
(<a href="https://pypi.org/project/openai-agents/">openai-agents</a>)
for building ‚Äúagents‚Äù, which is a replacement for their previous
<a href="https://github.com/openai/swarm">swarm</a> research project.
</p>
<p>
In this project, an ‚Äúagent‚Äù is a class that configures an LLM with a
system prompt an access to specific tools.
</p>
<p>
An interesting concept in this one is the concept of
<strong><a href="https://openai.github.io/openai-agents-python/handoffs/">handoffs</a></strong>,
where one agent can chose to hand execution over to a different
system-prompt-plus-tools agent treating it almost like a tool itself.
This code example illustrates the idea:
</p>
<pre><span class="pl-k">from</span> <span class="pl-s1">agents</span> <span class="pl-k">import</span> <span class="pl-v">Agent</span>, <span class="pl-s1">handoff</span>

<span class="pl-s1">billing_agent</span> <span class="pl-c1">=</span> <span class="pl-en">Agent</span>(
    <span class="pl-s1">name</span><span class="pl-c1">=</span><span class="pl-s">"Billing agent"</span>
)
<span class="pl-s1">refund_agent</span> <span class="pl-c1">=</span> <span class="pl-en">Agent</span>(
    <span class="pl-s1">name</span><span class="pl-c1">=</span><span class="pl-s">"Refund agent"</span>
)
<span class="pl-s1">triage_agent</span> <span class="pl-c1">=</span> <span class="pl-en">Agent</span>(
    <span class="pl-s1">name</span><span class="pl-c1">=</span><span class="pl-s">"Triage agent"</span>,
    <span class="pl-s1">handoffs</span><span class="pl-c1">=</span>[<span class="pl-s1">billing_agent</span>, <span class="pl-en">handoff</span>(<span class="pl-s1">refund_agent</span>)]
)</pre>
<p>
<p>The library also includes
<a href="https://openai.github.io/openai-agents-python/guardrails/">guardrails</a>
- classes you can add that attempt to filter user input to make sure it
fits expected criteria. Bits of this look suspiciously like trying to
<a href="https://simonwillison.net/2022/Sep/17/prompt-injection-more-ai/">solve
AI security problems with more AI</a> to me.</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/python&quot;&gt;python&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-agents&quot;&gt;ai-agents&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-tool-use&quot;&gt;llm-tool-use&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/11/openai-agents-sdk/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/11/openai-agents-sdk/#atom-everything</a></p>
<hr />
<h2 id="openai-api-responses-vs.-chat-completions">OpenAI API: Responses
vs.¬†Chat Completions</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://platform.openai.com/docs/guides/responses-vs-chat-completions">OpenAI
API: Responses vs.¬†Chat Completions</a></strong>
</p>
OpenAI released a bunch of new API platform features this morning under
the headline
‚Äú<a href="https://openai.com/index/new-tools-for-building-agents/">New
tools for building agents</a>‚Äù (their somewhat mushy interpretation of
‚Äúagents‚Äù here is ‚Äúsystems that independently accomplish tasks on behalf
of users‚Äù).
</p>
<p>
A particularly significant change is the introduction of a new
<strong>Responses API</strong>, which is a slightly different shape from
the Chat Completions API that they‚Äôve offered for the past couple of
years and which others in the industry have widely cloned as an ad-hoc
standard.
</p>
<p>
In
<a href="https://platform.openai.com/docs/guides/responses-vs-chat-completions">this
guide</a> they illustrate the differences, with a reassuring note that:
</p>
<blockquote>
<p>
The Chat Completions API is an industry standard for building AI
applications, and we intend to continue supporting this API
indefinitely. We‚Äôre introducing the Responses API to simplify workflows
involving tool use, code execution, and state management. We believe
this new API primitive will allow us to more effectively enhance the
OpenAI platform into the future.
</p>
</blockquote>
<p>
An API that <em>is</em> going away is the
<a href="https://platform.openai.com/docs/api-reference/assistants">Assistants
API</a>, a perpetual beta first launched at OpenAI DevDay in 2023. The
new responses API solves effectively the same problems but better, and
assistants will be sunset ‚Äúin the first half of 2026‚Äù.
</p>
<p>
The best illustration I‚Äôve seen of the differences between the two is
this
<a href="https://github.com/openai/openai-python/commit/2954945ecc185259cfd7cd33c8cbc818a88e4e1b">giant
commit</a> to the <code>openai-python</code> GitHub repository updating
ALL of the example code in one go.
</p>
<p>
The most important feature of the Responses API (a feature it shares
with the old Assistants API) is that it can manage conversation state on
the server for you. An oddity of the Chat Completions API is that you
need to maintain your own records of the current conversation, sending
back full copies of it with each new prompt. You end up making API calls
that look like this (from
<a href="https://platform.openai.com/docs/guides/conversation-state?api-mode=chat&amp;lang=javascript#manually-manage-conversation-state">their
examples</a>):
</p>
<div class="highlight highlight-source-json">
<pre>{
    <span class="pl-ent">"model"</span>: <span class="pl-s"><span class="pl-pds">"</span>gpt-4o-mini<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"messages"</span>: [
        {
            <span class="pl-ent">"role"</span>: <span class="pl-s"><span class="pl-pds">"</span>user<span class="pl-pds">"</span></span>,
            <span class="pl-ent">"content"</span>: <span class="pl-s"><span class="pl-pds">"</span>knock knock.<span class="pl-pds">"</span></span>,
        },
        {
            <span class="pl-ent">"role"</span>: <span class="pl-s"><span class="pl-pds">"</span>assistant<span class="pl-pds">"</span></span>,
            <span class="pl-ent">"content"</span>: <span class="pl-s"><span class="pl-pds">"</span>Who's there?<span class="pl-pds">"</span></span>,
        },
        {
            <span class="pl-ent">"role"</span>: <span class="pl-s"><span class="pl-pds">"</span>user<span class="pl-pds">"</span></span>,
            <span class="pl-ent">"content"</span>: <span class="pl-s"><span class="pl-pds">"</span>Orange.<span class="pl-pds">"</span></span>
        }
    ]
}</pre>
</div>
<p>
These can get long and unwieldy - especially when attachments such as
images are involved - but the real challenge is when you start
integrating tools: in a conversation with tool use you‚Äôll need to
maintain that full state <em>and</em> drop messages in that show the
output of the tools the model requested. It‚Äôs not a trivial thing to
work with.
</p>
<p>
The new Responses API continues to support this list of messages format,
but you also get the option to outsource that to OpenAI entirely: you
can add a new <code>‚Äústore‚Äù: true</code> property and then in subsequent
messages include a <code>‚Äúprevious_response_id: response_id</code> key
to continue that conversation.
</p>
<p>
This feels a whole lot more natural than the Assistants API, which
required you to think in terms of
<a href="https://platform.openai.com/docs/assistants/overview#objects">threads,
messages and runs</a> to achieve the same effect.
</p>
<p>
Also fun: the Response API
<a href="https://twitter.com/athyuttamre/status/1899541484308971822">supports
HTML form encoding</a> now in addition to JSON:
</p>
<pre><code>curl https://api.openai.com/v1/responses \
  -u :$OPENAI_API_KEY \
  -d model="gpt-4o" \
  -d input="What is the capital of France?"
</code></pre>
<p>
I found that in an excellent
<a href="https://twitter.com/athyuttamre/status/1899541471532867821">Twitter
thread</a> providing background on the design decisions in the new API
from OpenAI‚Äôs Atty Eleti. Here‚Äôs
<a href="https://nitter.net/athyuttamre/status/1899541471532867821">a
nitter link</a> for people who don‚Äôt have a Twitter account.
</p>
<h4>
New built-in tools
</h4>
<p>
A potentially more exciting change today is the introduction of default
tools that you can request while using the new Responses API. There are
three of these, all of which can be specified in the <code>‚Äútools‚Äù:
[‚Ä¶]</code> array.
</p>
<ul>
<li>
<code>{‚Äútype‚Äù: ‚Äúweb_search_preview‚Äù}</code> - the same search feature
available through ChatGPT. The documentation doesn‚Äôt clarify which
underlying search engine is used - I initially assumed Bing, but the
tool documentation links to this
<a href="https://platform.openai.com/docs/bots">Overview of OpenAI
Crawlers</a> page so maybe it‚Äôs entirely in-house now? Web search
<a href="https://platform.openai.com/docs/pricing#web-search">is
priced</a> at between $25 and $50 per thousand queries depending on if
you‚Äôre using GPT-4o or GPT-4o mini and the configurable size of your
‚Äúsearch context‚Äù.
</li>
<li>
<code>{‚Äútype‚Äù: ‚Äúfile_search‚Äù, ‚Äúvector_store_ids‚Äù: [‚Ä¶]}</code> provides
integration with the latest version of their
<a href="https://platform.openai.com/docs/guides/tools-file-search">file
search</a> vector store, mainly used for RAG. ‚ÄúUsage is priced‚Å† at $2.50
per thousand queries and file storage at $0.10/GB/day, with the first GB
free‚Äù.
</li>
<li>
<code>{‚Äútype‚Äù: ‚Äúcomputer_use_preview‚Äù, ‚Äúdisplay_width‚Äù: 1024,
‚Äúdisplay_height‚Äù: 768, ‚Äúenvironment‚Äù: ‚Äúbrowser‚Äù}</code> is the most
surprising to me: it‚Äôs tool access to the
<a href="https://openai.com/index/computer-using-agent/">Computer-Using
Agent</a> system they built for their Operator product. This one is
going to be <em>a lot</em> of fun to explore. The tool‚Äôs documentation
includes a warning
<a href="https://platform.openai.com/docs/guides/tools-computer-use#beware-of-prompt-injections">about
prompt injection risks</a>. Though on closer inspection I think this may
work more like
<a href="https://simonwillison.net/2024/Oct/22/computer-use/">Claude
Computer Use</a>, where you have to
<a href="https://platform.openai.com/docs/guides/tools-computer-use#setting-up-your-environment">run
the sandboxed environment yourself</a> rather than outsource that
difficult part to them.
</li>
</ul>
<p>
<p>I‚Äôm still thinking through how to expose these new features in my
<a href="https://llm.datasette.io/">LLM</a> tool, which is made harder
by the fact that a number of plugins now rely on the default OpenAI
implementation from core, which is currently built on top of Chat
Completions. I‚Äôve been worrying for a while about the impact of our
entire industry building clones of one proprietary API that might change
in the future, I guess now we get to see how that shakes out!</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/chatgpt&quot;&gt;chatgpt&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/apis&quot;&gt;apis&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-agents&quot;&gt;ai-agents&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-tool-use&quot;&gt;llm-tool-use&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm&quot;&gt;llm&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/rag&quot;&gt;rag&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/11/responses-vs-chat-completions/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/11/responses-vs-chat-completions/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>Really enjoyed this Fresh air interview with comedian Bill Burr.</p>
<p><br></p>
<p><a href="https://m.youtube.com/watch?v=hXHsmnOrWAk"
class="uri">https://m.youtube.com/watch?v=hXHsmnOrWAk</a></p>
<hr />
<h2 id="macos-15.3.2">macOS 15.3.2</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Juli Clover (release notes, security, no enterprise, no developer,
full installer/M4, IPSW/M4): According to Apple‚Äôs release notes, macOS
Sequoia 15.3.2 includes important bug fixes and security updates, and it
is recommended for all users. See also: Mr.¬†Macintosh and Howard Oakley.
Previously: macOS 15.3.1 Update (2025-03-12): Jeff Johnson: The use of
the plural ‚Äúsecurity fixes‚Äù [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/11/macos-15-3-2/"
class="uri">https://mjtsai.com/blog/2025/03/11/macos-15-3-2/</a></p>
<hr />
<h2 id="ios-18.3.2-and-ipados-18.3.2">iOS 18.3.2 and iPadOS 18.3.2</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Juli Clover (no iOS/iPadOS release notes, security, no enterprise, no
developer): According to Apple‚Äôs release notes, the iOS 18.3.2 update
adds important bug fixes and security updates. On some devices, it fixes
an issue that could prevent playback of some streaming content.
Previously: iOS 18.3.1 and iPadOS 18.3.1 Update (2025-03-12): Juli
Clover: After installing iOS [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/11/ios-18-3-2-and-ipados-18-3-2/"
class="uri">https://mjtsai.com/blog/2025/03/11/ios-18-3-2-and-ipados-18-3-2/</a></p>
<hr />
<h2 id="visionos-2.3.2">visionOS 2.3.2</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Juli Clover (no release notes, security, no developer, no
enterprise): According to Apple‚Äôs release notes, visionOS 2.3.2 provides
important bug fixes, security updates, and addresses an issue that may
prevent playback of some streaming content. Previously: visionOS
2.3.1</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/11/visionos-2-3-2/"
class="uri">https://mjtsai.com/blog/2025/03/11/visionos-2-3-2/</a></p>
<hr />
<h2 id="tvos-18.3.1">tvOS 18.3.1</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Juli Clover (release notes, no security, no developer): This update
addresses an issue that may prevent playback of some streaming content
on Apple TV 4K (3rd generation). Previously: tvOS 18.3</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/11/tvos-18-3-1/"
class="uri">https://mjtsai.com/blog/2025/03/11/tvos-18-3-1/</a></p>
<hr />
<h2 id="quoting-ryan-cavanaugh">Quoting Ryan Cavanaugh</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://github.com/microsoft/typescript-go/discussions/411">
<p>
Languages that allow for a structurally similar codebase offer a
significant boon for anyone making code changes because we can easily
port changes between the two codebases. In contrast, languages that
require fundamental rethinking of memory management, mutation, data
structuring, polymorphism, laziness, etc., might be a better fit for a
ground-up rewrite, but we‚Äôre undertaking this more as a <em>port</em>
that maintains the existing behavior and critical optimizations we‚Äôve
built into the language. Idiomatic Go strongly resembles the existing
coding patterns of the TypeScript codebase, which makes this porting
effort much more tractable.
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://github.com/microsoft/typescript-go/discussions/411">Ryan
Cavanaugh</a>, on why TypeScript chose to rewrite in Go, not Rust
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/typescript&quot;&gt;typescript&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/go&quot;&gt;go&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/rust&quot;&gt;rust&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/11/ryan-cavanaugh/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/11/ryan-cavanaugh/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="IIIF">@IIIF</span>
Mastodon feed</strong> (date: 2025-03-11, from: IIIF Mastodon feed)</p>
<p>
The preliminary program for the 2025
<a href="https://glammr.us/tags/IIIF" class="mention hashtag" rel="tag">#<span>IIIF</span></a>
Annual Conference is now live!
</p>
<p>
Take a peek at the lightning talks, presentations, workshops, and more:
iiif.io/event/2025/l‚Ä¶
</p>
<p>
Register here: www.conftool.org/iiif2025/
</p>
<p><br></p>
<p><a href="https://glammr.us/@IIIF/114145442337466248"
class="uri">https://glammr.us/@IIIF/114145442337466248</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>Was Mr Bill part of the SNL50 reunion? I must‚Äôve missed him! üòÄ</p>
<p><br></p>
<p><a href="https://m.youtube.com/watch?v=M0F3JBzAtE4"
class="uri">https://m.youtube.com/watch?v=M0F3JBzAtE4</a></p>
<hr />
<h2 id="shaking-the-wasps-nest">Shaking The Wasp‚Äôs Nest</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Tedium site</p>
<p>Looking back at Gamergate, a flash point in internet history, and
what it has to tell us about our current moment.</p>
<p><br></p>
<p><a
href="https://feed.tedium.co/link/15204/16981378/gamergate-lessons-internet-culture"
class="uri">https://feed.tedium.co/link/15204/16981378/gamergate-lessons-internet-culture</a></p>
<hr />
<h2 id="speaking-things-into-existence">Speaking things into
existence</h2>
<p>date: 2025-03-11, from: One Useful Thing</p>
<p>Expertise in a vibe-filled world of work</p>
<p><br></p>
<p><a
href="https://www.oneusefulthing.org/p/speaking-things-into-existence"
class="uri">https://www.oneusefulthing.org/p/speaking-things-into-existence</a></p>
<hr />
<h2
id="announcing-bacalhau-v1.6.5-networking-updates-usability-improvements">Announcing
Bacalhau v1.6.5: Networking Updates &amp; Usability Improvements</h2>
<p>date: 2025-03-11, from: Bacalhau Blog</p>
<p>(2:22) Bacalhau v1.6.5 adds networking opt-out options and usability
fixes ahead of v1.7‚Äôs default networking shift.</p>
<p><br></p>
<p><a
href="https://blog.bacalhau.org/p/announcing-bacalhau-v165-networking"
class="uri">https://blog.bacalhau.org/p/announcing-bacalhau-v165-networking</a></p>
<hr />
<h2
id="ikoolcore-r2-max-review-using-this-mini-pc-a-network-or-storage-appliance">iKOOLCore
R2 Max Review: Using this mini PC a network or storage appliance</h2>
<p>date: 2025-03-11, from: Liliputing</p>
<p>
The iKOOLCore R2 Max is a mini PC that launched late last year as a
small, versatile computer designed for use as a networking appliance or
general purpose computer. It supports up to an Intel Core 3 N355 Twin
Lake processor and 32GB of RAM. While the iKOOLCORE R2 I reviewed in
2023 was a [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/ikoolcore-r2-max-review-using-this-mini-pc-a-network-or-storage-appliance/">iKOOLCore
R2 Max Review: Using this mini PC a network or storage appliance</a>
appeared first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/ikoolcore-r2-max-review-using-this-mini-pc-a-network-or-storage-appliance/"
class="uri">https://liliputing.com/ikoolcore-r2-max-review-using-this-mini-pc-a-network-or-storage-appliance/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>Pocket Casts‚Äô Web Player Is Now Available to All.</p>
<p><br></p>
<p><a href="https://blog.pocketcasts.com/2025/03/11/webplayer/"
class="uri">https://blog.pocketcasts.com/2025/03/11/webplayer/</a></p>
<hr />
<h2
id="this-liquid-cooled-laptop-is-a-portable-pc-with-a-desktop-cpu-and-up-to-rtx-5090-graphics-crowdfunding">This
liquid-cooled ‚Äúlaptop‚Äù is a portable PC with a desktop CPU and up to RTX
5090 graphics (crowdfunding)</h2>
<p>date: 2025-03-11, from: Liliputing</p>
<p>
Laptop computers have gotten pretty powerful in recent years, but
desktops still have a few advantages. Want a user-replaceable CPU or
GPU? With a few notable exceptions that‚Äôs not possible with laptops, but
it‚Äôs pretty standard for desktops. And if you want to prioritize
performance over battery life or portability, desktop hardware is
usually the [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/this-liquid-cooled-laptop-is-a-portable-pc-with-a-desktop-cpu-and-up-to-rtx-5090-graphics-crowdfunding/">This
liquid-cooled ‚Äúlaptop‚Äù is a portable PC with a desktop CPU and up to RTX
5090 graphics (crowdfunding)</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/this-liquid-cooled-laptop-is-a-portable-pc-with-a-desktop-cpu-and-up-to-rtx-5090-graphics-crowdfunding/"
class="uri">https://liliputing.com/this-liquid-cooled-laptop-is-a-portable-pc-with-a-desktop-cpu-and-up-to-rtx-5090-graphics-crowdfunding/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>People who believe there is magic that makes Bluesky
billionaire-proof are misled. If they sold out you and I would have no
place to turn. It would be a replay of the mess with Twitter.</p>
<p><br></p>
<p><a
href="http://scripting.com/2025/03/11/113838.html?title=blueskyIsNotBillionaireproof"
class="uri">http://scripting.com/2025/03/11/113838.html?title=blueskyIsNotBillionaireproof</a></p>
<hr />
<h2 id="live-with-andy-borowitz">Live with Andy Borowitz</h2>
<p>date: 2025-03-11, from: Paul Krugman</p>
<p>A recording from Paul Krugman and Andy Borowitz‚Äôs live video</p>
<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://api.substack.com/feed/podcast/158852892/60fc9da1d0d8db5193d695d82dc507e0.mp3">
</source>
</audio>
<p><a href="https://api.substack.com/feed/podcast/158852892/60fc9da1d0d8db5193d695d82dc507e0.mp3" target="_blank">download
audio/mpeg</a><br></p>
<p><a href="https://paulkrugman.substack.com/p/live-with-andy-borowitz"
class="uri">https://paulkrugman.substack.com/p/live-with-andy-borowitz</a></p>
<hr />
<h2
id="more-than-samoas-thin-mints-and-tagalongs-properties-about-girl-scouts-in-the-national-register-of-historic-places">More
than Samoas, Thin Mints, and Tagalongs! Properties about Girl Scouts in
the National Register of Historic Places</h2>
<p>date: 2025-03-11, from: National Archives, Text Message blog</p>
<p>Tomorrow, March 12, is Girl Scout Day, which commemorates the
founding of the Girl Scouts of the USA, when Juliette Gordon Low
officially registered the organization‚Äôs first 18 members in Savannah,
Georgia.¬† There are a number of properties in the National Register that
are linked to the Girl Scouts, including the Juliette Gordon Low
Historic ‚Ä¶
<a href="https://text-message.blogs.archives.gov/2025/03/11/more-than-samoas-thin-mints-and-tagalongs-properties-about-girl-scouts-in-the-national-register-of-historic-places/" class="more-link">Continue
reading <span class="screen-reader-text">More than Samoas, Thin Mints,
and Tagalongs! Properties about Girl Scouts in the National Register of
Historic Places</span></a></p>
<p><br></p>
<p><a
href="https://text-message.blogs.archives.gov/2025/03/11/more-than-samoas-thin-mints-and-tagalongs-properties-about-girl-scouts-in-the-national-register-of-historic-places/"
class="uri">https://text-message.blogs.archives.gov/2025/03/11/more-than-samoas-thin-mints-and-tagalongs-properties-about-girl-scouts-in-the-national-register-of-historic-places/</a></p>
<hr />
<h2 id="educations-hidden-hero">Education‚Äôs Hidden Hero</h2>
<p>date: 2025-03-11, from: Guy Kawasaki blog</p>
<p>Valerie Hill-Jackson, Texas A&amp;M University.</p>
<p><br></p>
<p><a href="https://guykawasaki.substack.com/p/educations-hidden-hero"
class="uri">https://guykawasaki.substack.com/p/educations-hidden-hero</a></p>
<hr />
<h2 id="very-artificial-intelligence">Very Artificial Intelligence</h2>
<p>date: 2025-03-11, from: Status-Q blog</p>
<p>Regular readers might assume that I spurn all things AI-related, and
this is not the case. I do use and occasionally derive benefit from the
tools that tend to come under this all-embracing phrase du jour. But it
does sometimes seem as if, for general questions, the LLMs can throw up
wrong answers as often
<a class="more-link excerpt-link" href="https://statusq.org/archives/2025/03/11/13089/">Continue
Reading<span class="glyphicon glyphicon-chevron-right"></span></a></p>
<p><br></p>
<p><a href="https://statusq.org/archives/2025/03/11/13089/"
class="uri">https://statusq.org/archives/2025/03/11/13089/</a></p>
<hr />
<h2
id="this-handheld-gaming-pc-has-a-big-screen-detachable-controllers-and-a-slow-processor">This
handheld gaming PC has a big screen, detachable controllers, and a slow
processor</h2>
<p>date: 2025-03-11, from: Liliputing</p>
<p>
Handheld gaming PCs have come a long way in recent years thanks to
advances in mobile chips with high-performance integrated graphics
capable of delivering decent frame rates for many recent games. But
handhelds with the best processors can be pricey. A new models sold by
several Chinese PC brands takes a different approach. It‚Äôs basically [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/this-handheld-gaming-pc-has-a-big-screen-detachable-controllers-and-a-slow-processor/">This
handheld gaming PC has a big screen, detachable controllers, and a slow
processor</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/this-handheld-gaming-pc-has-a-big-screen-detachable-controllers-and-a-slow-processor/"
class="uri">https://liliputing.com/this-handheld-gaming-pc-has-a-big-screen-detachable-controllers-and-a-slow-processor/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Robert">@Robert</span>‚Äôs
feed at BlueSky</strong> (date: 2025-03-11, from: Robert‚Äôs feed at
BlueSky)</p>
<p>Time to start planning a national general strike.</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/rsdoiel.bsky.social/post/3lk4ekta2ws2w"
class="uri">https://bsky.app/profile/rsdoiel.bsky.social/post/3lk4ekta2ws2w</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-11, from: Feed for Alt USDS)</p>
<p>Couldn‚Äôt have said it better ourselves. #WeTheBuilders</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk4dbtzi522l"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk4dbtzi522l</a></p>
<hr />
<h2 id="heres-how-i-use-llms-to-help-me-write-code">Here‚Äôs how I use
LLMs to help me write code</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Simon Willison‚Äôs
Weblog</p>
<p>
Online discussions about
<a href="https://simonwillison.net/tags/ai-assisted-programming/">using
Large Language Models to help write code</a> inevitably produce comments
from developers who‚Äôs experiences have been disappointing. They often
ask what they‚Äôre doing wrong - how come some people are reporting such
great results when their own experiments have proved lacking?
</p>
<p>
Using LLMs to write code is <strong>difficult</strong> and
<strong>unintuitive</strong>. It takes significant effort to figure out
the sharp and soft edges of using them in this way, and there‚Äôs precious
little guidance to help people figure out how best to apply them.
</p>
<p>
If someone tells you that coding with LLMs is <em>easy</em> they are
(probably unintentionally) misleading you. They may well have stumbled
on to patterns that work, but those patterns do not come naturally to
everyone.
</p>
<p>
I‚Äôve been getting great results out of LLMs for code for over two years
now. Here‚Äôs my attempt at transferring some of that experience and
intution to you.
</p>
<ul>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#set-reasonable-expectations">Set
reasonable expectations</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#account-for-training-cut-off-dates">Account
for training cut-off dates</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#context-is-king">Context
is king</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#ask-them-for-options">Ask
them for options</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#tell-them-exactly-what-to-do">Tell
them exactly what to do</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#you-have-to-test-what-it-writes-">You
have to test what it writes!</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#remember-it-s-a-conversation">Remember
it‚Äôs a conversation</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#use-tools-that-can-run-the-code-for-you">Use
tools that can run the code for you</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#vibe-coding-is-a-great-way-to-learn">Vibe-coding
is a great way to learn</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#a-detailed-example">A
detailed example using Claude Code</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#be-ready-for-the-human-to-take-over">Be
ready for the human to take over</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#the-biggest-advantage-is-speed-of-development">The
biggest advantage is speed of development</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#llms-amplify-existing-expertise">LLMs
amplify existing expertise</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#bonus-answering-questions-about-codebases">Bonus:
answering questions about codebases</a>
</li>
</ul>
<h4 id="set-reasonable-expectations">
Set reasonable expectations
</h4>
<p>
Ignore the ‚ÄúAGI‚Äù hype - LLMs are still fancy autocomplete. All they do
is predict a sequence of tokens - but it turns out writing code is
mostly about stringing tokens together in the right order, so they can
be <em>extremely</em> useful for this provided you point them in the
right direction.
</p>
<p>
If you assume that this technology will implement your project perfectly
without you needing to exercise any of your own skill you‚Äôll quickly be
disappointed.
</p>
<p>
Instead, use them to <em>augment</em> your abilities. My current
favorite mental model is to think of them as an over-confident pair
programming assistant who‚Äôs lightning fast at looking things up, can
churn out relevant examples at a moment‚Äôs notice and can execute on
tedious tasks without complaint.
</p>
<p>
<strong>Over-confident</strong> is important. They‚Äôll absolutely make
mistakes - sometimes subtle, sometimes huge. These mistakes can be
<a href="https://simonwillison.net/2025/Mar/2/kellan-elliott-mccrea/">deeply
inhuman</a> - if a human collaborator hallucinated a non-existent
library or method you would instantly lose trust in them. Don‚Äôt fall
into the trap of anthropomorphizing LLMs and assuming that failures
which would discredit a human should discredit the machine in the same
way.
</p>
<p>
When working with LLMs you‚Äôll often find things that they just cannot
do. Make a note of these - they are useful lessons! They‚Äôre also
valuable examples to stash away for the future - a sign of a strong new
model is when it produces usable results for a task that previous models
had been unable to handle.
</p>
<h4 id="account-for-training-cut-off-dates">
Account for training cut-off dates
</h4>
<p>
A crucial characteristic of any model is its <strong>training cut-off
date</strong>. This is the date at which the data they were trained on
stopped being collected. For OpenAI‚Äôs models this is usually October of
2023. Anthropic and Gemini and other providers may have more recent
dates.
</p>
<p>
This is <em>extremely</em> important for code, because it influences
what libraries they will be familiar with. If the library you are using
had a major breaking change since October 2023, OpenAI models won‚Äôt know
about it!
</p>
<p>
I gain enough value from LLMs that I now deliberately consider this when
picking a library - I try to stick with libraries with good stability
and that are popular enough that many examples of them will have made it
into the training data. I like applying the principles of
<a href="https://boringtechnology.club/">boring technology</a> -
innovate on your project‚Äôs unique selling points, stick with tried and
tested solutions for everything else.
</p>
<p>
LLMs can still help you work with libraries that exist outside their
training data, but you need to put in more work - you‚Äôll need to feed
them recent examples of how those libraries should be used as part of
your prompt.
</p>
<p>
This brings us to the most important thing to understand when working
with LLMs:
</p>
<h4 id="context-is-king">
Context is king
</h4>
<p>
Most of the craft of getting good results out of an LLM comes down to
managing its context - the text that is part of your current
conversation.
</p>
<p>
This context isn‚Äôt just the prompt that you have fed it: successful LLM
interactions usually take the form of conversations, and the context
consists of every message from you <em>and</em> every reply from the LLM
that exist in the current conversation thread.
</p>
<p>
When you start a new conversation you reset that context back to zero.
This is important to know, as often the fix for a conversation that has
stopped being useful is to wipe the slate clean and start again.
</p>
<p>
Some LLM coding tools go beyond just the conversation. Claude Projects
for example allow you to pre-populate the context with quite a large
amount of text - including a recent ability to
<a href="https://support.anthropic.com/en/articles/10167454-using-the-github-integration">import
code directly from a GitHub</a> repository which I‚Äôm using a
<em>lot</em>.
</p>
<p>
Tools like Cursor and VS Code Copilot include context from your current
editor session and file layout automatically, and you can sometimes use
mechanisms like
<a href="https://docs.cursor.com/context/@-symbols/overview">Cursor‚Äôs
<span class="citation" data-cites="commands">@commands</span></a> to
pull in additional files or documentation.
</p>
<p>
One of the reasons I mostly work directly with the
<a href="https://chatgpt.com/">ChatGPT</a> and
<a href="https://claude.ai/">Claude</a> web or app interfaces is that it
makes it easier for me to understand exactly what is going into the
context. LLM tools that obscure that context from me are <em>less</em>
effective.
</p>
<p>
You can use the fact that previous replies are also part of the context
to your advantage. For complex coding tasks try getting the LLM to write
a simpler version first, check that it works and then iterate on
building to the more sophisticated implementation.
</p>
<p>
I often start a new chat by dumping in existing code to seed that
context, then work with the LLM to modify it in some way.
</p>
<p>
One of my favorite code prompting techniques is to drop in several full
examples relating to something I want to build, then prompt the LLM to
use them as inspiration for a new project. I wrote about that in detail
when I
<a href="https://simonwillison.net/2024/Mar/30/ocr-pdfs-images/">described
my JavaScript OCR application</a> that combines Tesseract.js and PDF.js
- two libraries I had used in the past and for which I could provide
working examples in the prompt.
</p>
<h4 id="ask-them-for-options">
Ask them for options
</h4>
<p>
Most of my projects start with some open questions: is the thing I‚Äôm
trying to do possible? What are the potential ways I could implement it?
Which of those options are the <em>best</em>?
</p>
<p>
I use LLMs as part of this initial research phase.
</p>
<p>
I‚Äôll use prompts like ‚Äúwhat are options for HTTP libraries in Rust?
Include usage examples‚Äù - or ‚Äúwhat are some useful drag-and-drop
libraries in JavaScript? Build me an artifact demonstrating each one‚Äù
(to Claude).
</p>
<p>
The training cut-off is relevant here, since it means newer libraries
won‚Äôt be suggested. Usually that‚Äôs OK - I don‚Äôt want the latest, I want
the most stable and the one that has been around for long enough for the
bugs to be ironed out.
</p>
<p>
If I‚Äôm going to use something more recent I‚Äôll do that research myself,
outside of LLM world.
</p>
<p>
The best way to start any project is with a prototype that proves that
the key requirements of that project can be met. I often find that an
LLM can get me to that working prototype within a few minutes of me
sitting down with my laptop - or sometimes even while working on my
phone.
</p>
<h4 id="tell-them-exactly-what-to-do">
Tell them exactly what to do
</h4>
<p>
Once I‚Äôve completed the initial research I change modes dramatically.
For production code my LLM usage is much more authoritarian: I treat it
like a digital intern, hired to type code for me based on my detailed
instructions.
</p>
<p>
Here‚Äôs a recent example:
</p>
<blockquote>
<p>
Write a Python function that uses asyncio httpx with this signature:
</p>
<pre><code>async def download_db(url, max_size_bytes=5 * 1025 * 1025): -&gt; pathlib.Path
</code></pre>
<p>
Given a URL, this downloads the database to a temp directory and returns
a path to it. BUT it checks the content length header at the start of
streaming back that data and, if it‚Äôs more than the limit, raises an
error. When the download finishes it uses
<code>sqlite3.connect(‚Ä¶)</code> and then runs a <code>PRAGMA
quick_check</code> to confirm the SQLite data is valid - raising an
error if not. Finally, if the content length header lies to us - if it
says 2MB but we download 3MB - we get an error raised as soon as we
notice that problem.
</p>
</blockquote>
<p>
I could write this function myself, but it would take me the better part
of fifteen minutes to look up all of the details and get the code
working right. Claude knocked it out
<a href="https://gist.github.com/simonw/5aed8bd87016c77465c23e0dc4563ec9">in
15 seconds</a>.
</p>
<p>
I find LLMs respond extremely well to function signatures like the one I
use here. I get to act as the function designer, the LLM does the work
of building the body to my specification.
</p>
<p>
I‚Äôll often follow-up with ‚ÄúNow write me the tests using pytest‚Äù. Again,
I dictate my technology of choice - I want the LLM to save me the time
of having to type out the code that‚Äôs sitting in my head already.
</p>
<p>
If your reaction to this is ‚Äúsurely typing out the code is faster than
typing out an English instruction of it‚Äù, all I can tell you is that it
really isn‚Äôt for me any more. Code needs to be correct. English has
enormous room for shortcuts, and vagaries, and typos, and saying things
like ‚Äúuse that popular HTTP library‚Äù if you can‚Äôt remember the name off
the top of your head.
</p>
<p>
The good coding LLMs are excellent at filling in the gaps. They‚Äôre also
much less lazy than me - they‚Äôll remember to catch likely exceptions,
add accurate docstrings, and annotate code with the relevant types.
</p>
<h4 id="you-have-to-test-what-it-writes-">
You have to test what it writes!
</h4>
<p>
I wrote about this
<a href="https://simonwillison.net/2025/Mar/2/hallucinations-in-code/#qa">at
length last week</a>: the one thing you absolutely cannot outsource to
the machine is testing that the code actually works.
</p>
<p>
Your responsibility as a software developer is to deliver working
systems. If you haven‚Äôt seen it run, it‚Äôs not a working system. You need
to invest in strengthening those manual QA habits.
</p>
<p>
This may not be glamorous but it‚Äôs always been a critical part of
shipping good code, with or without the involvement of LLMs.
</p>
<h4 id="remember-it-s-a-conversation">
Remember it‚Äôs a conversation
</h4>
<p>
If I don‚Äôt like what an LLM has written, they‚Äôll <em>never</em> complain
at being told to refactor it! ‚ÄúBreak that repetitive code out into a
function‚Äù, ‚Äúuse string manipulation methods rather than a regular
expression‚Äù, or even ‚Äúwrite that better!‚Äù - the code an LLM produces
first time is rarely the final implementation, but they can re-type it
dozens of times for you without ever getting frustrated or bored.
</p>
<p>
Occasionally I‚Äôll get a great result from my first prompt - more
frequently the more I practice - but I expect to need at least a few
follow-ups.
</p>
<p>
I often wonder if this is one of the key tricks that people are missing
- a bad initial result isn‚Äôt a failure, it‚Äôs a starting point for
pushing the model in the direction of the thing you actually want.
</p>
<h4 id="use-tools-that-can-run-the-code-for-you">
Use tools that can run the code for you
</h4>
<p>
An increasing number of LLM coding tools now have the ability to <em>run
that code</em> for you. I‚Äôm slightly cautious about some of these since
there‚Äôs a possibility of the wrong command causing real damage, so I
tend to stick to the ones that run code in a safe sandbox. My favorites
right now are:
</p>
<ul>
<li>
<strong>ChatGPT Code Interpreter</strong>, where ChatGPT can write and
then execute Python code directly in a Kubernetes sandbox VM managed by
OpenAI. This is completely safe - it can‚Äôt even make outbound network
connections so really all that can happen is the temporary filesystem
gets mangled and then reset.
</li>
<li>
<strong>Claude Artifacts</strong>, where Claude can build you a full
HTML+JavaScript+CSS web application that is displayed within the Claude
interface. This web app is displayed in a <em>very</em> locked down
iframe sandbox, greatly restricting what it can do but preventing
problems like accidental exfiltration of your private Claude data.
</li>
<li>
<strong>ChatGPT Canvas</strong> is a newer ChatGPT feature with similar
capabilites to Claude Artifacts. I have not explored this enough myself
yet.
</li>
</ul>
<p>
And if you‚Äôre willing to live a little more dangerously:
</p>
<ul>
<li>
<strong><a href="https://www.cursor.com/">Cursor</a></strong> has an
‚ÄúAgent‚Äù feature that can do this, as does
<strong><a href="https://codeium.com/windsurf">Windsurf</a></strong> and
a growing number of other editors. I haven‚Äôt spent enough time with
these to make recommendations yet.
</li>
<li>
<strong><a href="https://aider.chat/">Aider</a></strong> is the leading
open source implementation of these kinds of patterns, and is a great
example of
<a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">dogfooding</a>
- recent releases of Aider have been
<a href="https://aider.chat/HISTORY.html">80%+ written</a> by Aider
itself.
</li>
<li>
<strong><a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude
Code</a></strong> is Anthropic‚Äôs new entrant into this space. I‚Äôll
provide a detailed description of using that tool shortly.
</li>
</ul>
<p>
This run-the-code-in-a-loop pattern is so powerful that I chose my core
LLM tools for coding based primarily on whether they can safely run and
iterate on my code.
</p>
<h4 id="vibe-coding-is-a-great-way-to-learn">
Vibe-coding is a great way to learn
</h4>
<p>
Andrej Karpathy
<a href="https://simonwillison.net/2025/Feb/6/andrej-karpathy/">coined
the term</a> vibe-coding just over a month ago, and it has stuck:
</p>
<blockquote>
<p>
There‚Äôs a new kind of coding I call ‚Äúvibe coding‚Äù, where you fully give
in to the vibes, embrace exponentials, and forget that the code even
exists. [‚Ä¶] I ask for the dumbest things like ‚Äúdecrease the padding on
the sidebar by half‚Äù because I‚Äôm too lazy to find it. I ‚ÄúAccept All‚Äù
always, I don‚Äôt read the diffs anymore. When I get error messages I just
copy paste them in with no comment, usually that fixes it.
</p>
</blockquote>
<p>
Andrej suggests this is ‚Äúnot too bad for throwaway weekend projects‚Äù.
It‚Äôs also a <em>fantastic</em> way to explore the capabilities of these
models - and really fun.
</p>
<p>
The best way to learn LLMs is to play with them. Throwing absurd ideas
at them and vibe-coding until they almost sort-of work is a genuinely
useful way to accelerate the rate at which you build intuition for what
works and what doesn‚Äôt.
</p>
<p>
I‚Äôve been vibe-coding since before Andrej gave it a name! My
<a href="https://github.com/simonw/tools">simonw/tools</a> GitHub
repository has 77 HTML+JavaScript apps and 6 Python apps, and every
single one of them was built by prompting LLMs. I have learned <em>so
much</em> from building this collection, and I add to it at a rate of
several new prototypes per week.
</p>
<p>
You can try most of mine out directly on
<a href="https://tools.simonwillison.net/">tools.simonwillison.net</a> -
a GitHub Pages published version of the repo. I wrote more detailed
notes on some of these back in October in
<a href="https://simonwillison.net/2024/Oct/21/claude-artifacts/">Everything
I built with Claude Artifacts this week</a>.
</p>
<p>
If you want to see the transcript of the chat used for each one it‚Äôs
almost always linked to in the commit history for that page - or visit
the new <a href="https://tools.simonwillison.net/colophon">colophon
page</a> for an index that includes all of those links.
</p>
<h4 id="a-detailed-example">
A detailed example using Claude Code
</h4>
<p>
While I was writing this article I had the idea for that
<a href="https://tools.simonwillison.net/colophon">tools.simonwillison.net/colophon</a>
page - I wanted something I could link to that showed the commit history
of each of my tools in a more obvious way than GitHub.
</p>
<p>
I decided to use that as an opportunity to demonstrate my AI-assisted
coding process.
</p>
<p>
For this one I used
<a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude
Code</a>, because I wanted it to be able to run Python code directly
against my existing tools repository on my laptop.
</p>
<p>
Running the <code>/cost</code> command at the end of my session showed
me this:
</p>
<pre><code>&gt; /cost 
  ‚éø  Total cost: $0.61
     Total duration (API): 5m 31.2s
     Total duration (wall): 17m 18.7s
</code></pre>
<p>
The initial project took me just over 17 minutes from start to finish,
and cost me 61 cents in API calls to Anthropic.
</p>
<p>
I used the authoritarian process where I told the model exactly what I
wanted to build. Here‚Äôs my sequence of prompts
(<a href="https://gist.github.com/simonw/323e1b00ee4f8453c7834a7560eeafc1">full
transcript here</a>).
</p>
<p>
I started by asking for an initial script to gather the data needed for
the new page:
</p>
<blockquote>
<p>
Almost all of the HTML files in this directory were created using Claude
prompts, and the details of those prompts are linked in the commit
messages. Build a Python script that checks the commit history for each
HTML file in turn and extracts any URLs from those commit messages into
a list. It should then output a JSON file with this structure: {‚Äúpages‚Äù:
{‚Äúname-of-file.html‚Äù: [‚Äúurl‚Äù], {‚Äúname-of-file-2.html‚Äù: [‚Äúurl1‚Äù, ‚Äúurl2‚Äù],
‚Ä¶ - as you can see, some files may have more than one URL in their
commit history. The script should be called gather_links.py and it
should save a JSON file called gathered_links.json
</p>
</blockquote>
<p>
I really didn‚Äôt think very hard about this first prompt - it was more of
a stream of consciousness that I typed into the bot as I thought about
the initial problem.
</p>
<p>
I inspected the initial result and spotted some problems:
</p>
<blockquote>
<p>
It looks like it just got the start of the URLs, it should be getting
the whole URLs which might be to different websites - so just get
anything that starts https:// and ends with whitespace or the end of the
commit message
</p>
</blockquote>
<p>
Then I changed my mind - I wanted those full commit messages too:
</p>
<blockquote>
<p>
Update the script - I want to capture the full commit messages AND the
URLs - the new format should be {‚Äúpages‚Äù: {‚Äúaria-live-regions.html‚Äù:
{‚Äúcommits‚Äù: [{‚Äúhash‚Äù: hash, ‚Äúmessage‚Äù: message, ‚Äúdate‚Äù: iso formatted
date], ‚Äúurls‚Äù: [list of URLs like before]
</p>
</blockquote>
<p>
Providing examples like this is a great shortcut to getting exactly what
you want.
</p>
<p>
Note that at no point have I looked at the code it‚Äôs written in
<a href="https://github.com/simonw/tools/blob/87e2577983f11fc9c7bf7b7a268cf2404a21e1c5/gather_links.py">gather_links.py</a>!
This is pure vibe-coding: I‚Äôm looking at what it‚Äôs doing, but I‚Äôve left
the implementation details entirely up to the LLM.
</p>
<p>
The JSON looked good to me, so I said:
</p>
<blockquote>
<p>
This is working great. Write me a new script called build_colophon.py
which looks through that gathered JSON file and builds and saves an HTML
page. The page should be mobile friendly and should list every page -
with a link to that page - and for each one display the commit messages
neatly (convert newlines to br and linkify URLs but no other formatting)
- plus the commit message dates and links to the commits themselves
which are in
<a href="https://github.com/simonw/tools">https://github.com/simonw/tools</a>
</p>
</blockquote>
<p>
Claude knows how GitHub URLs works, so telling it to link to the commits
and providing the repo name was enough for it guess
<code>https://github.com/simonw/tools/commit/fd9daf885c924ba277806b3440457d52b0ad90a8</code>
for those commit URLs.
</p>
<p>
I tend to find Claude has good default taste when it comes to web page
design - I said ‚Äúthe page should be mobile friendly‚Äù and left it at
that.
</p>
<p>
Claude churned away and built me a page that wasn‚Äôt right, so I said:
</p>
<blockquote>
<p>
it‚Äôs not working right. ocr.html had a bunch of commits but in
colophon.html there is only one link and heading for the first commit
and the rest are shown within that same block - there should be separate
HTML chunks with links and formatted dates for each of the other
commits. Also the neatly formatted date should include the HH:MM as well
as the date
</p>
</blockquote>
<p>
It fixed the bug all on its own, leaving just two changes I decided to
make:
</p>
<blockquote>
<p>
it‚Äôs almost perfect, but each page should have the commits displayed in
the opposite order - oldest first
</p>
</blockquote>
<p>
And then:
</p>
<blockquote>
<p>
One last change - the pages are currently listed alphabetically, lets
instead list them with the most recently modified at the top
</p>
</blockquote>
<p>
And that was the entire project! Here‚Äôs
<a href="https://github.com/simonw/tools/blob/1e04f12a1cacea8856946162457d0d77e60ee549/build_colophon.py">build_colophon.py</a>,
and the page it generated came out
<a href="https://static.simonwillison.net/static/2025/colophon.html">looking
pretty good</a>:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/colophon.jpg" alt="Tools Colophon. This page documents the creation of the tools on tools.simonwillison.net, including links to the Claude conversations used to build them. social-media-cropper.html b4a2bc December 10, 2024 20:35 Social media cropper  https://gist.github.com/simonw/12b8f88932a71450071190e1289a17e9 a10954 February 28, 2025 16:02 Support 2:1 and 14:10 ratios  https://gist.github.com/simonw/e23917eddcbb368c9b6180d581f8f40a" style="max-width: 100%;" />
</p>
<p>
There was one remaining task: I needed to deploy the new colophon as
part of my site, but I didn‚Äôt want to check that new
<code>colophon.html</code> page into the repository itself. I wanted a
custom GitHub Pages build process.
</p>
<p>
I started a brand new Claude Code session (clearing the slate) to see if
that was something Claude could figure out too:
</p>
<blockquote>
<p>
This repo is deployed using GitHub Pages. I want to modify the build
slightly - before deploying to GitHub Pages it should run python
gather_links.py which will generate a file called gathered_links.json -
then it should run python build_colophon.py which will use that file to
create colophon.html - I want both gathered_links.json and colophon.html
to be published to GitHub pages but I do not want them checked into the
repo itself
</p>
</blockquote>
<p>
Unlike the first time, this time I watched what it was doing very
closely - I don‚Äôt know how to customize GitHub Pages builds in this way
and I wanted to both learn how to do it and stay cautious that it might
hallucinate details and fail at the task.
</p>
<p>
It told me it wanted to create this file and asked for my permission:
</p>
<div class="highlight highlight-source-yaml">
<pre><span class="pl-ent">name</span>: <span class="pl-s">Deploy to GitHub Pages</span>

<span class="pl-ent">on</span>:
  <span class="pl-c"><span class="pl-c">#</span> Run on push to main branch</span>
  <span class="pl-ent">push</span>:
    <span class="pl-ent">branches</span>: <span class="pl-s">[ main ]</span>
  <span class="pl-c"><span class="pl-c">#</span> Allow manual trigger</span>
  <span class="pl-ent">workflow_dispatch</span>:

<span class="pl-c"><span class="pl-c">#</span> Sets permissions of the GITHUB_TOKEN</span>
<span class="pl-ent">permissions</span>:
  <span class="pl-ent">contents</span>: <span class="pl-s">read</span>
  <span class="pl-ent">pages</span>: <span class="pl-s">write</span>
  <span class="pl-ent">id-token</span>: <span class="pl-s">write</span>

<span class="pl-c"><span class="pl-c">#</span> Allow only one concurrent deployment</span>
<span class="pl-ent">concurrency</span>:
  <span class="pl-ent">group</span>: <span class="pl-s">pages</span>
  <span class="pl-ent">cancel-in-progress</span>: <span class="pl-c1">true</span>

<span class="pl-ent">jobs</span>:
  <span class="pl-ent">build</span>:
    <span class="pl-ent">runs-on</span>: <span class="pl-s">ubuntu-latest</span>
    <span class="pl-ent">steps</span>:
      - <span class="pl-ent">name</span>: <span class="pl-s">Checkout</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/checkout@v4</span>
        <span class="pl-ent">with</span>:
          <span class="pl-ent">fetch-depth</span>: <span class="pl-c1">0</span>  <span class="pl-c"><span class="pl-c">#</span> Need full history for git log in gather_links.py</span>

      - <span class="pl-ent">name</span>: <span class="pl-s">Setup Python</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/setup-python@v5</span>
        <span class="pl-ent">with</span>:
          <span class="pl-ent">python-version</span>: <span class="pl-s"><span class="pl-pds">'</span>3.12<span class="pl-pds">'</span></span>

      - <span class="pl-ent">name</span>: <span class="pl-s">Install dependencies</span>
        <span class="pl-ent">run</span>: <span class="pl-s">|</span>
<span class="pl-s">          python -m pip install --upgrade pip</span>
<span class="pl-s"></span>
<span class="pl-s"></span>      - <span class="pl-ent">name</span>: <span class="pl-s">Generate links and build colophon</span>
        <span class="pl-ent">run</span>: <span class="pl-s">|</span>
<span class="pl-s">          python gather_links.py</span>
<span class="pl-s">          python build_colophon.py</span>
<span class="pl-s"></span>
<span class="pl-s"></span>      - <span class="pl-ent">name</span>: <span class="pl-s">Setup Pages</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/configure-pages@v4</span>

      - <span class="pl-ent">name</span>: <span class="pl-s">Build with Jekyll</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/jekyll-build-pages@v1</span>

      - <span class="pl-ent">name</span>: <span class="pl-s">Upload artifact</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/upload-pages-artifact@v3</span>

  <span class="pl-ent">deploy</span>:
    <span class="pl-ent">environment</span>:
      <span class="pl-ent">name</span>: <span class="pl-s">github-pages</span>
      <span class="pl-ent">url</span>: <span class="pl-s">${{ steps.deployment.outputs.page_url }}</span>
    <span class="pl-ent">runs-on</span>: <span class="pl-s">ubuntu-latest</span>
    <span class="pl-ent">needs</span>: <span class="pl-s">build</span>
    <span class="pl-ent">steps</span>:
      - <span class="pl-ent">name</span>: <span class="pl-s">Deploy to GitHub Pages</span>
        <span class="pl-ent">id</span>: <span class="pl-s">deployment</span>
        <span class="pl-ent">uses</span>: <span class="pl-s">actions/deploy-pages@v4</span></pre>
</div>
<p>
I guess that‚Äôs right? I appreciated that comment about ‚ÄúNeed full
history for git log in gather_links.py‚Äù - that‚Äôs something I could
easily have missed.
</p>
<p>
Then it said it wanted to add those files to <code>.gitignore</code> -
sounds like a good idea.
</p>
<p>
It spat out this summary of its actions so far:
</p>
<blockquote>
<p>
I‚Äôve created a GitHub Actions workflow for GitHub Pages that:
</p>
<ol>
<li>
Creates a .github/workflows/pages.yml file that:
<ul>
<li>
Runs on pushes to main branch
</li>
<li>
Checkouts the full git history (needed for gather_links.py)
</li>
<li>
Sets up Python 3.12
</li>
<li>
Runs both scripts in sequence to generate the files
</li>
<li>
Builds and deploys to GitHub Pages
</li>
</ul>
</li>
<li>
Created .gitignore file to prevent the generated files from being
committed
</li>
</ol>
<p>
Now when you push to main, GitHub Actions will:
</p>
<ul>
<li>
Run the gather_links.py script to create gathered_links.json
</li>
<li>
Run build_colophon.py to create colophon.html
</li>
<li>
Deploy both files (and all other files) to GitHub Pages
</li>
</ul>
</blockquote>
<p>
I checked to see how much it had cost:
</p>
<pre><code>&gt; /cost 
  ‚éø  Total cost: $0.1788
     Total duration (API): 44.6s
     Total duration (wall): 10m 18.6s
</code></pre>
<p>
So 17 cents and 45 seconds using the Claude API. (I got distracted,
hence the 10m of total time.) Here‚Äôs the
<a href="https://gist.github.com/simonw/a560b07eef577e6183021d1ccaae7e07">full
transcript</a>.
</p>
<p>
The code didn‚Äôt look like it would irreversibly break anything, so I
pushed it to GitHub to see what would happen.
</p>
<p>
‚Ä¶ and it worked! My new
<a href="https://tools.simonwillison.net/colophon">colophon page</a> was
live.
</p>
<p>
There‚Äôs a catch. I watched the
<a href="https://github.com/simonw/tools/actions">GitHub Actions</a>
interface while it was running and something didn‚Äôt look right:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/github-actions-colophon.jpg" alt="GitHub Actions interface showing three completed actions. Test for Custom pages workflow for colophon,2 Deploy for that same name and another one called pages-build-deployment." style="max-width: 100%;" />
</p>
<p>
I was expecting that ‚ÄúTest‚Äù job, but why were there two separate
deploys?
</p>
<p>
I had a hunch that the previous, default Jekyll deploy was still
running, while the new deploy ran at the same time - and it was pure
luck of the timing that the new script finished later and over-wrote the
result of the original.
</p>
<p>
It was time to ditch the LLMs and read some documentation!
</p>
<p>
I found this page on
<a href="https://docs.github.com/en/pages/getting-started-with-github-pages/using-custom-workflows-with-github-pages">Using
custom workflows with GitHub Pages</a> but it didn‚Äôt tell me what I
needed to know.
</p>
<p>
On another hunch I checked the GitHub Pages settings interface for my
repo and found this option:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/github-pages-settings.jpg" alt="GitHub Pages UI - shows your site is live at tools.simonwillison.net, deployed 7 minutes ago. - then under Buyld and deployment a source menu shows options for GitHub Actions or for Deploy from a branch (selected)" style="max-width: 100%;" />
</p>
<p>
My repo was set to ‚ÄúDeploy from a branch‚Äù, so I switched that over to
‚ÄúGitHub Actions‚Äù.
</p>
<p>
I manually updated my <code>README.md</code> to add a link to the new
Colophon page in
<a href="https://github.com/simonw/tools/commit/4ee15aaad8e9a412505210a30f485528cb3c0390">this
commit</a>, which triggered another build.
</p>
<p>
This time only two jobs ran, and the end result was the correctly
deployed site:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/github-actions-colophon-2.jpg" alt="Only two in-progress workflows now, one is the Test one and the other is the Deploy to GitHub Pages one." style="max-width: 100%;" />
</p>
<p>
(I later spotted another bug - some of the links inadvertently included
<code>&lt;br&gt;</code> tags in their <code>href=</code>, which I
<a href="https://github.com/simonw/tools/commit/87e2577983f11fc9c7bf7b7a268cf2404a21e1c5">fixed</a>
with another
<a href="https://gist.github.com/simonw/d5ccbca1b530868980609222790a97cb">11
cent Claude Code session</a>.)
</p>
<h4 id="be-ready-for-the-human-to-take-over">
Be ready for the human to take over
</h4>
<p>
I got lucky with this example because it helped illustrate my final
point: expect to need to take over.
</p>
<p>
LLMs are no replacement for human intuition and experience. I‚Äôve spent
enough time with GitHub Actions that I know what kind of things to look
for, and in this case it was faster for me to step in and finish the
project rather than keep on trying to get there with prompts.
</p>
<h4 id="the-biggest-advantage-is-speed-of-development">
The biggest advantage is speed of development
</h4>
<p>
My new <a href="https://tools.simonwillison.net/colophon">colophon
page</a> took me just under half an hour from conception to finished,
deployed feature.
</p>
<p>
I‚Äôm certain it would have taken me significantly longer without LLM
assistance - to the point that I probably wouldn‚Äôt have bothered to
build it at all.
</p>
<p>
<em>This</em> is why I care so much about the productivity boost I get
from LLMs so much: it‚Äôs not about getting work done faster, it‚Äôs about
being able to ship projects that I wouldn‚Äôt have been able to justify
spending time on at all.
</p>
<p>
I wrote about this in March 2023:
<a href="https://simonwillison.net/2023/Mar/27/ai-enhanced-development/">AI-enhanced
development makes me more ambitious with my projects</a>. Two years
later that effect shows no sign of wearing off.
</p>
<p>
It‚Äôs also a great way to accelerate learning new things - today that was
how to customize my GitHub Pages builds using Actions, which is
something I‚Äôll certainly use again in the future.
</p>
<p>
The fact that LLMs let me execute my ideas faster means I can implement
more of them, which means I can learn even more.
</p>
<h4 id="llms-amplify-existing-expertise">
LLMs amplify existing expertise
</h4>
<p>
Could anyone else have done this project in the same way? Probably not!
My prompting here leaned on 25+ years of professional coding experience,
including my previous explorations of GitHub Actions, GitHub Pages,
GitHub itself and the LLM tools I put into play.
</p>
<p>
I also <em>knew</em> that this was going to work. I‚Äôve spent enough time
working with these tools that I was confident that assembling a new HTML
page with information pulled from my Git history was entirely within the
capabilities of a good LLM.
</p>
<p>
My prompts reflected that - there was nothing particularly novel here,
so I dictated the design, tested the results as it was working and
occasionally nudged it to fix a bug.
</p>
<p>
If I was trying to build a Linux kernel driver - a field I know
virtually nothing about - my process would be entirely different.
</p>
<h4 id="bonus-answering-questions-about-codebases">
Bonus: answering questions about codebases
</h4>
<p>
If the idea of using LLMs to write code for you still feels deeply
unappealing, there‚Äôs another use-case for them which you may find more
compelling.
</p>
<p>
Good LLMs are <em>great</em> at answering questions about code.
</p>
<p>
This is also very low stakes: the worst that can happen is they might
get something wrong, which may take you a tiny bit longer to figure out.
It‚Äôs still likely to save you time compared to digging through thousands
of lines of code entirely by yourself.
</p>
<p>
The trick here is to dump the code into a long context model and start
asking questions. My current favorite for this is the catchily titled
<code>gemini-2.0-pro-exp-02-05</code>, a preview of Google‚Äôs Gemini 2.0
Pro which is currently free to use via their API.
</p>
<p>
I used this trick just
<a href="https://simonwillison.net/2025/Mar/6/monolith/">the other
day</a>. I was trying out a new-to-me tool called
<a href="https://github.com/Y2Z/monolith">monolith</a>, a CLI tool
written in Rust which downloads a web page and all of its dependent
assets (CSS, images etc) and bundles them together into a single
archived file.
</p>
<p>
I was curious as to how it worked, so I cloned it into my temporary
directory and ran these commands:
</p>
<div class="highlight highlight-source-shell">
<pre><span class="pl-c1">cd</span> /tmp
git clone https://github.com/Y2Z/monolith
<span class="pl-c1">cd</span> monolith

files-to-prompt <span class="pl-c1">.</span> -c <span class="pl-k">|</span> llm -m gemini-2.0-pro-exp-02-05 \
  -s <span class="pl-s"><span class="pl-pds">'</span>architectural overview as markdown<span class="pl-pds">'</span></span></pre>
</div>
<p>
I‚Äôm using my own
<a href="https://github.com/simonw/files-to-prompt">files-to-prompt</a>
tool (built for me by Claude 3 Opus
<a href="https://simonwillison.net/2024/Apr/8/files-to-prompt/">last
year</a>) here to gather the contents of all of the files in the repo
into a single stream. Then I pipe that into my
<a href="https://llm.datasette.io/">LLM</a> tool and tell it (via the
<a href="https://github.com/simonw/llm-gemini">llm-gemini</a> plugin) to
prompt Gemini 2.0 Pro with a system prompt of ‚Äúarchitectural overview as
markdown‚Äù.
</p>
<p>
This gave me back a
<a href="https://gist.github.com/simonw/2c80749935ae3339d6f7175dc7cf325b">detailed
document</a> describing how the tool works - which source files do what
and, crucially, which Rust crates it was using. I learned that it used
<code>reqwest</code>, <code>html5ever</code>,
<code>markup5ever_rcdom</code> and <code>cssparser</code> and that it
doesn‚Äôt evaluate JavaScript at all, an important limitation.
</p>
<p>
I use this trick several times a week. It‚Äôs a great way to start diving
into a new codebase - and often the alternative isn‚Äôt spending more time
on this, it‚Äôs failing to satisfy my curiosity at all.
</p>
<p>
I included three more examples in
<a href="https://simonwillison.net/2025/Feb/14/files-to-prompt/">this
recent post</a>.
</p>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/tools&quot;&gt;tools&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/github-actions&quot;&gt;github-actions&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-assisted-programming&quot;&gt;ai-assisted-programming&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/anthropic&quot;&gt;anthropic&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/claude&quot;&gt;claude&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/claude-artifacts&quot;&gt;claude-artifacts&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/11/using-llms-for-code/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/11/using-llms-for-code/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-11, from: Feed for Alt USDS)</p>
<p>Major! Here‚Äôs a source:
https://www.cnn.com/2025/03/10/politics/doge-freedom-of-information-act-foia-ruling/index.html</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk4abresn22i"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk4abresn22i</a></p>
<hr />
<h2
id="bug-fixes-unlocking-health-secrets-from-insect-genomes-on-thelio-astra">Bug
Fixes: Unlocking Health Secrets from Insect Genomes on Thelio Astra</h2>
<p>date: 2025-03-11, from: System76 Blog</p>
<p>RJ Nowling joins Transmission Log to discuss mosquitos,
bioinformatics, and Thelio Astra.</p>
<p><br></p>
<p><a
href="https://blog.system76.com/post/bioinformatics-thelio-astra-rjnowling-interview"
class="uri">https://blog.system76.com/post/bioinformatics-thelio-astra-rjnowling-interview</a></p>
<hr />
<h2 id="think-different">Think Different</h2>
<p>date: 2025-03-11, from: O‚ÄôReilly Radar</p>
<p>There‚Äôs something that bothers me about the chatter that AI is making
‚Äúintelligence‚Äù ubiquitous. For example, in a recent Bloomberg article,
‚ÄúAI Will Upend a Basic Assumption About How Companies Are Organized,‚Äù
Azeem Azhar wrote: As intelligence becomes cheaper and faster, the basic
assumption underpinning our institutions‚Äîthat human insight is scarce
and expensive‚Äîno longer holds. [‚Ä¶]</p>
<p><br></p>
<p><a href="https://www.oreilly.com/radar/think-different/"
class="uri">https://www.oreilly.com/radar/think-different/</a></p>
<hr />
<h2 id="a-bio-integrated-gustatory-interface">A Bio-Integrated Gustatory
Interface</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: One Foot Tsunami</p>
<p><br></p>
<p><a
href="https://onefoottsunami.com/2025/03/11/a-bio-integrated-gustatory-interface/"
class="uri">https://onefoottsunami.com/2025/03/11/a-bio-integrated-gustatory-interface/</a></p>
<hr />
<h2 id="here-is-nasas-contract-with-clearview-ai">Here is NASA‚Äôs
Contract with Clearview AI</h2>
<p>date: 2025-03-11, from: 404 Media Group</p>
<p>The oversight and investigative body of NASA spent $16,000 on the
technology, 404 Media previously reported.</p>
<p><br></p>
<p><a
href="https://www.404media.co/here-is-nasas-contract-with-clearview-ai-2/"
class="uri">https://www.404media.co/here-is-nasas-contract-with-clearview-ai-2/</a></p>
<hr />
<h2 id="ais-future-not-always-bigger">AI‚Äôs Future: Not Always
Bigger</h2>
<p>date: 2025-03-11, from: O‚ÄôReilly Radar</p>
<p>On May 8, O‚ÄôReilly Media will be hosting¬†Coding with AI: The End of
Software Development as We Know It‚Äîa live virtual tech conference
spotlighting how AI is already supercharging developers, boosting
productivity, and providing real value to their organizations. If you‚Äôre
in the trenches building tomorrow‚Äôs development practices today and
interested in speaking at the [‚Ä¶]</p>
<p><br></p>
<p><a href="https://www.oreilly.com/radar/ais-future-not-always-bigger/"
class="uri">https://www.oreilly.com/radar/ais-future-not-always-bigger/</a></p>
<hr />
<h2 id="the-economic-excuse-industry-is-booming">The Economic Excuse
Industry is Booming</h2>
<p>date: 2025-03-11, from: Paul Krugman</p>
<p>No, we don‚Äôt need an economic ‚Äúdetox‚Äù</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/the-economic-excuse-industry-is-booming"
class="uri">https://paulkrugman.substack.com/p/the-economic-excuse-industry-is-booming</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>Kansas politicians say an awful lot. But look beyond those words to
catch what they actually do.</p>
<p><br></p>
<p><a
href="https://kansasreflector.com/2025/03/11/kansas-politicians-say-an-awful-lot-but-look-beyond-those-words-to-catch-what-they-actually-do/"
class="uri">https://kansasreflector.com/2025/03/11/kansas-politicians-say-an-awful-lot-but-look-beyond-those-words-to-catch-what-they-actually-do/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-11, from: Dave Winer‚Äôs linkblog)</p>
<p>German Thru-Hiker Detained, Deported, and Banned From US.</p>
<p><br></p>
<p><a
href="https://thetrek.co/a-german-thru-hiker-has-been-detained-deported-and-banned-from-the-us-without-a-hearing-heres-what-you-need-to-know/"
class="uri">https://thetrek.co/a-german-thru-hiker-has-been-detained-deported-and-banned-from-the-us-without-a-hearing-heres-what-you-need-to-know/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-11, from: Feed for Alt USDS)</p>
<p><span class="citation"
data-cites="alt18f.bsky.social">@alt18f.bsky.social</span> saved
taxpayers billions, modernized digital services, and made government
work better. üí°Now, DOGE has shut it down‚Äîeliminating projects that
served voters, refugees, military personnel, and more. Read about 18F‚Äôs
impact ‚¨áÔ∏è</p>
<p>üîó https://www.wethebuilders.org/posts/what-is-18F</p>
<p>#altgov #18f</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3lk2yqrc3vc2g"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3lk2yqrc3vc2g</a></p>
<hr />
<h2 id="tricked-into-installing-macos-update">Tricked Into Installing
macOS Update</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Howard Oakley: Over the last few years, many have reported that their
Macs spontaneously updated or even upgraded macOS when they didn‚Äôt
expect them to, and often against their wishes. This can occur when
Software Update in System Settings has Install macOS updates turned off.
Explanations of how Apple appears to be able to override [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/10/tricked-into-installing-macos-update/"
class="uri">https://mjtsai.com/blog/2025/03/10/tricked-into-installing-macos-update/</a></p>
<hr />
<h2 id="purgeable-disk-space">Purgeable Disk Space</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Howard Oakley: The Finder was certain there was nearly 228 GB
available, but also claimed that 852 GB was used. Did that mean that my
1 TB SSD had grown in size overnight to just over 1.08 TB? What was this
‚Äú83.71 GB purgeable‚Äù, and had the Finder already assumed it had been
deleted? Could [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/10/purgeable-disk-space/"
class="uri">https://mjtsai.com/blog/2025/03/10/purgeable-disk-space/</a></p>
<hr />
<h2
id="nsfilewrapper-sometimes-changes-modification-date-to-the-past">NSFileWrapper
Sometimes Changes Modification Date to the Past</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Matteo Rattoti (via Christian Tietze): This repository demonstrates a
bug in NSFileWrapper where overwriting a file with the same content
using -[NSFileWrapper writeToURL:options:originalContentsURL:error:]
sometimes causes the file‚Äôs modification date to be set in the past
instead of updating to the current time. I think I saw someone speculate
that this is due to it not [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/03/10/nsfilewrapper-sometimes-changes-modification-date-to-the-past/"
class="uri">https://mjtsai.com/blog/2025/03/10/nsfilewrapper-sometimes-changes-modification-date-to-the-past/</a></p>
<hr />
<h2 id="weak-m4-mac-mini-wi-fi">Weak M4 Mac mini Wi-Fi</h2>
<p>date: 2025-03-11, from: Michael Tsai</p>
<p>Mike Wuerthele: Hours after the new Mac mini hit customers‚Äô desks,
some users started complaining that when they hooked up a hard drive or
SSD to the unit, they‚Äôd see Wi-Fi speeds drop, or be cut entirely. The
reports were almost always followed up with the realization that when
disconnected, the Wi-Fi speeds would go [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/03/10/weak-m4-mac-mini-wi-fi/"
class="uri">https://mjtsai.com/blog/2025/03/10/weak-m4-mac-mini-wi-fi/</a></p>
<hr />
<h2 id="abandon-all-keyboards">Abandon all keyboards</h2>
<p>date: 2025-03-11, updated: 2025-03-11, from: Robin Rendle Essays</p>
<p><br></p>
<p><a href="https://robinrendle.com/notes/abandon-all-keyboards/"
class="uri">https://robinrendle.com/notes/abandon-all-keyboards/</a></p>
<hr />
<h2 id="llvm-fortran-levels-up-goodbye-flang-new-hello-flang">LLVM
Fortran Levels Up: Goodbye flang-new, Hello flang!</h2>
<p>date: 2025-03-11, from: LLVM Blog</p>
<p>
LLVM has included a Fortran compiler ‚ÄúFlang‚Äù since LLVM 11 in late 2020.
However,until recently the Flang binary was not <code>flang</code> (like
<code>clang</code>) but instead<code>flang-new</code>.
</p>
<p>
LLVM 20 ends the era of <code>flang-new</code>. The community has
decided that Flang isworthy of a new name.
</p>
<p>
The ‚Äúnew‚Äù name? You guessed it, <code>flang</code>.
</p>
<p>
A simple change that represents a major milestone for Flang.
</p>
<p>
This article will cover the almost 10 year journey of Flang. The
firstconcepts, multiple rewrites, the adoption of LLVM‚Äôs Multi Level
IntermediateRepresentation (MLIR) and Flang entering the LLVM Project.
</p>
<p>
If you want to try <code>flang</code> right now, you
can<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>it
or try it in your browser
using<a href="https://godbolt.org/z/3hhYM37Kh">Compiler Explorer</a>.
</p>
<h1 id="why-fortran">
Why Fortran?
</h1>
<p>
Fortran was first created in the 1950s, and the name came from ‚ÄúFormula
Translation‚Äù.Fortran focused on the mathematics use case and freed
programmers from writingassembly code that could only run on specific
machines.
</p>
<p>
Instead they could write code that looked like a formula. You expect
this todaybut for the time it was a revolution. This feature led to
heavy use in scientificcomputing: weather modelling, fluid dynamics and
computational chemistry, justto name a few.
</p>
<blockquote>
<p>
Whilst many alternative programming languages have comeand gone, it
[Fortran] has regained its popularity for writing highperformance codes.
Indeed, over 80% of the applicationsrunning on ARCHER2, a 750,000 core
Cray EX which isthe UK national supercomputer, are written in Fortran.
</p>
</blockquote>
<ul>
<li>
<a href="https://arxiv.org/pdf/2308.13274">Fortran High-Level Synthesis:
Reducing the barriersto accelerating High Performance Computing (HPC)
codes on FPGAs</a> (Gabriel Rodriguez-Canal et al., 2023)
</li>
</ul>
<p>
Fortran has had a
<a href="https://ondrejcertik.com/blog/2021/03/resurrecting-fortran/">resurgence</a>in
recent years, gaining a <a href="https://fpm.fortran-lang.org/">package
manager</a>, an
unofficial<a href="https://github.com/fortran-lang/stdlib">standard
library</a> and <a href="https://lfortran.org/">LFortran</a>,a compiler
that supports interactive programming (LFortran also uses LLVM).
</p>
<p>
For the full history of Fortran, IBM has an excellent
<a href="https://www.ibm.com/history/fortran">article</a>on the topic
and I encourage you to look at
the<a href="https://archive.computerhistory.org/resources/text/Fortran/102653987.05.01.acc.pdf">‚ÄúProgrammer‚Äôs
Primer for Fortran‚Äù</a>if you want to see the early form of Fortran.
</p>
<p>
If you want to learn the language,
<a href="https://fortran-lang.org/">fortran-lang.org</a>is a great place
to start.
</p>
<h1 id="why-would-you-make-another-fortran-compiler">
Why Would You Make Another Fortran Compiler?
</h1>
<p>
There are many Fortran compilers. Some are vendor specific such as
the<a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/fortran-compiler.html">Intel
Fortran Compiler</a>or NVIDIA‚Äôs
<a href="https://developer.nvidia.com/hpc-compilers">HPC compilers</a>.
Thenthere are open source options like
<a href="https://gcc.gnu.org/fortran/">GFortran</a>, whichsupports many
platforms.
</p>
<p>
Why build one more?
</p>
<p>
The two partners in the early days of Flang were the US National Labs
and NVIDIA.
</p>
<p>
For Pat McCormick (Flang project lead at Los Alamos National Laboratory)
preservingthe utility of Fortran code was imperative:
</p>
<blockquote>
<p>
These [Fortran] codes represent an essential capability that supports
manyelements of our [The United States‚Äô] scientific mission and will
continue to doso for the foreseeable future. A fundamental risk facing
these codes is theabsence of a long-term, non-proprietary support path
for Fortran.
</p>
</blockquote>
<p>
GFortran might seem to counter that statement, but remember that a
single projectis a single point of failures, incompatibilities and
disagreements. Having multipleimplementations reduces that risk.
</p>
<p>
NVIDIA‚Äôs Gary Klimowicz
<a href="https://www.youtube.com/watch?v=Fy68k5hHgLk">laid out</a>their
goals for Flang in a presentation to FortranCon in 2020:
</p>
<ul>
<li>
Use a permissive license like that of
<a href="https://llvm.org/LICENSE.txt">LLVM</a>,which is more palatable
to commercial users and contributors.
</li>
<li>
Develop an active community of Fortran compiler developers that
includescompanies and institutions.
</li>
<li>
Support Fortran tool development by basing Flang on existing LLVM
frameworks.
</li>
<li>
Support Fortran language experimentation for future language standards
proposals.
</li>
</ul>
<p>
Intentions echoed by Pat McCormick:
</p>
<blockquote>
<p>
The overarching goal was to establish an open-source, modern
implementation andsimultaneously grow a community that spanned industry,
academia, and federalagencies at both the national and international
levels.
</p>
</blockquote>
<p>
Fortran as a language also benefits from having many implementations.
For C++language features, it is common to implement them on top of Clang
and GCC, toprove the feature is viable and get feedback.
</p>
<p>
Implementing the feature multiple times in different compilers
uncoversassumptions that may be a problem for certain compilers, or
certain groups ofcompiler users.
</p>
<p>
In the same way, Flang and GFortran can provide that diversity.
</p>
<p>
However, even when features are standardised, standards can be ambiguous
andimplementations do make mistakes. A new compiler is a chance to
uncover these.
</p>
<p>
Jeff Hammond (NVIDIA) is very familiar with this, having tested Flang
with manyexisting applications. They had this to say on the motivations
for Flangand how users have reacted to it:
</p>
<blockquote>
<p>
The Fortran language has changed quite a bit over the past 30 years.
Modern Fortrandeserves a modern compiler ecosystem, that‚Äôs not only
capable of compiling allthe old codes and all the code written for the
current standard, but also supportsinnovation in the future.
</p>
<p>
Because it‚Äôs a huge amount of work to build a feature-complete modern
Fortran compiler,it‚Äôs useful to leverage the resources of the entire
LLVM community for this effort.NVIDIA and ARM play leading roles right
now, with important contributions from IBM,Fujitsu and LBNL [Lawrence
Berkeley National Laboratory], e.g.¬†related to testsuites and coarrays.
We hope to see the developer community grow in the future.
</p>
<p>
Another benefit from the LLVM Fortran compiler is that users are more
likely toinvest in supporting a new compiler when it has full language
support and runs onall the platforms. A broad developer base is critical
to support all the platforms.
</p>
<p>
What I have seen so far interacting with our Fortran users is that they
are veryexcited about LLVM Flang and were willing to commit to
supporting it in theirbuild systems and CI systems, which has driven
quality improvements in both theFlang compiler and the applications.
</p>
<p>
Like Clang did with C and C++ codes when it started to become popular,
Flangis helping to identify bugs in Fortran code that weren‚Äôt noticed
before, whichis making the Fortran software ecosystem better.
</p>
</blockquote>
<h1 id="pgi-to-llvm-the-flang-timeline">
PGI to LLVM: The Flang Timeline
</h1>
<p>
The story of Flang really starts in 2015, but the Portland Group (PGI)
collaboratedwith US National Labs prior to this. PGI would later become
part of NVIDIA andbe instrumental to the Flang project.
</p>
<ul>
<li>
<strong>1989</strong> The
<a href="https://web.archive.org/web/19970628161656/http://www.pgroup.com/corp_home.html">Portland
Group</a>is formed. To provide C, Fortran 77 and C++ compilers for the
Intel i860 market.
</li>
<li>
<strong>1990</strong> Intel bundles PGI compilers with its iPSC/860
supercomputer.
</li>
<li>
<strong>1996</strong>
PGI<a href="https://web.archive.org/web/20100528051556/http://www.sandia.gov/ASCI/Red/papers/Mattson/OVERVIEW.html">works
with</a>Sandia National Laboratories to provide compilers for the
Accelerated Strategic Computing Initiative (ASCI) Option
Redsupercomputer.
</li>
<li>
<strong>December 2000</strong> PGI becomes
a<a href="https://www.electronicsweekly.com/news/archived/resources-archived/stmicroelectronics-to-acquire-pgi-2000-12/">wholly
owned subsidiary</a> ofSTMicroElectronics.
</li>
<li>
<strong>August 2011</strong> Away from PGI, Bill Wendling
<a href="https://github.com/llvm-fortran/fort/commit/af352bf765ecf3e55da38c34cb480b269a157894">starts</a>an
LLVM based Fortran compiler called ‚ÄúFlang‚Äù (later known as
<a href="https://github.com/llvm-fortran/fort">‚ÄúFort‚Äù</a>).Bill is
joined by several collaborators a few months later.
</li>
<li>
<strong>July 2013</strong> PGI is
<a href="https://www.theregister.com/2013/07/30/nvidia_buys_the_portland_group/">sold
to NVIDIA</a>.
</li>
</ul>
<p>
In late 2015 there were the first signs of what would become ‚ÄúClassic
Flang‚Äù. Thoughat the time it was just ‚ÄúFlang‚Äù, I will use ‚ÄúClassic
Flang‚Äù here for clarity.
</p>
<p>
Development of what was to become ‚ÄúFort‚Äù continued under the ‚ÄúFlang‚Äù
name,completely separate from the Classic Flang project.
</p>
<ul>
<li>
<p>
<strong>November 2015</strong> NVIDIA joins the US Department of
EnergyExascale Computing Project. Including a commitment to create an
open
source<a href="https://www.llnl.gov/article/41756/nnsa-national-labs-team-nvidia-develop-open-source-fortran-compiler-technology">Fortran
compiler</a>.
</p>
<blockquote>
<p>
‚ÄúThe U.S. Department of Energy‚Äôs National Nuclear Security
Administration and itsthree national labs [Los Alamos, Lawrence
Livermore and Sandia] have reached anagreement with NVIDIA‚Äôs PGI
division to adapt and open-source PGI‚Äôs Fortranfrontend, and associated
Fortran runtime library, for contribution to the LLVM project.‚Äù
</p>
</blockquote>
<p>
(this news is also the first appearance of Flang in an issue
of<a href="https://llvmweekly.org/issue/98">LLVM Weekly</a>)
</p>
</li>
<li>
<p>
<strong>May 2017</strong> The first release of Classic Flang as a
separaterepository, outside of the LLVM Project. Composed of a PGI
compiler frontendand a new backend that generates LLVM Intermediate
Representation (LLVM IR).
</p>
</li>
<li>
<p>
<strong>August 2017</strong> The Classic Flang project is
<a href="https://llvmweekly.org/issue/191">announced
officially</a>(according to LLVM Weekly‚Äôs report, the original mailing
list is offline).
</p>
</li>
</ul>
<p>
During this time, plans were formed to propose moving Classic Flang into
the LLVMProject.
</p>
<ul>
<li>
<p>
<strong>December 2017</strong> The original ‚ÄúFlang‚Äù is renamed
to<a href="https://github.com/llvm-fortran/fort/commit/0585746476e3c1abe8ab4109b9dd98483cabdf09">‚ÄúFort‚Äù</a>so
as not to compete with Classic Flang.
</p>
</li>
<li>
<p>
<strong>April 2018</strong> Steve Scalpone (NVIDIA)
<a href="https://www.youtube.com/watch?v=sFVRQDgKihY">announces</a>at
the European LLVM Developers‚Äô Conference that the frontend of Classic
Flang will be rewritten to addressfeedback from the LLVM community. This
new front end became known as ‚ÄúF18‚Äù.
</p>
</li>
<li>
<p>
<strong>August 2018</strong> Eric Schweitz (NVIDIA) begins work on what
would become‚ÄúFortran Intermediate Representation‚Äù, otherwise known as
‚ÄúFIR‚Äù. This work wouldlater become the <code>fir-dev</code> branch.
</p>
</li>
<li>
<p>
<strong>February 2019</strong> Steve Scalpone
<a href="https://lists.llvm.org/pipermail/llvm-dev/2019-February/130497.html">proposes</a>contributing
F18 to the LLVM Project.
</p>
</li>
<li>
<p>
<strong>April 2019</strong> F18 is
<a href="https://discourse.llvm.org/t/f18-is-accepted-as-part-of-llvm-project/51719">approved</a>for
migration into the LLVM Project monorepo.
</p>
<p>
At this point F18 was only the early parts of the compiler, it could not
generatecode (later <code>fir-dev</code> work addressed this). Despite
that, it moved into <code>flang/</code>in the monorepo, awaiting the
completion of the rest of the work.
</p>
</li>
<li>
<p>
<strong>June 2019</strong> Peter Waller (Arm)
<a href="https://discourse.llvm.org/t/rfc-adding-a-fortran-mode-to-the-clang-driver-for-flang/52307">proposes</a>adding
a Fortran mode to the Clang compiler driver.
</p>
</li>
<li>
<p>
<strong>August 2019</strong> The
<a href="https://github.com/flang-compiler/f18/commit/b6c30284e7876f6ccd4bb024bd5f349128e99b7c">first
appearance</a>of the <code>flang.sh</code> driver wrapper script (more
on this later).
</p>
</li>
<li>
<p>
<strong>December 2019</strong> The
<a href="https://discourse.llvm.org/t/flang-landing-in-the-monorepo/54022">plan</a>for
rewriting the F18 git history to fit into the LLVM project is
announced.This effort was led by Arm, with Peter Waller going so far as
to writea
<a href="https://github.com/llvm/llvm-project/commit/55d5e6cbe2509a24132d056e1f361dc39312929b#diff-c389405236998090c7c8b9741506f01fb28abbd7da52e9566323c585ac0eb89cL910">custom
tool</a>to rewrite the history of F18.
</p>
<p>
Kiran Chandramohan (Arm)
<a href="https://groups.google.com/a/tensorflow.org/g/mlir/c/SCerbBpoxng/m/bVqWTRY7BAAJ">proposes</a>an
OpenMP dialect for MLIR, with the intention of using it in Flang
(discussioncontinues on
<a href="https://discourse.llvm.org/t/rfc-openmp-dialect-in-mlir/397">Discourse</a>during
the following January).
</p>
</li>
<li>
<p>
<strong>February 2020</strong> The
<a href="https://discourse.llvm.org/t/plan-for-landing-flang-in-monorepo/54546">plan</a>for
improvements to F18 to meet the standards required for inclusion in
theLLVM monorepo is announced by Richard Barton (Arm).
</p>
</li>
<li>
<p>
<strong>April 2020</strong> Upstreaming of F18 into the LLVM monorepo
is<a href="https://github.com/llvm/llvm-project/commit/b98ad941a40c96c841bceb171725c925500fce6c">completed</a>.
</p>
</li>
</ul>
<p>
At this point what was in the LLVM monorepo was F18, the rewritten
frontend ofClassic Flang. Classic Flang remained unchanged, still using
the PGI based frontend.
</p>
<p>
Around this time work started in the Classic Flang repo on the
<code>fir-dev</code> branchthat would enable code generation when using
F18.
</p>
<p>
For the following events remember that Classic Flang was still in use.
The ClassicFlang binary is named <code>flang</code>, just like the
folder F18 now occupies in the LLVM Project.
</p>
<p>
<strong>Note:</strong> Some LLVM changes referenced below will appear to
have skipped an LLVM release.This is because they were done after the
release branch was created, but beforethe first release from that branch
was distributed.
</p>
<ul>
<li>
<p>
<strong>April 2020</strong> The first attempt at adding a new compiler
driver for Flang is<a href="https://reviews.llvm.org/D79092">posted</a>
for review. It used the name<code>flang-tmp</code>. This change was
later abandoned in favour of a different approach.
</p>
</li>
<li>
<p>
<strong>September 2020</strong> Flang‚Äôs new compiler driver is
<a href="https://reviews.llvm.org/D86089">added</a>as an experimental
option. This is the first appearance of the <code>flang-new</code>
binary,instead of <code>flang-tmp</code> as proposed before.
</p>
<blockquote>
<p>
The name was intended as temporary, but not the driver.
</p>
</blockquote>
<ul>
<li>
Andrzej Warzy≈Ñski (Arm, Flang Driver Maintainer)
</li>
</ul>
</li>
<li>
<p>
<strong>October 2020</strong> Flang is included in an LLVM release for
the first time inLLVM 11.0.0. There is an <code>f18</code> binary and
the previously mentioned script<code>flang.sh</code>.
</p>
</li>
<li>
<p>
<strong>August 2021</strong> <code>flang-new</code> is no longer
experimental and
<a href="https://reviews.llvm.org/D105811">replaces</a>the previous
Flang compiler driver binary <code>f18</code>.
</p>
</li>
<li>
<p>
<strong>October 2021</strong> LLVM 13.0.0 is the first release to
include a <code>flang-new</code> binary(alongside <code>f18</code>).
</p>
</li>
<li>
<p>
<strong>March 2022</strong> LLVM 14.0.0 releases, with
<code>flang-new</code> replacing <code>f18</code> as the Flangcompiler
driver.
</p>
</li>
<li>
<p>
<strong>April 2022</strong> NVIDIA
<a href="https://discourse.llvm.org/t/nvidia-transition-from-fir-dev/61947">ceases
development</a>of the <code>fir-dev</code> branch in the Classic Flang
project. Upstreaming of <code>fir-dev</code>to the LLVM Project begins
around this date.
</p>
<p>
<code>flang-new</code> can now do
<a href="https://reviews.llvm.org/D122008">code generation</a>if the
<code>-flang-experimental-exec</code> option is used. This change used
workoriginally done on the <code>fir-dev</code> branch.
</p>
</li>
<li>
<p>
<strong>May 2022</strong> Kiran Chandramohan
<a href="https://www.youtube.com/watch?v=FoIjafZGDdE">announces</a>at
the European LLVM Developers‚Äô Meeting that Flang‚Äôs OpenMP 1.1 support is
close to complete.
</p>
<p>
The <code>flang.sh</code> compiler driver script becomes
<code>flang-to-external-fc</code>. Itallows the user to use
<code>flang-new</code> to parse Fortran source code, then write it
backto a file to be compiled with an existing Fortran compiler.
</p>
<p>
The script can be put in place of an existing compiler to test Flang‚Äôs
parser onlarge projects.
</p>
</li>
<li>
<p>
<strong>June 2022</strong> Brad Richardson (Berkeley Lab)
<a href="https://reviews.llvm.org/D153379">changes</a><code>flang-new</code>
to generate code by default, removing the
<code>-flang-experimental-exec</code>option.
</p>
</li>
<li>
<p>
<strong>July 2022</strong> Valentin Cl√©ment (NVIDIA)
<a href="https://discourse.llvm.org/t/nvidia-transition-from-fir-dev/61947/5">announces</a>that
upstreaming of <code>fir-dev</code> to the LLVM Project is complete.
</p>
</li>
<li>
<p>
<strong>September 2022</strong> LLVM 15.0.0 releases, including Flang‚Äôs
experimental codegeneration option.
</p>
</li>
<li>
<p>
<strong>September 2023</strong> LLVM 17.0.0 releases, with Flang‚Äôs code
generation enabledby default.
</p>
</li>
</ul>
<p>
At this point the LLVM Project contained Flang as it is known today.
Sometimesreferred to as ‚ÄúLLVM Flang‚Äù.
</p>
<p>
‚ÄúLLVM Flang‚Äù is the combination of the F18 frontend and MLIR-based code
generationfrom <code>fir-dev</code>. As opposed to ‚ÄúClassic Flang‚Äù that
combines a PGI based frontend andits own custom backend.
</p>
<p>
The initiative to upstream Classic Flang was in some sense complete.
Thoughwith all of the compiler rewritten in the process, what landed in
the LLVM Projectwas very different to Classic Flang.
</p>
<!-- For some reason Hugo doesn't render any plaintext in these bullet points correctly unless they have blank lines between them. -->
<ul>
<li>
<p>
<strong>April 2024</strong> The <code>flang-to-external-fc</code> script
is
<a href="https://github.com/llvm/llvm-project/pull/88904">removed</a>.
</p>
</li>
<li>
<p>
<strong>September 2024</strong> LLVM 19.1.0 releases. The first release
of <code>flang-new</code>as a standalone compiler.
</p>
</li>
<li>
<p>
<strong>October 2024</strong> The community deems that Flang has met the
criteria to not be‚Äúnew‚Äù and the name is changed. Goodbye
<code>flang-new</code>, hello <code>flang</code>!
</p>
</li>
<li>
<p>
<strong>November 2024</strong> AMD
<a href="https://rocm.blogs.amd.com/ecosystems-and-partners/fortran-journey/README.html">announces</a>its
next generation Fortran compiler, based on LLVM Flang.
</p>
<p>
Arm
<a href="https://developer.arm.com/Tools%20and%20Software/Arm%20Compiler%20for%20Linux#Downloads">releases</a>
an experimental versionof its new Arm Toolchain for Linux product, which
includes LLVM Flangas the Fortan compiler.
</p>
</li>
<li>
<p>
<strong>March 2025</strong> LLVM 20.1.0 releases. The first time the
<code>flang</code> binary has beenincluded in a release.
</p>
</li>
</ul>
<h1 id="flang-and-the-definition-of-new">
Flang and the Definition of New
</h1>
<p>
Renaming Flang was discussed a few times
<a href="https://discourse.llvm.org/t/reviving-rename-flang-new-to-flang/68130/1">before</a>the
final proposal. It was always contentious, so for the
final<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462">proposal</a>Brad
Richardson decided to use the
<a href="https://github.com/llvm/llvm-www/blob/main/proposals/LP0001-LLVMDecisionMaking.md">LLVM
proposal process</a>.Rarely used, but specifically designed for these
situations.
</p>
<blockquote>
<p>
After several rounds of back and forth, I thought the discussion
wasdevolving and there wasn‚Äôt much chance we‚Äôd come to a consensus
without someoutside perspective.
</p>
</blockquote>
<ul>
<li>
Brad Richardson
</li>
</ul>
<p>
That outside perspective included Chris Lattner (co-founder of the LLVM
Project),who
quickly<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/25">identified</a>a
unique problem:
</p>
<blockquote>
<p>
We have a bit of an unprecedented situation where an LLVM project is
takingthe name of an already established compiler [Classic Flang].
Everyone seems towant the older flang [Classic Flang] to fade away, but
flang-new is not asmature and it isn‚Äôt clear when and what the criteria
should be for that.
</p>
</blockquote>
<p>
Confusion about the <code>flang</code> name was a key motivation for
Brad Richardson too:
</p>
<blockquote>
<p>
Part of my concern was that the name ‚Äúflang-new‚Äù would get common
usagebefore we were able to change it. I think it‚Äôs now been
demonstrated that thatconcern was valid, because right now [November
2024] fpm [<a href="https://fpm.fortran-lang.org/">Fortran Package
Manager</a>]recognizes the compiler by that name.
</p>
<p>
My main goal at that point was just clear goals for when we wouldmake
the name change.
</p>
</blockquote>
<p>
No single list of goals was agreed, but some came up many times:
</p>
<ul>
<li>
Known limitations and supported features should be documented.
</li>
<li>
As much as possible, work that was expected to fix knownbugs should be
completed, to prevent duplicate bug reports.
</li>
<li>
Unimplemented language features should fail with a message saying that
they areunimplemented. Rather than with a confusing failure or by
producing incorrectcode.
</li>
<li>
LLVM Flang should perform relatively well when compared to other
Fortrancompilers.
</li>
<li>
LLVM Flang must have a reasonable pass rate with a large Fortran
language testsuite, and results of that must be shown publicly.
</li>
<li>
All reasonable steps should be taken to prevent anyone using a
pre-packagedClassic Flang confusing it with LLVM Flang.
</li>
</ul>
<p>
You will see a lot of relative language in those, like ‚Äúreasonable‚Äù.
Noone could say exactly what that meant, but everyone agreed that it
wasinevitable that one day it would all be true.
</p>
<p>
Paul T Robinson summarised the dilemma
<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/15">early</a>in
the thread:
</p>
<blockquote>
<blockquote>
<p>
the plan is to replace Classic Flang with the new Flang in the future.
</p>
</blockquote>
<p>
I suppose one of the relevant questions here is: Has the future arrived?
</p>
</blockquote>
<p>
After that Steve Scalpone (NVIDIA)
gave<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/16">their
perspective</a>that it was not yet time to change the name.
</p>
<p>
So the community got to work on those goals:
</p>
<ul>
<li>
Many performance and correctness issues were addressed by the ‚ÄúHigh
LevelFortran Intermediate Representation‚Äù (HLFIR) (which this article
will explain later).
</li>
<li>
A cross-company team including Arm, Huawei, Linaro, Nvidia and
Qualcomm<a href="https://github.com/orgs/llvm/projects/35/views/1">collaborated</a>to
make it possible to build the popular
<a href="https://www.spec.org/cpu2017/">SPEC 2017</a>benchmark with
Flang.
</li>
<li>
Flang gained support for OpenMP up to version 2.5, and was able to
compile OpenMPspecific benchmarks like
<a href="https://www.spec.org/omp2012/">SPEC OMP</a> and
the<a href="https://www.nas.nasa.gov/software/npb.html">NAS Parallel
Benchmarks</a>.
</li>
<li>
Linaro <a href="https://www.youtube.com/watch?v=Gua80XRPhyY">showed
that</a> the performanceof Flang compared favourably with Classic Flang
and was not far behind GFortran.
</li>
<li>
The GFortran test suite was added to the
<a href="https://github.com/llvm/llvm-test-suite/tree/main/Fortran/gfortran">LLVM
Test Suite</a>,and Flang achieved good results.
</li>
<li>
Fujitsu‚Äôs <a href="https://github.com/fujitsu/compiler-test-suite">test
suite</a> was madepublic and tested with Flang. The process to make
IBM‚Äôs Fortran test suite publicwas started.
</li>
</ul>
<p>
With all that done, in October of 2024
<code>flang-new</code><a href="https://github.com/llvm/llvm-project/commit/06eb10dadfaeaadc5d0d95d38bea4bfb5253e077">became</a><code>flang</code>.
The future had arrived.
</p>
<blockquote>
<p>
And it‚Äôs merged! It‚Äôs been a long (and sometimes contentious) process,
butthank you to everyone who contributed to the discussion.
</p>
</blockquote>
<ul>
<li>
Brad Richardson,
<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/86">closing
out</a> the proposal.
</li>
</ul>
<p>
The goals the community achieved have certainly been worth it for Flang
as acompiler, but did Brad achieve their own goals?
</p>
<blockquote>
<p>
What did I hope to see as a result of the name change? I wanted it to
beeasier for more people to try it out.
</p>
</blockquote>
<p>
So once you have finished reading this
article,<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>Flang
or try it out on <a href="https://godbolt.org/z/3hhYM37Kh">Compiler
Explorer</a>.You know at least one person will appreciate it!
</p>
<h1 id="fortran-intermediate-representation-fir">
Fortran Intermediate Representation (FIR)
</h1>
<p>
All compilers that use LLVM as a backend eventually produce code in the
form ofthe <a href="https://llvm.org/docs/LangRef.html">LLVM
Intermediate Representation</a>(LLVM IR).
</p>
<p>
A drawback of this is that LLVM IR does not include language specific
information.This means that for example, it cannot be used to optimise
arrays in a wayspecific to Fortran programs.
</p>
<p>
One solution to this has been to build a higher level IR that represents
theunique features of the language, optimise that, then convert the
result into LLVM IR.
</p>
<p>
Eric Schweitz (NVIDIA) started to do that for Fortran in late 2018:
</p>
<blockquote>
<p>
FIR was originally conceived as a high-level IR that would interoperate
withLLVM but have a representation more friendly and amenable to
Fortranoptimizations.
</p>
</blockquote>
<p>
Naming is hard but Eric did well here:
</p>
<blockquote>
<p>
FIR was a pun of sorts. Fortran IR and meant to be evocative of the
trees(Abstract Syntax Trees).
</p>
</blockquote>
<p>
We will not go into detail about this early FIR, because
<a href="https://mlir.llvm.org/">MLIR</a>was revealed soon after Eric
started the project and they quickly adopted it.
</p>
<blockquote>
<p>
When MLIR was announced, I quickly switched gears from building
datastructures for a new ‚Äúintermediate IR‚Äù to porting my IR design to
MLIR andusing that instead.
</p>
<p>
I believe FIR was probably the first ‚Äúserious project‚Äù outside of Google
tostart using MLIR.
</p>
</blockquote>
<p>
The FIR work continued to develop, with Jean Perier (NVIDIA) joining
Eric onthe project. It became its own public branch
<code>fir-dev</code>, which was later contributedto the LLVM Project.
</p>
<p>
The following sections will go into detail on the intermediate
representationsthat Flang uses today.
</p>
<h1 id="mlir">
MLIR
</h1>
<p>
The journey from Classic Flang to LLVM Flang involved a rewrite of
theentire compiler. This provided an opportunity to pick up new things
fromthe LLVM Project. Most notably MLIR.
</p>
<p>
‚ÄúMulti-Level Intermediate Representation‚Äù (MLIR) was
first<a href="https://www.youtube.com/watch?v=qzljG6DKgic">introduced</a>
to the LLVMcommunity in 2019, around the time that F18 was approved to
move into the LLVM Project.
</p>
<p>
The problem that MLIR addresses is the same one that Eric Schweitz
tackled with FIR:It is difficult to map high level details of
programming languagesinto LLVM IR.
</p>
<p>
You either have to attach them to the IR as metadata, try to recover
thelost details later, or fight an uphill battle to add the details
toLLVM IR itself. These details are crucial for producing optimised code
in certainlanguages. (Fortran array optimisations were one use case
referenced).
</p>
<p>
This led languages such as Swift and Rust to create their own IRs that
includeinformation relevant to their own optimisations. After that IR
has been optimisedit is converted into LLVM IR and goes through the
normal compilation pipeline.
</p>
<p>
To implement these IRs they have to build a lot of infrastructure, but
it cannotbe shared between the compilers. This is where MLIR comes in.
</p>
<blockquote>
<p>
The MLIR project aims to directly tackle these programming language
design andimplementation challenges‚Äîby making it very cheap to define
and introduce newabstraction levels, and provide ‚Äúin the box‚Äù
infrastructure to solve commoncompiler engineering problems.
</p>
</blockquote>
<ul>
<li>
<a href="https://arxiv.org/abs/2002.11054">‚ÄúMLIR: A Compiler
Infrastructure for the End of Moore‚Äôs Law‚Äù</a>(Chris Lattner, Mehdi
Amini et al., 2020)
</li>
</ul>
<h2 id="flang-and-mlir">
Flang and MLIR
</h2>
<p>
The same year MLIR debuted, Eric Schweitz gave a talk at the later
USLLVM Developers‚Äô meeting
titled<a href="https://www.youtube.com/watch?v=ff3ngdvUang">‚ÄúAn MLIR
Dialect for High-Level Optimization of Fortran‚Äù</a>.FIR by that point
was implemented as an MLIR dialect.
</p>
<blockquote>
<p>
That [switching FIR to be based on MLIR] happened very quickly and I
neverlooked back.
</p>
<p>
MLIR, even in its infancy, was clearly solving many of the exact same
problemsthat we were facing building a new Fortran compiler.
</p>
</blockquote>
<ul>
<li>
Eric Schweitz
</li>
</ul>
<p>
The MLIR community were also happy to have Flang on board:
</p>
<blockquote>
<p>
It was fantastic to have very quickly in the early days of MLIR a non-ML
[Machine Learning] frontendto exercise features we built in MLIR in
anticipation. It led us to course-correctin some cases, and Flang was a
motivating factor for many feature requests.It contributed significantly
to establishing and validating that MLIR had the right foundations.
</p>
</blockquote>
<ul>
<li>
Mehdi Amini
</li>
</ul>
<p>
Flang did not stop there, later adding another
dialect<a href="https://flang.llvm.org/docs/HighLevelFIR.html">‚ÄúHigh
Level Fortran Intermediate Representation‚Äù</a>(HLFIR) which works at a
higher level than FIR. A big target of HLFIRwas array optimisations,
that were more complex to handle using FIR alone.
</p>
<blockquote>
<p>
FIR was a compromise on both ends to some degree. It wasn‚Äôt trying to
capturesyntactic information from Fortran, and I assumed there would be
work done onan Abstract Syntax Tree. That niche would later be filled by
‚ÄúHigh Level FIR‚Äù[HLFIR].
</p>
</blockquote>
<ul>
<li>
Eric Schweitz
</li>
</ul>
<h2 id="irs-all-the-way-down">
IRs All the Way Down
</h2>
<p>
The compilation process starts with Fortran source code.
</p>
<div class="highlight">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fortran" data-lang="fortran"><span style="display:flex;"><span><span style="color:#66d9ef">subroutine</span> example(a, b)</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">real</span> <span style="color:#66d9ef">::</span> a(:), b(:)</span></span><span style="display:flex;"><span> a <span style="color:#f92672">=</span> b</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span> <span style="color:#66d9ef">subroutine</span></span></span></code></pre>
</div>
<p>
(<a href="https://godbolt.org/z/8j3W46j3j">Compiler Explorer</a>)
</p>
<p>
The<a href="https://fortran-lang.org/learn/quickstart/organising_code/">subroutine</a>
<code>example</code>assigns array <code>b</code> to array
<code>a</code>.
</p>
<p>
It is tempting to think of the IRs in a ‚Äústack‚Äù where each one is
convertedinto the next. However, MLIR allows multiple ‚Äúdialects‚Äù of MLIR
to exist in thesame file.
</p>
<p>
(The steps shown here are the most important ones for Flang. In reality
thereare many more between Fortran and LLVM IR.)
</p>
<p>
In the first step, Flang produces a file that is a mixture of HLFIR,
FIRand the built-in MLIR dialect <code>func</code> (function).
</p>
<div class="highlight">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-mlir" data-lang="mlir"><span style="display:flex;"><span>module attributes {&lt;...&gt;} {</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">func</span>.<span style="color:#66d9ef">func</span> <span style="color:#a6e22e">@_QPexample</span>(%arg0: !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt; {fir.bindc_name = <span style="color:#e6db74">&#34;a&#34;</span>}, %arg1: !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt; {fir.bindc_name = <span style="color:#e6db74">&#34;b&#34;</span>}) {</span></span><span style="display:flex;"><span> %0 = fir.dummy_scope : !fir.dscope</span></span><span style="display:flex;"><span> %1:2 = hlfir.declare %arg0 dummy_scope %0 {uniq_name = <span style="color:#e6db74">&#34;_QFexampleEa&#34;</span>} : (!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;, !fir.dscope) -&gt; (!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;)</span></span><span style="display:flex;"><span> %2:2 = hlfir.declare %arg1 dummy_scope %0 {uniq_name = <span style="color:#e6db74">&#34;_QFexampleEb&#34;</span>} : (!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;, !fir.dscope) -&gt; (!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;)</span></span><span style="display:flex;"><span> hlfir.assign %2#0 to %1#0 : !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">return</span></span></span><span style="display:flex;"><span> }</span></span><span style="display:flex;"><span>}</span></span></code></pre>
</div>
<p>
For example, the ‚Äúdummy arguments‚Äù
(the<a href="https://fortran-lang.org/en/learn/quickstart/organising_code/#subroutines">arguments
of a subroutine</a>)are declared with <code>hlfir.declare</code> but
their type is specified with <code>fir.array</code>.
</p>
<p>
As MLIR allows multiple dialects to exist in the same file, there is no
need forHLFIR to have a <code>hlfir.array</code> that duplicates
<code>fir.array</code>, unless HLFIR wantedto handle that differently.
</p>
<p>
The next step is to convert HLFIR into FIR:
</p>
<div class="highlight">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-mlir" data-lang="mlir"><span style="display:flex;"><span>module attributes {&lt;...&gt;} {</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">func</span>.<span style="color:#66d9ef">func</span> <span style="color:#a6e22e">@_QPexample</span>(&lt;...&gt;) {</span></span><span style="display:flex;"><span> &lt;...&gt;</span></span><span style="display:flex;"><span> %c3_i32 = arith.<span style="color:#66d9ef">constant</span> <span style="color:#ae81ff">3</span> : <span style="color:#66d9ef">i32</span></span></span><span style="display:flex;"><span> %7 = fir.convert %0 : (!fir.ref&lt;!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;&gt;) -&gt; !fir.ref&lt;!fir.box&lt;none&gt;&gt;</span></span><span style="display:flex;"><span> %8 = fir.convert %5 : (!fir.box&lt;!fir.array&lt;<span style="color:#ae81ff">?x</span><span style="color:#66d9ef">f32</span>&gt;&gt;) -&gt; !fir.box&lt;none&gt;</span></span><span style="display:flex;"><span> %9 = fir.convert %6 : (!fir.ref&lt;!fir.char&lt;<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">17</span>&gt;&gt;) -&gt; !fir.ref&lt;<span style="color:#66d9ef">i8</span>&gt;</span></span><span style="display:flex;"><span> %10 = fir.call <span style="color:#a6e22e">@_FortranAAssign</span>(%7, %8, %9, %c3_i32) : (!fir.ref&lt;!fir.box&lt;none&gt;&gt;, !fir.box&lt;none&gt;, !fir.ref&lt;<span style="color:#66d9ef">i8</span>&gt;, <span style="color:#66d9ef">i32</span>) -&gt; none</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">return</span></span></span><span style="display:flex;"><span> }</span></span><span style="display:flex;"><span>&lt;...&gt;</span></span><span style="display:flex;"><span>}</span></span></code></pre>
</div>
<p>
Then this bundle of MLIR dialects is converted into LLVM IR:
</p>
<div class="highlight">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-mlir" data-lang="mlir"><span style="display:flex;"><span>define void <span style="color:#a6e22e">@example_</span>(ptr %0, ptr %1) {</span></span><span style="display:flex;"><span> &lt;...&gt;</span></span><span style="display:flex;"><span> store { ptr, <span style="color:#66d9ef">i64</span>, <span style="color:#66d9ef">i32</span>, <span style="color:#66d9ef">i8</span>, <span style="color:#66d9ef">i8</span>, <span style="color:#66d9ef">i8</span>, <span style="color:#66d9ef">i8</span>, [<span style="color:#ae81ff">1 x</span> [<span style="color:#ae81ff">3 x</span> <span style="color:#66d9ef">i64</span>]] } %37, ptr %3, align <span style="color:#ae81ff">8</span></span></span><span style="display:flex;"><span> call void <span style="color:#a6e22e">@llvm.memcpy.p0.p0.i32</span>(ptr %5, ptr %4, <span style="color:#66d9ef">i32</span> <span style="color:#ae81ff">48</span>, <span style="color:#66d9ef">i1</span> false)</span></span><span style="display:flex;"><span> %38 = call {} <span style="color:#a6e22e">@_FortranAAssign</span>(ptr %5, ptr %3, ptr <span style="color:#a6e22e">@_QQclX2F6170702F6578616D706C652E66393000</span>, <span style="color:#66d9ef">i32</span> <span style="color:#ae81ff">3</span>)</span></span><span style="display:flex;"><span> ret void</span></span><span style="display:flex;"><span>}</span></span><span style="display:flex;"><span>&lt;...&gt;</span></span></code></pre>
</div>
<p>
This LLVM IR passes through the standard compilation pipeline that clang
also uses.Eventually being converted into target
specific<a href="https://llvm.org/docs/MIRLangRef.html">Machine IR</a>
(MIR), into assembly andfinally into a binary program.
</p>
<ul>
<li>
Fortran
</li>
<li>
MLIR (including HLFIR and FIR)
</li>
<li>
MLIR (including FIR)
</li>
<li>
LLVM IR
</li>
<li>
MIR
</li>
<li>
Assembly
</li>
<li>
Binary
</li>
</ul>
<p>
At each stage, the optimisations most suited to that stage are done.For
example, while you have HLFIR you could optimise array accesses because
at thatpoint you have the most information about how the Fortran treats
arrays.
</p>
<p>
If Flang were to do this later on, in LLVM IR, it would be much more
difficult.Either the information would be lost or incomplete, or you
would be at a stage inthe pipeline where you cannot assume that you
started with a specific sourcelanguage.
</p>
<h1 id="openmp-to-everyone">
OpenMP to Everyone
</h1>
<p>
<strong>Note:</strong> Most of the points made in this section also
apply to <a href="https://www.openacc.org/">OpenACC</a> support in
Flang. In the interest of brevity, Iwill only describe OpenMP in this
article. You can find more about OpenACCin this
<a href="https://www.youtube.com/watch?v=vVmCLdSboWc">presentation</a>.
</p>
<h2 id="openmp-basics">
OpenMP Basics
</h2>
<p>
<a href="https://www.openmp.org/">OpenMP</a> is a standardised API for
addingparallelism to C, C++ and Fortran programs.
</p>
<p>
Programmers mark parts of their code with ‚Äúdirectives‚Äù. These
directivestell the compiler how the work of the program should be
distributed.Based on this, the compiler transforms the code and inserts
calls to anOpenMP runtime library for certain operations.
</p>
<p>
This is a Fortran example:
</p>
<div class="highlight">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Fortran" data-lang="Fortran"><span style="display:flex;"><span><span style="color:#66d9ef">SUBROUTINE</span> SIMPLE(N, A, B)</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">INTEGER</span> I, N</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">REAL</span> B(N), A(N)</span></span><span style="display:flex;"><span><span style="color:#75715e">!$OMP PARALLEL DO</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span> <span style="color:#66d9ef">DO</span> I<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,N</span></span><span style="display:flex;"><span> B(I) <span style="color:#f92672">=</span> (A(I) <span style="color:#f92672">+</span> A(I<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.0</span></span></span><span style="display:flex;"><span> <span style="color:#66d9ef">ENDDO</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">END</span> <span style="color:#66d9ef">SUBROUTINE</span> SIMPLE</span></span></code></pre>
</div>
<p>
(from
<a href="https://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf">‚ÄúOpenMP
Application Programming Interface Examples‚Äù</a>,
<a href="https://godbolt.org/z/chjzs3o6r">Compiler Explorer</a>)
</p>
<p>
<strong>Note:</strong> Fortran arrays are
<a href="https://fortran-lang.org/en/learn/quickstart/arrays_strings/">one-based</a>
by default. So the first element is at index 1. This example reads the
previous element as well, so it starts <code>I</code> at 2.
</p>
<p>
<code>!<span class="math inline">$OMP PARALLEL DO&lt;/code&gt; is a
directive in the form of a Fortran comment (Fortrancomments start with
&lt;code&gt;!&lt;/code&gt;).&lt;code&gt;PARALLEL DO&lt;/code&gt; starts
a parallel &amp;ldquo;region&amp;rdquo; thatincludes the code from
&lt;code&gt;DO&lt;/code&gt; to
&lt;code&gt;ENDDO&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This tells the
compiler that the work in the &lt;code&gt;DO&lt;/code&gt; loop should be
shared amongstall the threads available to the
program.&lt;/p&gt;&lt;p&gt;Clang has &lt;a
href="https://blog.llvm.org/2015/05/openmp-support_22.html"&gt;supported
OpenMP&lt;/a&gt;for many years now. The equivalent C++ code
is:&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-C++" data-lang="C++"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&lt;span
style="color:#66d9ef"&gt;void&lt;/span&gt; &lt;span
style="color:#a6e22e"&gt;simple&lt;/span&gt;(&lt;span
style="color:#66d9ef"&gt;int&lt;/span&gt; n, &lt;span
style="color:#66d9ef"&gt;float&lt;/span&gt; &lt;span
style="color:#f92672"&gt;*&lt;/span&gt;a, &lt;span
style="color:#66d9ef"&gt;float&lt;/span&gt; &lt;span
style="color:#f92672"&gt;*&lt;/span&gt;b)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#66d9ef"&gt;int&lt;/span&gt;
i;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#75715e"&gt;#pragma omp parallel
for&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&lt;span
style="color:#75715e"&gt;&lt;/span&gt; &lt;span
style="color:#66d9ef"&gt;for&lt;/span&gt; (i&lt;span
style="color:#f92672"&gt;=&lt;/span&gt;&lt;span
style="color:#ae81ff"&gt;1&lt;/span&gt;; i&lt;span
style="color:#f92672"&gt;&amp;lt;&lt;/span&gt;n; i&lt;span
style="color:#f92672"&gt;++&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; b[i] &lt;span
style="color:#f92672"&gt;=&lt;/span&gt; (a[i] &lt;span
style="color:#f92672"&gt;+&lt;/span&gt; a[i&lt;span
style="color:#f92672"&gt;-&lt;/span&gt;&lt;span
style="color:#ae81ff"&gt;1&lt;/span&gt;]) &lt;span
style="color:#f92672"&gt;/&lt;/span&gt; &lt;span
style="color:#ae81ff"&gt;2.0&lt;/span&gt;;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(&lt;a
href="https://godbolt.org/z/Yh9jb8rKe"&gt;Compiler
Explorer&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;For C++, the directive is in the
form of a &lt;code&gt;#pragma&lt;/code&gt; and attachedto the
&lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;&lt;p&gt;LLVM IR does not
know anything about OpenMP specifically, so Clang does all thework of
converting the intent of the directives into LLVM IR. The output
fromClang looks like this:&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre
tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-mlir" data-lang="mlir"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;define dso_local void &lt;span
style="color:#a6e22e"&gt;@simple&lt;/span&gt;(int, float*,
float*)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; (&lt;span
style="color:#66d9ef"&gt;i32&lt;/span&gt; noundef %n, ptr noundef %a,
ptr noundef %b) &amp;lt;...&amp;gt; {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;entry:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void (&amp;lt;...&amp;gt;)
&lt;span
style="color:#a6e22e"&gt;@__kmpc_fork_call&lt;/span&gt;(&lt;span
style="color:#a6e22e"&gt;@simple&lt;/span&gt; &amp;lt;...&amp;gt;
(.omp_outlined) &amp;lt;...&amp;gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ret
void&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;define internal void &lt;span
style="color:#a6e22e"&gt;@simple&lt;/span&gt;(int, float*, float*)
(.omp_outlined)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; (ptr &amp;lt;...&amp;gt;
%.global_tid.,&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ptr &amp;lt;...&amp;gt;
%.bound_tid.,&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ptr &amp;lt;...&amp;gt;
%n,&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ptr &amp;lt;...&amp;gt;
%b,&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ptr &amp;lt;...&amp;gt; %a)
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;entry:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void &lt;span
style="color:#a6e22e"&gt;@__kmpc_for_static_init_4&lt;/span&gt;(&amp;lt;...&amp;gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;omp.inner.for.body.i:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;omp.loop.exit.i:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void &lt;span
style="color:#a6e22e"&gt;@__kmpc_for_static_fini&lt;/span&gt;(&amp;lt;...&amp;gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ret
void&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(output
edited for readability)&lt;/p&gt;&lt;p&gt;The body of
&lt;code&gt;simple&lt;/code&gt; no longer does all the work. Instead it
uses&lt;code&gt;__kmpc_fork_call&lt;/code&gt; to tell the OpenMP&lt;a
href="https://github.com/llvm/llvm-project/tree/main/openmp"&gt;runtime
library&lt;/a&gt;to run another function, &lt;code&gt;simple
(.omp_outlined)&lt;/code&gt; to do the work.&lt;/p&gt;&lt;p&gt;This
second function is referred to as a &amp;ldquo;micro task&amp;rdquo;.
The runtime librarysplits the work across many instances of the micro
task and each timethe micro task function is called, it gets a different
slice of the work.&lt;/p&gt;&lt;p&gt;The number of instances is only
known at runtime, and can be controlled withsettings such as &lt;a
href="https://www.openmp.org/spec-html/5.0/openmpse50.html"&gt;&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The
LLVM IR representation of &lt;code&gt;simple
(.omp_outlined)&lt;/code&gt; includes labels
like&lt;code&gt;omp.loop.exit.i&lt;/code&gt;, but these are not specific
to OpenMP. They are just normal LLVM IRlabels whose name includes
&lt;code&gt;omp&lt;/code&gt;.&lt;/p&gt;&lt;h2
id="sharing-clangs-openmp-knowledge"&gt;Sharing Clang&amp;rsquo;s OpenMP
Knowledge&lt;/h2&gt;&lt;p&gt;Shortly after Flang was approved to join
the LLVM Project, it was proposed thatFlang should share OpenMP support
code with Clang.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;This is an RFC for
the design of the OpenMP front-ends under the LLVMumbrella. It is
necessary to talk about this now as Flang (aka. F18) ismaturing at a
very promising rate and about to become a sub-project nextto
Clang.&lt;/p&gt;&lt;p&gt;TLDR;Keep AST nodes and Sema separated but
unify LLVM-IR generation forOpenMP constructs based on the (almost)
identical OpenMP
directivelevel.&lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&amp;ldquo;[RFC]
Proposed interplay of Clang &amp;amp; Flang &amp;amp; LLVM wrt.
OpenMP&amp;rdquo;,Johannes Doerfert (Lawrence Livermore National
Laboratory), May 2019 (only one&lt;a
href="https://discourse.llvm.org/t/rfc-proposed-interplay-of-clang-flang-llvm-wrt-openmp-flang-dev/51905"&gt;part&lt;/a&gt;of
this still exists online, this quote is from a copy of the other part,
which was provided to me).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For our
purposes, the &amp;ldquo;TLDR&amp;rdquo; means that although both
compilers have differentinternal representations of the OpenMP
directives, they both have to produceLLVM IR from that
representation.&lt;/p&gt;&lt;p&gt;This proposal led to the creation of
the &lt;code&gt;LLVMFrontendOpenMP&lt;/code&gt; library
in&lt;code&gt;llvm&lt;/code&gt;. By using the same class
&lt;code&gt;OpenMPIRBuilder&lt;/code&gt;, there is no need to repeat
work inboth compilers, at least for this part of the OpenMP
pipeline.&lt;/p&gt;&lt;p&gt;As you will see in the following sections,
Flang has diverged from Clang for otherparts of OpenMP
processing.&lt;/p&gt;&lt;h2 id="bringing-openmp-to-mlir"&gt;Bringing
OpenMP to MLIR&lt;/h2&gt;&lt;p&gt;Early in 2020, Kiran Chandramohan
(Arm) &lt;a
href="https://discourse.llvm.org/t/rfc-openmp-dialect-in-mlir/397"&gt;proposed&lt;/a&gt;an
MLIR dialect for OpenMP, for use by
Flang.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;We started the work for the
OpenMP MLIR dialect because of Flang.&amp;hellip; So, MLIR has an OpenMP
dialect because of
Flang.&lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;Kiran
Chandramohan&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This dialect would represent
OpenMP specifically, unlike the generic LLVM IRyou get from
Clang.&lt;/p&gt;&lt;p&gt;If you compile the original Fortran OpenMP
example without OpenMP enabled, youget this MLIR:&lt;/p&gt;&lt;div
class="highlight"&gt;&lt;pre tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-mlir" data-lang="mlir"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;module attributes
{&amp;lt;...&amp;gt;} {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#66d9ef"&gt;func&lt;/span&gt;.&lt;span
style="color:#66d9ef"&gt;func&lt;/span&gt; &lt;span
style="color:#a6e22e"&gt;@_QPsimple&lt;/span&gt;(&amp;lt;...&amp;gt;
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %1:2 = hlfir.declare %arg0
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEn&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %3:2 = hlfir.declare %2
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEi&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %10:2 = hlfir.declare %arg1(%9)
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEa&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %17:2 = hlfir.declare %arg2(%16)
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEb&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %22:2 = fir.do_loop
&amp;lt;...&amp;gt; {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; hlfir.assign %34 to %37 : &lt;span
style="color:#66d9ef"&gt;f32&lt;/span&gt;, !fir.ref&amp;lt;&lt;span
style="color:#66d9ef"&gt;f32&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; fir.store %22#1 to %3#1 :
!fir.ref&amp;lt;&lt;span
style="color:#66d9ef"&gt;i32&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#66d9ef"&gt;return&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(output
edited for readability)&lt;/p&gt;&lt;p&gt;Notice that the
&lt;code&gt;DO&lt;/code&gt; loop has been converted into
&lt;code&gt;fir.do_loop&lt;/code&gt;. Now enableOpenMP and compile
again:&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-mlir" data-lang="mlir"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;module attributes
{&amp;lt;...&amp;gt;} {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#66d9ef"&gt;func&lt;/span&gt;.&lt;span
style="color:#66d9ef"&gt;func&lt;/span&gt; &lt;span
style="color:#a6e22e"&gt;@_QPsimple&lt;/span&gt;(&amp;lt;...&amp;gt;
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %1:2 = hlfir.declare %arg0
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEn&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %10:2 = hlfir.declare %arg1(%9)
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEa&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %17:2 = hlfir.declare %arg2(%16)
&amp;lt;...&amp;gt; {uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEb&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; omp.parallel
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %19:2 = hlfir.declare %18
{uniq_name = &lt;span
style="color:#e6db74"&gt;&amp;#34;_QFsimpleEi&amp;#34;&lt;/span&gt;} :
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; omp.wsloop
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; omp.loop_nest (%arg3) : i32 =
(%c2_i32) to (%20) inclusive step (%c1_i32)
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; hlfir.assign %32 to %35 : &lt;span
style="color:#66d9ef"&gt;f32&lt;/span&gt;, !fir.ref&amp;lt;&lt;span
style="color:#66d9ef"&gt;f32&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
omp.yield&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
omp.terminator&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; &lt;span
style="color:#66d9ef"&gt;return&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(output
edited for readability)&lt;/p&gt;&lt;p&gt;You will see that instead of
&lt;code&gt;fir.do_loop&lt;/code&gt; you have
&lt;code&gt;omp.parallel&lt;/code&gt;,&lt;code&gt;omp.wsloop&lt;/code&gt;
and &lt;code&gt;omp.loop_nest&lt;/code&gt;. &lt;code&gt;omp&lt;/code&gt;
is an MLIR dialect that describes&lt;a
href="https://mlir.llvm.org/docs/Dialects/OpenMPDialect/"&gt;OpenMP&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This
translation of the &lt;code&gt;PARALLEL DO&lt;/code&gt; directive is
much more literal thanthe LLVM IR produced by Clang for
&lt;code&gt;parallel for&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As the
&lt;code&gt;omp&lt;/code&gt; dialect is specifically made for OpenMP, it
can representit much more naturally. This makes it easier to understand
the code and towrite optimisations.&lt;/p&gt;&lt;p&gt;Of course Flang
needs to produce LLVM IR eventually, and to do that ituses the same
&lt;code&gt;OpenMPIRBuilder&lt;/code&gt; class that Clang does. From
theMLIR shown previously, &lt;code&gt;OpenMPIRBuilder&lt;/code&gt;
produces the following LLVM IR:&lt;/p&gt;&lt;div
class="highlight"&gt;&lt;pre tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-mlir" data-lang="mlir"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;define void &lt;span
style="color:#a6e22e"&gt;@simple_&lt;/span&gt; &amp;lt;...&amp;gt;
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;entry:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void (&amp;lt;...&amp;gt;)
&lt;span style="color:#a6e22e"&gt;@__kmpc_fork_call&lt;/span&gt;(
&amp;lt;...&amp;gt; &lt;span
style="color:#a6e22e"&gt;@simple_..omp_par&lt;/span&gt;
&amp;lt;...&amp;gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ret
void&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;define internal void &lt;span
style="color:#a6e22e"&gt;@simple_..omp_par&lt;/span&gt;
&amp;lt;...&amp;gt; {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;omp.par.entry:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void &lt;span
style="color:#a6e22e"&gt;@__kmpc_for_static_init_4u&lt;/span&gt;
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;omp_loop.exit:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; call void &lt;span
style="color:#a6e22e"&gt;@__kmpc_barrier&lt;/span&gt;(&amp;lt;...&amp;gt;)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; ret
void&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;omp_loop.body:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The
LLVM IR produced by Flang and Clang is superficially different,
butstructurally very similar. Considering the differences in source
languageand compiler passes, it is not surprising that they are not
identical.&lt;/p&gt;&lt;h2 id="clangir-and-the-future"&gt;ClangIR and
the Future&lt;/h2&gt;&lt;p&gt;It is surprising that a compiler for a
language as old as Fortran got ahead ofClang (the most well known LLVM
based compiler) when it came to adopting MLIR.&lt;/p&gt;&lt;p&gt;This is
largely due to timing, MLIR is a recent invention and Clang
existedbefore MLIR arrived. Clang also has a legacy to protect, so it is
unlikely tomigrate to a new technology right away.&lt;/p&gt;&lt;p&gt;The
&lt;a href="https://llvm.github.io/clangir/"&gt;ClangIR&lt;/a&gt;
project is working to changeClang to use a new MLIR dialect,
&amp;ldquo;Clang Intermediate Representation&amp;rdquo;
(&amp;ldquo;CIR&amp;rdquo;).Much like Flang and its HLFIR/FIR dialects,
ClangIR will convert C and C++into the CIR
dialect.&lt;/p&gt;&lt;p&gt;Work on OpenMP support for ClangIR has
already &lt;a
href="https://github.com/llvm/clangir/pull/382"&gt;started&lt;/a&gt;,using
the &lt;code&gt;omp&lt;/code&gt; dialect that was originally added for
Flang.&lt;/p&gt;&lt;p&gt;Unfortunately at time of writing the
&lt;code&gt;parallel&lt;/code&gt; directive is not supported byClangIR.
However, if you look at the CIR produced when OpenMP is disabled, you
cansee the &lt;code&gt;cir.for&lt;/code&gt; element that the OpenMP
dialect might replace:&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre
tabindex="0"
style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code
class="language-mlir" data-lang="mlir"&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;module &amp;lt;...&amp;gt;
attributes {&amp;lt;...&amp;gt;} {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.&lt;span
style="color:#66d9ef"&gt;func&lt;/span&gt; &lt;span
style="color:#a6e22e"&gt;@_Z6simpleiPfS_&lt;/span&gt;(
&amp;lt;...&amp;gt; {&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %1 = cir.alloca
&amp;lt;...&amp;gt; [&lt;span
style="color:#e6db74"&gt;&amp;#34;a&amp;#34;&lt;/span&gt;, init]
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %2 = cir.alloca
&amp;lt;...&amp;gt; [&lt;span
style="color:#e6db74"&gt;&amp;#34;b&amp;#34;&lt;/span&gt;, init]
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; %3 = cir.alloca
&amp;lt;...&amp;gt; [&lt;span
style="color:#e6db74"&gt;&amp;#34;i&amp;#34;&lt;/span&gt;]
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.scope
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.for
:&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cond { &amp;lt;...&amp;gt;
}&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; body
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.yield &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc13)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; } step
{&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;
&amp;lt;...&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.yield &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc36)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; } &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc36)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; } &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc36)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; cir.&lt;span
style="color:#66d9ef"&gt;return&lt;/span&gt; &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc2)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt; } &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc31)&lt;/span&gt;&lt;/span&gt;&lt;span
style="display:flex;"&gt;&lt;span&gt;} &lt;span
style="color:#66d9ef"&gt;loc&lt;/span&gt;(#loc)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(on
&lt;a href="https://godbolt.org/z/Yj9EKK7ao"&gt;Compiler
Explorer&lt;/a&gt;)&lt;/p&gt;&lt;h1
id="flang-takes-driving-lessons"&gt;Flang Takes Driving
Lessons&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This
section paraphrases material from&lt;a
href="https://github.com/llvm/llvm-project/blob/main/flang/docs/FlangDriver.md"&gt;&amp;ldquo;Flang
Drivers&amp;rdquo;&lt;/a&gt;.If you want more detail please refer to
that document, or&lt;a
href="https://fabiensanglard.net/dc/index.php"&gt;Driving
Compilers&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;ldquo;Driver&amp;rdquo; in a
compiler context means the part of the compiler that decideshow to
handle a set of options. For instance, when you use the option
&lt;code&gt;-march=armv8a+memtag&lt;/code&gt;,something in Flang knows
that you want to compile for Armv8.0-a with the MemoryTagging Extension
enabled.&lt;/p&gt;&lt;p&gt;&lt;code&gt;-march=&lt;/code&gt; is an
example of a &amp;ldquo;compiler driver&amp;rdquo; option. These options
are what usersgive to the compiler. There is actually a second driver
after this, confusinglycalled the &amp;ldquo;frontend&amp;rdquo; driver,
despite being behind the scenes.&lt;/p&gt;&lt;p&gt;In Flang&amp;rsquo;s
case the &amp;ldquo;compiler driver&amp;rdquo; is
&lt;code&gt;flang&lt;/code&gt; and the &amp;ldquo;frontend
driver&amp;rdquo; is&lt;code&gt;flang -fc1&lt;/code&gt; (they are two
separate tools, contained in the same binary).&lt;/p&gt;&lt;p&gt;They
are separate tools so that the compiler driver can provide an
interfacesuited to compiler users, with stable options that do not
change over time.On the other hand, the frontend driver is suited to
compiler developers, exposesinternal compiler details and does not have
a stable set of options.&lt;/p&gt;&lt;p&gt;You can see the differences
if you add &lt;code&gt;-###&lt;/code&gt; to the compiler
command:&lt;/p&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;$</span>
./bin/flang /tmp/test.f90 -march=armv8a+memtag -### "&lt;‚Ä¶&gt;/flang"
"-fc1" "-triple" "aarch64-unknown-linux-gnu" "-target-feature" "+v8a"
"-target-feature" "+mte" "/usr/bin/ld" ¬†"-o" "a.out"
"-L/usr/lib/gcc/aarch64-linux-gnu/11"</code>
</pre>
<p>
(output edited for readability)
</p>
<p>
The compiler driver has split the compilation into a job for the
frontend(<code>flang -fc1</code>) and the linker (<code>ld</code>).
<code>-march=</code> has been converted into manyarguments to
<code>flang -fc1</code>. This means that if compiler developers decided
tochange how <code>-march=</code> was converted, existing
<code>flang</code> commands would still work.
</p>
<p>
Another responsibility of the compiler driver is to know where to find
librariesand header files. This differs between operating systems and
evendistributions of the same family of operating systems (for example
Linuxdistributions).
</p>
<p>
This created a problem when implementing the compiler driver for Flang.
All thesedetails would take a long time to get right.
</p>
<p>
Luckily, by this time Flang was in the LLVM Project alongside
Clang.Clang already knew how to handle this and had been tested on all
sorts ofsystems over many years.
</p>
<blockquote>
<p>
The intent is to mirror clang, for both the driver and
CompilerInvocation, asmuch as makes sense to do so. The aim is to avoid
re-inventing the wheel andto enable people who have worked with either
the clang or flang entry points,drivers, and frontends to easily
understand the other.
</p>
</blockquote>
<ul>
<li>
<a href="https://discourse.llvm.org/t/rfc-adding-a-fortran-mode-to-the-clang-driver-for-flang/52307">Peter
Waller</a> (Arm)
</li>
</ul>
<p>
Flang became the first in-tree project to use Clang‚Äôs compiler
driverlibrary (<code>clangDriver</code>) to implement its own compiler
driver.
</p>
<p>
This meant that Flang was able to handle all the targets and tools that
Clangcould, without duplicating large amounts of code.
</p>
<h1 id="reflections-on-flang">
Reflections on Flang
</h1>
<p>
We are almost 10 years from the first announcement of what would become
LLVMFlang. In the LLVM monorepo alone there have been close to 10,000
commitsfrom around 400 different contributors. Undoubtedly more in
Classic Flang beforethat.
</p>
<p>
So it is time to hear from users, contributors, and supporters, past
andpresent, about their experiences with Flang.
</p>
<blockquote>
<p>
Collaborating with NVIDIA and PGI on Classic Flang was crucial in
establishingArm in High Performance Computing. It has been an honour to
continue investingin Flang, helping it become an integral part of the
LLVM project and a solidfoundation for building HPC toolchains.
</p>
<p>
We are delighted to see the project reach maturity, as this was the last
step inallowing us to remove all downstream code from our compiler. Look
out for ArmToolchain for Linux 20, which will be a fully open source,
freely availablecompiler based on LLVM 20, available later this year.‚Äù
</p>
</blockquote>
<ul>
<li>
Will Lovett, Director Technology Management at Arm.
</li>
</ul>
<p>
(the following quote is presented in Japanese and English, in case of
differences,Japanese is the authoritative version)
</p>
<blockquote>
<p>
ÂØåÂ£´ÈÄö„ÅØ„ÄÅÊàë„ÄÖ„ÅÆÊï∞ÂçÅÂπ¥„Å´„Çè„Åü„ÇãHPC„ÅÆÁµåÈ®ì„ÇíÈÄö„Åò„Å¶Âüπ„Å£„Åü„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„Éà„ÇíÁî®„ÅÑ„Å¶„ÄÅFlang„ÅÆÊîπÂñÑ„Å´Ë≤¢ÁåÆ„Åß„Åç„Åü„Åì„Å®„ÇíÂ¨â„Åó„ÅèÊÄù„ÅÑ„Åæ„Åô„ÄÇFlang„ÅÆË¶™Âàá„ÅßÂçîÂäõÁöÑ„Å™„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Å´Â§ßÂ§âÊÑüÈäò„ÇíÂèó„Åë„Åæ„Åó„Åü„ÄÇ
</p>
<p>
ÂØåÂ£´ÈÄö„ÅØ„ÄÅ„Çà„ÇäÈ´ò„ÅÑ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Å®‰Ωø„ÅÑ„ÇÑ„Åô„Åï„ÇíÂÆüÁèæ„Åó„ÄÅÊàë„ÄÖ„ÅÆ„Éó„É≠„Çª„ÉÉ„Çµ„ÇíÊúÄÂ§ßÈôê„Å´Ê¥ªÁî®„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÂºï„ÅçÁ∂ö„ÅçFlang„Å´Âèñ„ÇäÁµÑ„Çì„Åß„ÅÑ„Åç„Åæ„Åô„ÄÇFlang„ÅåÊîπÂñÑ„ÇíÁ∂ö„Åë„ÄÅ„É¶„Éº„Ç∂„Éº„ÇíÂ¢ó„ÇÑ„Åó„Å¶„ÅÑ„Åè„Åì„Å®„ÇíÂº∑„ÅèÈ°ò„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ
</p>
<p>
Fujitsu is pleased to have contributed to the improvement of Flang with
ourtest suite, which we have developed through our decades of HPC
experience.Flang‚Äôs helpful and collaborative community really impressed
us.
</p>
<p>
Fujitsu will continue to work on Flang to achieve higher performance
andusability, to make the best of our processors. We hope that Flang
will continueto improve and gain users.
</p>
</blockquote>
<ul>
<li>
ÂØåÂ£´ÈÄöÊ†™Âºè‰ºöÁ§æ „Ç≥„É≥„Éë„Ç§„É©ÈñãÁô∫ÊãÖÂΩì „Éû„Éç„Éº„Ç∏„É£„Éº ÈéåÂ°ö„ÄÄ‰øä (Shun Kamatsuka,
Manager of the Compiler Development Team at Fujitsu).
</li>
</ul>
<blockquote>
<p>
Collaboration between Linaro and Fujitsu on an active CI using
Fujitsu‚Äôstestsuite helped find several issues and make Flang more
robust, inaddition to detecting any regressions early.
</p>
<p>
Linaro has been contributing to Flang development for two years now,
fixing agreat number of issues found by the Fujitsu testsuite.
</p>
</blockquote>
<ul>
<li>
Carlos Seo, Tech Lead at Linaro.
</li>
</ul>
<blockquote>
<p>
<a href="https://scipy.org/">SciPy</a> is a foundational Python package.
It provides easyaccess to scientific algorithms, many of which are
written in Fortran.
</p>
<p>
This has caused a long stream of problems for packaging and shipping
SciPy,especially because users expect first-class support for Windows;a
platform that (prior to Flang) had no license-free Fortran compilersthat
would work with the default platform runtime.
</p>
<p>
As maintainers of SciPy and redistributors in the
<a href="https://conda-forge.org/">conda-forge</a>ecosystem, we hoped
for a solution to this problem for many years. In the end,we switched to
using Flang, and that
<a href="https://labs.quansight.org/blog/building-scipy-with-flang">process</a>was
a minor miracle.
</p>
<p>
Huge thanks to the Flang developers for removing a major source of pain
for us!
</p>
</blockquote>
<ul>
<li>
Axel Obermeier, Quantsight Labs.
</li>
</ul>
<blockquote>
<p>
At the Barcelona Supercomputing Center, like many other HPC centers, we
cannotignore Fortran.
</p>
<p>
As part of our research activities, Flang has allowed us to apply our
work inlong vectors for RISC-V to complex Fortran applications which we
have been ableto run and analyze in our prototype systems. We have also
used Flang to supportan in-house task-based directive-based programming
model.
</p>
<p>
These developments have proved to us that Flang is a powerful
infrastructure.
</p>
</blockquote>
<ul>
<li>
Roger Ferrer Ib√°√±ez, Senior Research Engineer at the Barcelona
Supercomputing Center (BSC).
</li>
</ul>
<blockquote>
<p>
I am thrilled to see the LLVM Flang project achieve this milestone. It
is a uniqueproject in that it marries state of the art compiler
technologies like MLIR withthe venerable Fortran language and its large
community of developers focused onhigh performance compute.
</p>
<p>
Flang has set the standard for LLVM frontends by adopting MLIR and C++17
featuresearlier than others, and I am thrilled to see Clang and other
frontends modernizebased on those experiences.
</p>
<p>
Flang also continues something very precious to me: the LLVM Project‚Äôs
abilityto enable collaboration by uniting people with shared interests
even if theyspan organizations like academic institutions, companies,
and other research groups.
</p>
</blockquote>
<ul>
<li>
Chris Lattner, serving member of the LLVM Board of Directors, co-founder
ofthe LLVM Project, the Clang C++ compiler and MLIR.
</li>
</ul>
<blockquote>
<p>
The need for a more modern Fortran compiler motivated the creation of
the LLVM Flangproject and AMD fully supports that path.
</p>
<p>
In following with community trends, AMD‚Äôs Next-Gen Fortran Compiler will
be adownstream flavor of LLVM Flang and will in time supplant the
current AMD Flangcompiler, a downstream flavor of ‚ÄúClassic Flang‚Äù.
</p>
<p>
Our mission is to allow anyone that is using and developing a Fortran
HPC codebaseto directly leverage the power of AMD‚Äôs GPUs. AMD‚Äôs Next-Gen
Fortran Compiler‚Äôs goalis fulfilling our vision by allowing you to
deploy and accelerate your Fortran codeson AMD GPUs using OpenMP
offloading, and to directly interface and invoke HIP andROCm kernels.
</p>
</blockquote>
<ul>
<li>
AMD,
<a href="https://rocm.blogs.amd.com/ecosystems-and-partners/fortran-journey/README.html">‚ÄúIntroducing
AMD‚Äôs Next-Gen Fortran Compiler‚Äù</a>
</li>
</ul>
<h1 id="getting-involved">
Getting Involved
</h1>
<p>
Flang might not be new anymore, but it is definitely still improving. If
youwant to try Flang on your own projects, you
can<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>it
right now.
</p>
<p>
If you want to contribute, there are many ways to do so. Bug
reports,code contributions, documentation improvements and so on. Flang
follows the<a href="https://llvm.org/docs/Contributing.html">LLVM
contribution process</a> and youcan find links to the forums, community
calls and anything else youmight need
<a href="https://flang.llvm.org/docs/GettingInvolved.html">here</a>.
</p>
<h1 id="credits">
Credits
</h1>
<p>
Thank you to the following people for their contributions to this
article:
</p>
<ul>
<li>
Alex Bradbury (Igalia)
</li>
<li>
Andrzej Warzy≈Ñski (Arm)
</li>
<li>
Axel Obermeier (Quansight Labs)
</li>
<li>
Brad Richardson (Lawrence Berkeley National Laboratory)
</li>
<li>
Carlos Seo (Linaro)
</li>
<li>
Daniel C Chen (IBM)
</li>
<li>
Eric Schweitz (NVIDIA)
</li>
<li>
Hao Jin
</li>
<li>
Jeff Hammond (NVIDIA)
</li>
<li>
Kiran Chandramohan (Arm)
</li>
<li>
Leandro Lupori (Linaro)
</li>
<li>
Luis Machado (Arm)
</li>
<li>
Mehdi Amini
</li>
<li>
Pat McCormick (Los Alamos National Laboratory)
</li>
<li>
Peter Waller (Arm)
</li>
<li>
Steve Scalpone (NVIDIA)
</li>
<li>
Tarun Prabhu (Los Alamos National Laboratory)
</li>
</ul>
<h1 id="further-reading">
Further reading
</h1>
<ul>
<li>
<a href="https://fortran-lang.org/learn/">Learn Fortran</a>
</li>
<li>
<a href="https://labs.quansight.org/blog/building-scipy-with-flang">The
‚Äôeu‚Äô in eucatastrophe ‚Äì Why SciPy builds for Python 3.12 on Windows are
a minor miracle</a>
</li>
<li>
<a href="https://ondrejcertik.com/blog/2021/03/resurrecting-fortran/">Resurrecting
Fortran</a>
</li>
<li>
<a href="https://everythingfunctional.wordpress.com/2021/03/12/the-fortran-package-managers-first-birthday/">The
Fortran Package Manager‚Äôs First Birthday</a>
</li>
<li>
<a href="https://www.youtube.com/watch?v=OvTiKWfhaho">How to write a new
compiler driver? The LLVM Flang perspective</a>
</li>
<li>
<a href="https://www.exascaleproject.org/research-project/flang/">Flang
in the Exascale Supercomputing Project</a>
</li>
</ul>
<p><br></p>
<p><a href="https://blog.llvm.org/posts/2025-03-11-flang-new/"
class="uri">https://blog.llvm.org/posts/2025-03-11-flang-new/</a></p>
<hr />
<h2
id="lilbits-xbox-handheld-nintendo-switch-2-pixel-4a-battery-and-the-return-of-android-lock-screen-widgets">Lilbits:
Xbox handheld, Nintendo Switch 2, Pixel 4a battery, and the return of
Android lock screen widgets</h2>
<p>date: 2025-03-10, from: Liliputing</p>
<p>
The long-awaited Xbox handheld could arrive this year‚Ä¶ but if the latest
rumor is accurate it won‚Äôt be made by Microsoft and it may be more of a
Windows 11 handheld with Xbox branding than a portable console. In other
recent tech news from around the web, if you were wondering why Google
issued a [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/lilbits-xbox-handheld-nintendo-switch-2-pixel-4a-battery-and-the-return-of-android-lock-screen-widgets/">Lilbits:
Xbox handheld, Nintendo Switch 2, Pixel 4a battery, and the return of
Android lock screen widgets</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/lilbits-xbox-handheld-nintendo-switch-2-pixel-4a-battery-and-the-return-of-android-lock-screen-widgets/"
class="uri">https://liliputing.com/lilbits-xbox-handheld-nintendo-switch-2-pixel-4a-battery-and-the-return-of-android-lock-screen-widgets/</a></p>
<hr />
<h2 id="llm-openrouter-0.4">llm-openrouter 0.4</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://github.com/simonw/llm-openrouter/releases/tag/0.4">llm-openrouter
0.4</a></strong>
</p>
I found out this morning that
<a href="https://openrouter.ai/">OpenRouter</a> include support for a
number of (rate-limited)
<a href="https://openrouter.ai/models?max_price=0">free API models</a>.
</p>
<p>
I occasionally run workshops on top of LLMs
(<a href="https://simonwillison.net/2025/Mar/8/cutting-edge-web-scraping/">like
this one</a>) and being able to provide students with a quick way to
obtain an API key against models where they don‚Äôt have to setup billing
is really valuable to me!
</p>
<p>
This inspired me to upgrade my existing
<a href="https://github.com/simonw/llm-openrouter">llm-openrouter</a>
plugin, and in doing so I closed out a bunch of open feature requests.
</p>
<p>
Consider this post the
<a href="https://simonwillison.net/tags/annotated-release-notes/">annotated
release notes</a>:
</p>
<blockquote>
<ul>
<li>
LLM <a href="https://llm.datasette.io/en/stable/schemas.html">schema
support</a> for OpenRouter models that
<a href="https://openrouter.ai/models?order=newest&amp;supported_parameters=structured_outputs">support
structured output</a>.
<a href="https://github.com/simonw/llm-openrouter/issues/23">#23</a>
</li>
</ul>
</blockquote>
<p>
I‚Äôm trying to get support for LLM‚Äôs
<a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">new schema
feature</a> into as many plugins as possible.
</p>
<p>
OpenRouter‚Äôs OpenAI-compatible API includes support for the
<code>response_format</code>
<a href="https://openrouter.ai/docs/features/structured-outputs">structured
content option</a>, but with an important caveat: it only works for some
models, and if you try to use it on others it is silently ignored.
</p>
<p>
I
<a href="https://github.com/OpenRouterTeam/openrouter-examples/issues/20">filed
an issue</a> with OpenRouter requesting they include schema support in
their machine-readable model index. For the moment LLM will let you
specify schemas for unsupported models and will ignore them entirely,
which isn‚Äôt ideal.
</p>
<blockquote>
<ul>
<li>
<code>llm openrouter key</code> command displays information about your
current API key.
<a href="https://github.com/simonw/llm-openrouter/issues/24">#24</a>
</li>
</ul>
</blockquote>
<p>
Useful for debugging and checking the details of your key‚Äôs rate limit.
</p>
<blockquote>
<ul>
<li>
<code>llm -m ‚Ä¶ -o online 1</code> enables
<a href="https://openrouter.ai/docs/features/web-search">web search
grounding</a> against any model, powered by
<a href="https://exa.ai/">Exa</a>.
<a href="https://github.com/simonw/llm-openrouter/issues/25">#25</a>
</li>
</ul>
</blockquote>
<p>
OpenRouter apparently make this feature available to every one of their
supported models! They‚Äôre using new-to-me
<a href="https://exa.ai/">Exa</a> to power this feature, an AI-focused
search engine startup who appear to have built their own index with
their own crawlers (according to
<a href="https://docs.exa.ai/reference/faqs#how-often-is-the-index-updated">their
FAQ</a>). This feature is currently priced by OpenRouter at $4 per 1000
results, and since 5 results are returned for every prompt that‚Äôs 2
cents per prompt.
</p>
<blockquote>
<ul>
<li>
<code>llm openrouter models</code> command for listing details of the
OpenRouter models, including a <code>‚Äìjson</code> option to get JSON and
a <code>‚Äìfree</code> option to filter for just the free models.
<a href="https://github.com/simonw/llm-openrouter/issues/26">#26</a>
</li>
</ul>
</blockquote>
<p>
This offers a neat way to list the available models. There are examples
of the output
<a href="https://github.com/simonw/llm-openrouter/issues/26#issuecomment-2711908704">in
the comments on the issue</a>.
</p>
<blockquote>
<ul>
<li>
New option to specify custom provider routing: <code>-o provider ‚Äò{JSON
here}‚Äô</code>.
<a href="https://github.com/simonw/llm-openrouter/issues/17">#17</a>
</li>
</ul>
</blockquote>
<p>
Part of OpenRouter‚Äôs USP is that it can route prompts to different
providers depending on factors like latency, cost or as a fallback if
your first choice is unavailable - great for if you are using open
weight models like Llama which are hosted by competing companies.
</p>
<p>
<p>The options they provide for routing are
<a href="https://openrouter.ai/docs/features/provider-routing">very
thorough</a> - I had initially hoped to provide a set of CLI options
that covered all of these bases, but I decided instead to reuse their
JSON format and forward those options directly on to the model.</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/llm&quot;&gt;llm&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/projects&quot;&gt;projects&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/plugins&quot;&gt;plugins&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/annotated-release-notes&quot;&gt;annotated-release-notes&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/10/llm-openrouter-04/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/10/llm-openrouter-04/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>NOAA set to fire 1,029 more employees, bringing cuts to near 15
percent.</p>
<p><br></p>
<p><a
href="https://thehill.com/policy/energy-environment/5186175-noaa-staffing-cuts/"
class="uri">https://thehill.com/policy/energy-environment/5186175-noaa-staffing-cuts/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>JD Vance's cousin criticises him for 'belittling' Zelensky.</p>
<p><br></p>
<p><a
href="https://www.bbc.com/news/articles/cm2yl5kevrpo?xtor=AL-71-%5Bpartner%5D-%5Bbbc.news.twitter%5D-%5Bheadline%5D-%5Bnews%5D-%5Bbizdev%5D-%5Bisapi%5D&amp;at_link_id=262EAA4A-FDEF-11EF-9D53-B8CB82023E84&amp;at_ptr_name=twitter&amp;at_medium=social&amp;at_link_origin=BBCWorld&amp;at_campaign=Social_Flow&amp;at_link_type=web_link&amp;at_campaign_type=owned&amp;at_format=link&amp;at_bbc_team=editorial"
class="uri">https://www.bbc.com/news/articles/cm2yl5kevrpo?xtor=AL-71-[partner]-[bbc.news.twitter]-[headline]-[news]-[bizdev]-[isapi]&amp;at_link_id=262EAA4A-FDEF-11EF-9D53-B8CB82023E84&amp;at_ptr_name=twitter&amp;at_medium=social&amp;at_link_origin=BBCWorld&amp;at_campaign=Social_Flow&amp;at_link_type=web_link&amp;at_campaign_type=owned&amp;at_format=link&amp;at_bbc_team=editorial</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>Bluesky has us locked in despite what their CEO says.</p>
<p><br></p>
<p><a
href="https://techcrunch.com/2025/03/10/at-sxsw-bluesky-ceo-jay-graber-pokes-fun-at-mark-zuckerberg-with-latin-phrase-t-shirt/"
class="uri">https://techcrunch.com/2025/03/10/at-sxsw-bluesky-ceo-jay-graber-pokes-fun-at-mark-zuckerberg-with-latin-phrase-t-shirt/</a></p>
<hr />
<h2 id="on-the-miraculous-tradition-in-silicon-valley-thought">On the
Miraculous Tradition in Silicon Valley Thought</h2>
<p>date: 2025-03-10, from: Dave Karpf‚Äôs blog</p>
<p>‚ÄúAnd then a miracle happens‚Äù is not a plan.</p>
<p><br></p>
<p><a
href="https://davekarpf.substack.com/p/on-the-miraculous-tradition-in-silicon"
class="uri">https://davekarpf.substack.com/p/on-the-miraculous-tradition-in-silicon</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>Data Rescue Project.</p>
<p><br></p>
<p><a href="https://www.datarescueproject.org/"
class="uri">https://www.datarescueproject.org/</a></p>
<hr />
<h2
id="urgent-warning-black-mirror-has-entered-the-united-states-with-ai-as-its-handmaiden">Urgent
warning: Black Mirror has entered the United States, with AI as its
handmaiden</h2>
<p>date: 2025-03-10, from: Gary Marcus blog</p>
<p>AI as a smoke screen to cover for authoritarian actions</p>
<p><br></p>
<p><a
href="https://garymarcus.substack.com/p/urgent-warning-black-mirror-has-entered"
class="uri">https://garymarcus.substack.com/p/urgent-warning-black-mirror-has-entered</a></p>
<hr />
<h2
id="wistron-dual-fusion-laptop-concept-includes-a-detachable-handheld-game-streaming-console">Wistron
Dual Fusion laptop concept includes a detachable handheld game streaming
console</h2>
<p>date: 2025-03-10, from: Liliputing</p>
<p>
Taiwanese electronics manufacturer Wistron has designed a gaming laptop
that‚Äôs¬†also a handheld game console. While some companies have achieved
something similar by shrinking their laptops until they‚Äôre small enough
to hold between two hands, the¬†Wistron Dual Fusion takes a different
approach. It‚Äôs a gaming laptop with a full-sized keyboard and display.
But the area¬†below the [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/wistron-dual-fusion-laptop-concept-includes-a-detachable-handheld-game-streaming-console/">Wistron
Dual Fusion laptop concept includes a detachable handheld game streaming
console</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/wistron-dual-fusion-laptop-concept-includes-a-detachable-handheld-game-streaming-console/"
class="uri">https://liliputing.com/wistron-dual-fusion-laptop-concept-includes-a-detachable-handheld-game-streaming-console/</a></p>
<hr />
<h2 id="this-palm-sized-mini-pc-has-an-amd-ryzen-3-5425u-processor">This
palm-sized mini PC has an AMD Ryzen 3 5425U processor</h2>
<p>date: 2025-03-10, from: Liliputing</p>
<p>
Over the past few years we‚Äôve seen a lot of tiny cube-like computers
small enough to fit in the palm of your hand. But most have been powered
by low-cost, low-power Intel processors. Now Topton has introduced a new
model sporting an AMD Ryzen processor. The new Topton M1 is a 78 x 78 x
[‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/this-palm-sized-mini-pc-has-an-amd-ryzen-3-5425u-processor/">This
palm-sized mini PC has an AMD Ryzen 3 5425U processor</a> appeared first
on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/this-palm-sized-mini-pc-has-an-amd-ryzen-3-5425u-processor/"
class="uri">https://liliputing.com/this-palm-sized-mini-pc-has-an-amd-ryzen-3-5425u-processor/</a></p>
<hr />
<h2
id="this-8-inch-mini-laptop-has-an-intel-n150-processor-and-12gb-ram">This
8 inch mini-laptop has an Intel N150 processor and 12GB RAM</h2>
<p>date: 2025-03-10, from: Liliputing</p>
<p>
A handful of Chinese PC makers have been selling mini-laptops with 8
inch displays and Intel Alder Lake-N processors for a few years. Now
we‚Äôre starting to see new models with Intel Twin Lake chips. One of the
first to arrive is the X-Plus Piccolo 2-in-1 mini-laptop with an Intel
N150 processor, which sells for [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/this-8-inch-mini-laptop-has-an-intel-n150-processor-and-12gb-ram/">This
8 inch mini-laptop has an Intel N150 processor and 12GB RAM</a> appeared
first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/this-8-inch-mini-laptop-has-an-intel-n150-processor-and-12gb-ram/"
class="uri">https://liliputing.com/this-8-inch-mini-laptop-has-an-intel-n150-processor-and-12gb-ram/</a></p>
<hr />
<h2
id="generating-automatic-alt-text-with-the-bacalhau-bluesky-bot">Generating
Automatic Alt-Text with the Bacalhau Bluesky Bot</h2>
<p>date: 2025-03-10, from: Bacalhau Blog</p>
<p>(4:10) Using the latest in Large Vision Models (LLaVa), we‚Äôve built a
Bluesky Bot which can generate alt-text for any image in seconds with
Bacalhau</p>
<p><br></p>
<p><a
href="https://blog.bacalhau.org/p/generating-automatic-alt-text-with"
class="uri">https://blog.bacalhau.org/p/generating-automatic-alt-text-with</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-10, from: Feed for Alt USDS)</p>
<p>75% of the VA‚Äôs new website projects had security issues before this
team stepped in. Now, because of firings &amp; forced resignations,
there are very few people who can keep Veterans‚Äô data safe. Gutting
government tech hurts real people.</p>
<p>üîó
https://www.wethebuilders.org/posts/protecting-the-privacy-of-our-veterans</p>
<p>#Veterans #altgov</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3ljzwe6bnsz23"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3ljzwe6bnsz23</a></p>
<hr />
<h2 id="why-werent-women-allowed-to-act-in-shakespeares-plays">Why
weren‚Äôt women allowed to act in Shakespeare‚Äôs plays?</h2>
<p>date: 2025-03-10, from: Guy Kawasaki blog</p>
<p>Elizabeth Steinway, Colorado State University.</p>
<p><br></p>
<p><a
href="https://guykawasaki.substack.com/p/why-werent-women-allowed-to-act-in"
class="uri">https://guykawasaki.substack.com/p/why-werent-women-allowed-to-act-in</a></p>
<hr />
<h2
id="columbia-protestor-arrested-by-ice-moved-to-louisiana-detention-facility">Columbia
Protestor Arrested by ICE Moved to Louisiana Detention Facility</h2>
<p>date: 2025-03-10, from: 404 Media Group</p>
<p>On Sunday Mahmoud Khalil‚Äôs wife was unable to find the protester at
the New Jersey facility he was allegedly detained in. ICE‚Äôs system now
says he is in Louisiana.</p>
<p><br></p>
<p><a
href="https://www.404media.co/columbia-protestor-arrested-by-ice-moved-to-louisiana-detention-facility/"
class="uri">https://www.404media.co/columbia-protestor-arrested-by-ice-moved-to-louisiana-detention-facility/</a></p>
<hr />
<h2
id="photographers-are-on-a-mission-to-fix-wikipedias-famously-bad-celebrity-portraits">Photographers
Are on a Mission to Fix Wikipedia‚Äôs Famously Bad Celebrity
Portraits</h2>
<p>date: 2025-03-10, from: 404 Media Group</p>
<p>WikiPortraits, a group of volunteer photographers, has been covering
festivals and shooting celebrities specifically to improve images in the
public domain.</p>
<p><br></p>
<p><a
href="https://www.404media.co/photographers-are-on-a-mission-to-fix-wikipedias-famously-bad-celebrity-portraits/"
class="uri">https://www.404media.co/photographers-are-on-a-mission-to-fix-wikipedias-famously-bad-celebrity-portraits/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>The shit is going to hit the fan when the Repubs cancel Medicare.
Look at the picture in this post by Maddow on Bluesky. Yeah they're all
on Medicare. They're already going to town hall meetings. They're
boomers. We're not done yet folks.</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/maddow.msnbc.com/post/3ljyapf5uck2r"
class="uri">https://bsky.app/profile/maddow.msnbc.com/post/3ljyapf5uck2r</a></p>
<hr />
<h2 id="unnecessary-violence-is-frowned-upon">Unnecessary Violence Is
Frowned Upon</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: One Foot Tsunami</p>
<p><br></p>
<p><a
href="https://onefoottsunami.com/2025/03/10/unnecessary-violence-is-frowned-upon/"
class="uri">https://onefoottsunami.com/2025/03/10/unnecessary-violence-is-frowned-upon/</a></p>
<hr />
<h2 id="hexboard-midi-controller-strikes-a-chord-with-rp2040">HexBoard
MIDI controller strikes a chord with RP2040</h2>
<p>date: 2025-03-10, from: Raspberry Pi News (.com)</p>
<p>
This RP2040-powered keyboard features 140 addressable RGB LED-lit
mechanical switches, giving rise to distinctive chord shapes and melodic
concepts.
</p>
<p>
The post
<a href="https://www.raspberrypi.com/news/hexboard-midi-controller-strikes-a-chord-with-rp2040/">HexBoard
MIDI controller strikes a chord with RP2040</a> appeared first on
<a href="https://www.raspberrypi.com">Raspberry Pi</a>.
</p>
<p><br></p>
<p><a
href="https://www.raspberrypi.com/news/hexboard-midi-controller-strikes-a-chord-with-rp2040/"
class="uri">https://www.raspberrypi.com/news/hexboard-midi-controller-strikes-a-chord-with-rp2040/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>The Diseases Are Coming.</p>
<p><br></p>
<p><a
href="https://www.theatlantic.com/ideas/archive/2025/03/diseases-doge-trump/681964/?utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share"
class="uri">https://www.theatlantic.com/ideas/archive/2025/03/diseases-doge-trump/681964/?utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share</a></p>
<hr />
<h2 id="the-paranoid-style-in-maga-policy">The Paranoid Style in MAGA
Policy</h2>
<p>date: 2025-03-10, from: Paul Krugman</p>
<p>We basically have government by QAnon</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/the-paranoid-style-in-maga-policy"
class="uri">https://paulkrugman.substack.com/p/the-paranoid-style-in-maga-policy</a></p>
<hr />
<h2
id="dc-romas-second-risc-v-mainboard-for-the-framework-laptop-delivers-up-to-50-tops-of-ai-performance">DC-ROMA‚Äôs
second RISC-V mainboard for the Framework Laptop delivers up to 50 TOPS
of AI performance</h2>
<p>date: 2025-03-10, from: Liliputing</p>
<p>
RISC-V computer maker DeepComputing has made some of the first laptops,
tablets, and other products including a new RISC-V mainboard for
Framework‚Äôs modular 13 inch laptop. Now DeepComputing is jumping on the
AI bandwagon by introducing a new product it calls the DC-ROMA RISC-V AI
PC. It‚Äôs basically a single-board computer with a 64-bit, 8-core [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/dc-romas-second-risc-v-mainboard-for-the-framework-laptop-delivers-up-to-50-tops-of-ai-performance/">DC-ROMA‚Äôs
second RISC-V mainboard for the Framework Laptop delivers up to 50 TOPS
of AI performance</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/dc-romas-second-risc-v-mainboard-for-the-framework-laptop-delivers-up-to-50-tops-of-ai-performance/"
class="uri">https://liliputing.com/dc-romas-second-risc-v-mainboard-for-the-framework-laptop-delivers-up-to-50-tops-of-ai-performance/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>Why cross-posting to Bluesky and Mastodon is not on the roadmap for
WordLand. They are too limited in the features they support for
writers.</p>
<p><br></p>
<p><a href="http://scripting.com/2025/02/24/153052.html"
class="uri">http://scripting.com/2025/02/24/153052.html</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>‚ÄúNobody voted for Elon Musk,‚Äù protestors chanted at a Tesla
dealership in Manhattan yesterday in one of the many protests at the
dealerships associated with Musk‚Äôs cars.</p>
<p><br></p>
<p><a
href="https://heathercoxrichardson.substack.com/p/march-9-2025?publication_id=20533&amp;post_id=158748735&amp;isFreemail=true&amp;r=w33x&amp;triedRedirect=true"
class="uri">https://heathercoxrichardson.substack.com/p/march-9-2025?publication_id=20533&amp;post_id=158748735&amp;isFreemail=true&amp;r=w33x&amp;triedRedirect=true</a></p>
<hr />
<h2
id="meghans-buzzkill-and-a-murder-mystery-solved-after-48-years">Meghan‚Äôs
Buzzkill and a Murder Mystery Solved After 48 Years</h2>
<p>date: 2025-03-10, from: Tina Brown</p>
<p>With Love, Meghan never really recovers from its preposterous opening
scene of Meghan, dressed in a veiled beekeeper‚Äôs space suit, whispering
with her apiarist about the wonder of bees.</p>
<p><br></p>
<p><a
href="https://tinabrown.substack.com/p/meghans-buzzkill-and-a-murder-mystery"
class="uri">https://tinabrown.substack.com/p/meghans-buzzkill-and-a-murder-mystery</a></p>
<hr />
<h2 id="quoting-thane-ruthenis">Quoting Thane Ruthenis</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress">
<p>
It seems to me that ‚Äúvibe checks‚Äù for how smart a model feels are easily
gameable by making it have a better personality.
</p>
<p>
My guess is that it‚Äôs most of the reason Sonnet 3.5.1 was so beloved.
Its personality was made much more <em>appealing</em>, compared to e. g.
OpenAI‚Äôs corporate drones. [‚Ä¶]
</p>
<p>
Deep Research was this for me, at first. Some of its summaries were just
<em>pleasant</em> to read, they felt so information-dense and
intelligent! Not like typical AI slop at all! But then it turned out
most of it was just AI slop underneath anyway, and now my
slop-recognition function has adjusted and the effect is gone.
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress">Thane
Ruthenis</a>, A Bear Case: My Predictions Regarding AI Progress
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/slop&quot;&gt;slop&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/deep-research&quot;&gt;deep-research&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/10/thane-ruthenis/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/10/thane-ruthenis/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>Cops help insure no Tesla sales.</p>
<p><br></p>
<p><a
href="https://mastodon.social/@daveverse.wordpress.com@daveverse.wordpress.com/114135420307616462"
class="uri">https://mastodon.social/@daveverse.wordpress.com@daveverse.wordpress.com/114135420307616462</a></p>
<hr />
<h2 id="building-websites-with-lots-of-little-html-pages">Building
Websites With Lots of Little HTML Pages</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://blog.jim-nielsen.com/2025/lots-of-little-html-pages/">Building
Websites With Lots of Little HTML Pages</a></strong>
</p>
Jim Nielsen coins a confusing new acronym - LLMS for (L)ots of (L)ittle
ht(M)l page(S). He‚Äôs using this to describe his latest site refresh
which makes extensive use of
<a href="https://developer.chrome.com/docs/web-platform/view-transitions/cross-document">cross-document
view transitions</a> - a fabulous new progressive enhancement CSS
technique that‚Äôs
<a href="https://caniuse.com/view-transitions">supported</a> in Chrome
and Safari (and hopefully soon
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1823896">in
Firefox</a>).
</p>
<blockquote>
<p>
With cross-document view transitions getting broader and broader
support, I‚Äôm realizing that building in-page, progressively-enhanced
interactions is more work than simply building two HTML pages and
linking them.
</p>
</blockquote>
<p>
Jim now has small static pages powering his home page filtering
interface and even his navigation menu, with CSS view transitions
configured to smoothly animate between the pages. I think it feels
really good - here‚Äôs what it looked like for me in Chrome (it looked the
same both with and without JavaScript disabled):
</p>
<p>
<img alt="Animated demo - clicking the tabs to swap between Latest, Trending and Hacker News Hits rearranges the list of posts in a smooth animation, then navigating to a post causes its title to enlarge and move to the top while the rest of the article loads in." src="https://static.simonwillison.net/static/2025/llms-demo.gif" />
</p>
<p>
Watching the network panel in my browser, most of these pages are
17-20KB gzipped (~45KB after they‚Äôve decompressed). No wonder it feels
so snappy.
</p>
<p>
I poked around <a href="https://blog.jim-nielsen.com/styles.css">in
Jim‚Äôs CSS</a> and found this relevant code:
</p>
<div class="highlight highlight-source-css">
<pre><span class="pl-k">@view-transition</span> {
  <span class="pl-c1">navigation</span><span class="pl-kos">:</span> auto;
}

.<span class="pl-c1">posts-nav</span> <span class="pl-ent">a</span>[<span class="pl-c1">aria-current</span><span class="pl-c1">=</span><span class="pl-s">"page"</span>]<span class="pl-kos">:</span><span class="pl-c1">not</span>(<span class="pl-kos">:</span><span class="pl-c1">last-child</span>)<span class="pl-kos">:</span><span class="pl-c1">after</span> {
  <span class="pl-c1">border-color</span><span class="pl-kos">:</span> <span class="pl-en">var</span>(<span class="pl-s1">--c-text</span>);
  <span class="pl-c1">view-transition-name</span><span class="pl-kos">:</span> posts-nav;
}

<span class="pl-c">/* Old stuff going out */</span>
::<span class="pl-c1">view-transition-old</span>(<span class="pl-ent">posts-nav</span>) {
  <span class="pl-c1">animation</span><span class="pl-kos">:</span> fade <span class="pl-c1">0.2<span class="pl-smi">s</span></span> linear forwards;
  <span class="pl-c">/* <a href="https://jakearchibald.com/2024/view-transitions-handling-aspect-ratio-changes/">https://jakearchibald.com/2024/view-transitions-handling-aspect-ratio-changes/</a> */</span>
  <span class="pl-c1">height</span><span class="pl-kos">:</span> <span class="pl-c1">100<span class="pl-smi">%</span></span>;
}

<span class="pl-c">/* New stuff coming in */</span>
::<span class="pl-c1">view-transition-new</span>(<span class="pl-ent">posts-nav</span>) {
  <span class="pl-c1">animation</span><span class="pl-kos">:</span> fade <span class="pl-c1">0.3<span class="pl-smi">s</span></span> linear reverse;
  <span class="pl-c1">height</span><span class="pl-kos">:</span> <span class="pl-c1">100<span class="pl-smi">%</span></span>;
}

<span class="pl-k">@keyframes</span> fade {
  <span class="pl-k">from</span> {
    <span class="pl-c1">opacity</span><span class="pl-kos">:</span> <span class="pl-c1">1</span>;
  }
  <span class="pl-k">to</span> {
    <span class="pl-c1">opacity</span><span class="pl-kos">:</span> <span class="pl-c1">0</span>;
  }
}</pre>
</div>
<p>
Jim observes:
</p>
<blockquote>
<p>
This really feels like a game-changer for simple sites. If you can keep
your site simple, it‚Äôs easier to build traditional, JavaScript-powered
on-page interactions as small, linked HTML pages.
</p>
</blockquote>
<p>
I‚Äôve experimented with view transitions for
<a href="https://datasette.io/">Datasette</a> in the past and the
results were very promising. Maybe I‚Äôll pick that up again.
</p>
<p>
<p>Bonus: Jim has a
<a href="https://lobste.rs/s/csr4mw/building_websites_with_lots_little_html#c_ncxssq">clever
JavaScript trick</a> to avoid clicks to the navigation menu being added
to the browser‚Äôs history in the default case.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://lobste.rs/s/csr4mw/building_websites_with_lots_little_html&quot;&gt;lobste.rs&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/css&quot;&gt;css&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/view-transitions&quot;&gt;view-transitions&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/progressive-enhancement&quot;&gt;progressive-enhancement&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/10/building-websites-with-llms/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/10/building-websites-with-llms/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>I dream of getting reviews like this for my software. This time we‚Äôre
making blog writing as fluid as tweeting. When it‚Äôs done we‚Äôll be using
our Wordpress blogs the way you write in twitter, but without the
limits.</p>
<p><br></p>
<p><a
href="https://www.hyperorg.com/blogger/2025/03/08/trying-out-wordland-for-blogging/"
class="uri">https://www.hyperorg.com/blogger/2025/03/08/trying-out-wordland-for-blogging/</a></p>
<hr />
<h2 id="the-french-revolution-the-monarchy-falls-part-3">546. The French
Revolution: The Monarchy Falls (Part 3)</h2>
<p>date: 2025-03-10, from: This is history podcast</p>
<p>
‚ÄúFrom this place and from this day forth commences a new era in the
world‚Äôs history, and you can all say you were present at its birth!‚Äù By
September 1792, the Prussians, under the leadership of the formidable
Duke of Brunswick, were closing in on revolutionary Paris. There, the
streets roiled with the clanging of [‚Ä¶]
</p>
<p>
The post
<a href="https://therestishistory.com/546-the-french-revolution-the-monarchy-falls-part-3/">546.
The French Revolution: The Monarchy Falls (Part 3)</a> appeared first on
<a href="https://therestishistory.com">The Rest is History</a>.
</p>
<p><br></p>
<p><a
href="https://therestishistory.com/546-the-french-revolution-the-monarchy-falls-part-3/"
class="uri">https://therestishistory.com/546-the-french-revolution-the-monarchy-falls-part-3/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-10, from: Dave Winer‚Äôs linkblog)</p>
<p>Rep.¬†Mark Pocan draws packed crowd at town hall meeting, calls out
Rep.¬†Derrick Van Orden on transparency.</p>
<p><br></p>
<p><a
href="https://www.wkow.com/news/politics/rep-mark-pocan-draws-packed-crowd-at-town-hall-meeting-calls-out-rep-derrick-van/article_d66c5c3e-fc84-11ef-8623-5fd46eda89b2.html"
class="uri">https://www.wkow.com/news/politics/rep-mark-pocan-draws-packed-crowd-at-town-hall-meeting-calls-out-rep-derrick-van/article_d66c5c3e-fc84-11ef-8623-5fd46eda89b2.html</a></p>
<hr />
<h2
id="how-to-authenticate-your-http-tunnels-with-inlets-and-oauth.">How to
authenticate your HTTP tunnels with inlets and OAuth.</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: Inlets.dev, cloud
tunneling</p>
<p>In this tutorial you will learn how to secure your tunnelled HTTP
services using the Inlets built-in HTTP authentication.</p>
<p><br></p>
<p><a
href="https://inlets.dev/blog/tutorial/2025/03/10/secure-http-tunnels-with-oauth.html"
class="uri">https://inlets.dev/blog/tutorial/2025/03/10/secure-http-tunnels-with-oauth.html</a></p>
<hr />
<h2
id="understanding-memory-management-part-3-c-smart-pointers">Understanding
Memory Management, Part 3: C++ Smart Pointers</h2>
<p>date: 2025-03-10, updated: 2025-03-10, from: Educated Guesswork
blog</p>
<p><br></p>
<p><a href="https://educatedguesswork.org/posts/memory-management-3/"
class="uri">https://educatedguesswork.org/posts/memory-management-3/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-09, from: Feed for Alt USDS)</p>
<p>68 million Americans rely on Social Security. But did you know SSA
has the lowest staffing levels in 50 years? The people behind the checks
aren‚Äôt just bureaucrats‚Äîthey‚Äôre Americans working nonstop to make sure
you get what you‚Äôre owed. üíô</p>
<p>üîó Read more: buff.ly/KjDzK0T</p>
<p>#SSA #ProtectOurBenefits</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3ljy65bzogh2v"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3ljy65bzogh2v</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Democrats are reeling. Is Stephen A Smith the way back to the White
House?</p>
<p><br></p>
<p><a
href="https://www.theguardian.com/us-news/2025/mar/09/democratic-candidates-stephen-a-smith"
class="uri">https://www.theguardian.com/us-news/2025/mar/09/democratic-candidates-stephen-a-smith</a></p>
<hr />
<h2 id="nobel-prizes-and-the-ai-hype-hall-of-fame">Nobel Prizes and The
AI Hype Hall of Fame</h2>
<p>date: 2025-03-09, from: Gary Marcus blog</p>
<p>GPT-5 may not be here, but just wait til you see the new round of
hype</p>
<p><br></p>
<p><a
href="https://garymarcus.substack.com/p/nobel-prizes-and-the-ai-hype-hall"
class="uri">https://garymarcus.substack.com/p/nobel-prizes-and-the-ai-hype-hall</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Why I love Severance.</p>
<p><br></p>
<p><a
href="https://daveverse.wordpress.com/2025/03/09/why-i-love-severance/"
class="uri">https://daveverse.wordpress.com/2025/03/09/why-i-love-severance/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Ghost is implementing ActivityPub. It seems to be taking a long
time.</p>
<p><br></p>
<p><a href="https://activitypub.ghost.org/actually-i-take-that-back/"
class="uri">https://activitypub.ghost.org/actually-i-take-that-back/</a></p>
<hr />
<h2 id="quoting-steve-yegge">Quoting Steve Yegge</h2>
<p>date: 2025-03-09, updated: 2025-03-09, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://twitter.com/Steve_Yegge/status/1898674257808515242">
<p>
I‚Äôve been using Claude Code for a couple of days, and it has been
absolutely ruthless in chewing through legacy bugs in my gnarly old code
base. It‚Äôs like a wood chipper fueled by dollars. It can power through
shockingly impressive tasks, using nothing but chat. [‚Ä¶]
</p>
<p>
Claude Code‚Äôs form factor is clunky as hell, it has no multimodal
support, and it‚Äôs hard to juggle with other tools. But it doesn‚Äôt
matter. It might look antiquated but it makes Cursor, Windsurf, Augment
and the rest of the lot (yeah, ours too, and Copilot, let‚Äôs be honest)
FEEL antiquated.
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://twitter.com/Steve_Yegge/status/1898674257808515242">Steve
Yegge</a>, who works on Cody at Sourcegraph
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/steve-yegge&quot;&gt;steve-yegge&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/anthropic&quot;&gt;anthropic&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/claude&quot;&gt;claude&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-assisted-programming&quot;&gt;ai-assisted-programming&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/9/steve-yegge/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/9/steve-yegge/#atom-everything</a></p>
<hr />
<h2 id="the-curious-about-everything-newsletter-48">The Curious About
Everything Newsletter #48</h2>
<p>date: 2025-03-09, from: Curious about everything blog</p>
<p>The many interesting things I read in February 2024</p>
<p><br></p>
<p><a href="https://jodiettenberg.substack.com/p/forty-eight"
class="uri">https://jodiettenberg.substack.com/p/forty-eight</a></p>
<hr />
<h2 id="wolf-h3-viewer.glitch.me">wolf-h3-viewer.glitch.me</h2>
<p>date: 2025-03-09, updated: 2025-03-09, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://wolf-h3-viewer.glitch.me/">wolf-h3-viewer.glitch.me</a></strong>
</p>
Neat interactive visualization of Uber‚Äôs
<a href="https://h3geo.org/">H3</a> hexagonal geographical indexing
mechanism.
</p>
<p>
<img alt="Map showing H3 geospatial index hexagons overlaid on the Minneapolis-Saint Paul metropolitan area. Various H3 cell IDs are displayed including &quot;852621b3fffffff&quot;, &quot;852621a7fffffff&quot;, &quot;8527526fffffff&quot;, &quot;85262cd3fffffff&quot;, and &quot;85262c83fffffff&quot;. A sidebar shows input fields for &quot;lat,lon&quot; with a &quot;Go&quot; button and &quot;valid H3 id&quot; with a &quot;Find&quot; button. Text indicates &quot;Current H3 resolution: 5&quot; and &quot;Tip: Clicking an H3 cell will copy its id to the clipboard.&quot; Map attribution shows &quot;Leaflet | ¬© OpenStreetMap contributors&quot;." src="https://static.simonwillison.net/static/2025/h3-map.jpg" />
</p>
<p>
Here‚Äôs <a href="https://github.com/clupasq/h3-viewer">the source
code</a>.
</p>
<p>
Why does H3 use hexagons? Because
<a href="https://www.youtube.com/watch?v=thOifuHs6eY">Hexagons are the
Bestagons</a>:
</p>
<blockquote>
<p>
When hexagons come together, they form three-sided joints 120 degrees
apart. This, for the least material, is the most mechanically stable
arrangement.
</p>
</blockquote>
<p>
<p>Only triangles, squares, and hexagons can tile a plane without gaps,
and of those three shapes hexagons offer the best ratio of perimeter to
area.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://news.ycombinator.com/item?id=43305920#43307944&quot;&gt;Hacker News comment&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/geospatial&quot;&gt;geospatial&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/javascript&quot;&gt;javascript&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Mar/9/h3-viewer/#atom-everything"
class="uri">https://simonwillison.net/2025/Mar/9/h3-viewer/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Evicting the elderly to make a millennial 'live near friends'
utopia.</p>
<p><br></p>
<p><a
href="https://sfstandard.com/2025/03/09/san-francisco-friend-compound-eviction-secret/"
class="uri">https://sfstandard.com/2025/03/09/san-francisco-friend-compound-eviction-secret/</a></p>
<hr />
<h2 id="the-clean-little-secret-of-social-security">The Clean Little
Secret of Social Security</h2>
<p>date: 2025-03-09, from: Paul Krugman</p>
<p>It‚Äôs a pretty good program, and we can afford it</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/the-clean-little-secret-of-social"
class="uri">https://paulkrugman.substack.com/p/the-clean-little-secret-of-social</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>The FAA‚Äôs Troubles Are More Serious Than You Know.</p>
<p><br></p>
<p><a
href="https://www.theatlantic.com/politics/archive/2025/03/faa-trump-elon-plane-crash/681975/?utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share"
class="uri">https://www.theatlantic.com/politics/archive/2025/03/faa-trump-elon-plane-crash/681975/?utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Bluesky is trapping users in the same way Twitter/X did.</p>
<p><br></p>
<p><a
href="https://blogs.lse.ac.uk/impactofsocialsciences/2025/03/03/bluesky-will-trap-academics-in-the-same-way-twitter-x-did/"
class="uri">https://blogs.lse.ac.uk/impactofsocialsciences/2025/03/03/bluesky-will-trap-academics-in-the-same-way-twitter-x-did/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer‚Äôs
linkblog</strong> (date: 2025-03-09, from: Dave Winer‚Äôs linkblog)</p>
<p>Anger at Elon Musk hits Tesla with violence, vandalism in
protest.</p>
<p><br></p>
<p><a href="https://wapo.st/3Fegr9c"
class="uri">https://wapo.st/3Fegr9c</a></p>
<hr />
<p><strong><span class="citation" data-cites="Feed">@Feed</span> for Alt
USDS</strong> (date: 2025-03-09, from: Feed for Alt USDS)</p>
<p>This seems to be an emerging trend‚Ä¶ Let‚Äôs hope it continues to
happen.</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/altusds.bsky.social/post/3ljw5uqqwes2e"
class="uri">https://bsky.app/profile/altusds.bsky.social/post/3ljw5uqqwes2e</a></p>
<hr />
<h2 id="everything-to-everyone">Everything To Everyone</h2>
<p>date: 2025-03-09, updated: 2025-03-09, from: Tedium site</p>
<p>Companies like Amazon and Apple are attempting to do business in so
many spaces that, when the cracks show, they really show. Hence why
Apple Intelligence looks like a hot mess right now.</p>
<p><br></p>
<p><a
href="https://feed.tedium.co/link/15204/16979569/apple-amazon-businesses-unfocused"
class="uri">https://feed.tedium.co/link/15204/16979569/apple-amazon-businesses-unfocused</a></p>
</section>
<footer>
Antenna is a personal aggregation of items found around the web.
Curated with <a href="https://rsdoiel.github.io/skimmer">skimmer</a> and <a href="https://sqlite.org">sqlite</a> then rendered with <a href="https://pandoc.org">Pandoc</a>.
</footer>
</body>
</html>
