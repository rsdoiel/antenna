---
title: snapshots
updated: 2025-08-08 06:08:35
---

# snapshots

(date: 2025-08-08 06:08:35)

---

## It Looks Like a School Vape Detector. A Teen Hacker Showed It Could Become an Audio Bug

date: 2025-08-08, from: 404 Media Group

The Halo 3C is a vape detector installed in schools and public housing. A young hacker found it contains microphones and that it can be turned into an audio bug, raising privacy concerns. 

<br> 

<https://www.404media.co/it-looks-like-a-school-vape-detector-a-teen-hacker-showed-it-could-become-an-audio-bug/>

---

## It‚Äôs Beginning to Smell a Lot Like Stagflation

date: 2025-08-08, from: Paul Krugman

And it's all about Trumponomics 

<br> 

<https://paulkrugman.substack.com/p/its-beginning-to-smell-a-lot-like>

---

## Navigating the EU‚Äôs new Radio Equipment Directive: how Raspberry Pi provides an industrial advantage

date: 2025-08-08, from: Raspberry Pi News (.com)

<p>Raspberry Pi computers are already fully compliant with the core elements of the Radio Equipment Directive.</p>
<p>The post <a href="https://www.raspberrypi.com/news/navigating-the-eus-new-radio-equipment-directive-how-raspberry-pi-provides-an-industrial-advantage/">Navigating the EU&#8217;s new Radio Equipment Directive: how Raspberry Pi provides an industrial advantage</a> appeared first on <a href="https://www.raspberrypi.com">Raspberry Pi</a>.</p>
 

<br> 

<https://www.raspberrypi.com/news/navigating-the-eus-new-radio-equipment-directive-how-raspberry-pi-provides-an-industrial-advantage/>

---

## The AdTech Underbelly

date: 2025-08-08, updated: 2025-08-08, from: Tedium site

Ever wonder why online advertising is so confusing, complicated, and privacy-threatening? A new book by an industry insider helps explain why‚Äîand how Google put its giant hand on the scale. 

<br> 

<https://feed.tedium.co/link/15204/17113618/ari-paparo-yield-google-antitrust-review>

---

## GHC 9.10.3-rc3 is now available

date: 2025-08-08, from: Glasgow Haskell Compiler

<h1>GHC 9.10.3-rc3 is now available</h1>
<h4 class="text-muted">wz1000 - 2025-08-08</h4>

<p>The GHC developers are very pleased to announce the availability
of the second release candidate for GHC 9.10.3. Binary distributions, source
distributions, and documentation are available at <a href="https://downloads.haskell.org/ghc/9.10.3-rc3">downloads.haskell.org</a> and
via <a href="https://www.haskell.org/ghcup/">GHCup</a>.</p>
<p>GHC 9.10.3 is a bug-fix release fixing over 50 issues of a variety of
severities and scopes. A full accounting of these fixes can be found in the
<a href="https://gitlab.haskell.org/ghc/ghc/-/blob/ghc-9.10/docs/users_guide/9.10.3-notes.rst?ref_type=heads&amp;plain=1">release notes</a>. As always, GHC‚Äôs release status, including planned future
releases, can be found on the GHC Wiki <a href="https://gitlab.haskell.org/ghc/ghc/-/wikis/GHC-status">status</a>.</p>
<p>The changes from the first release candidate are:</p>
<ul>
<li>Reverting a change the exports of the <code>Backtrace</code> constructor in the base library that was backported
due to confusion on CLC approvals (<a href="https://gitlab.haskell.org/ghc/ghc/merge_requests/14587">!14587</a>)</li>
<li>Reverting a change to the configure script (<a href="https://gitlab.haskell.org/ghc/ghc/merge_requests/14324">!14324</a>) that dropped probing for ld.gold</li>
</ul>
<p>This release candidate will have a two-week testing period. If all goes well
the final release will be available the week of 22 August 2025.</p>
<p>We would like to thank Well-Typed, Tweag I/O, Juspay, QBayLogic, Channable,
Serokell, SimSpace, the Haskell Foundation, and other anonymous contributors
whose on-going financial and in-kind support has facilitated GHC maintenance
and release management over the years. Finally, this release would not have
been possible without the hundreds of open-source contributors whose work
comprise this release.</p>
<p>As always, do give this release a try and open a <a href="https://gitlab.haskell.org/ghc/ghc/-/issues/new">ticket</a> if you see
anything amiss.</p>
 

<br> 

<http://haskell.org/ghc/blog/20250808-ghc-9.10.3-rc3-released.html>

---

## StarFive VisionFive 2 Lite is a cheap(er) RISC-V single-board computer (crowdfunding)

date: 2025-08-07, from: Liliputing

<p>The¬†VisionFive 2 Lite is a credit card-sized single-board computer (SBC) that looks a lot like a Raspberry Pi. But it&#8217;s actually a smaller, cheaper, and less powerful version of the VisionFive 2 RISC-V SBC that launched a few years ago. The new model has a slower version of the same processor and loses a few [&#8230;]</p>
<p>The post <a href="https://liliputing.com/starfive-visionfive-2-lite-is-a-cheaper-risc-v-single-board-computer-crowdfunding/">StarFive VisionFive 2 Lite is a cheap(er) RISC-V single-board computer (crowdfunding)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/starfive-visionfive-2-lite-is-a-cheaper-risc-v-single-board-computer-crowdfunding/>

---

## Sendable, @unchecked Sendable, @Sendable, sending, and nonsending

date: 2025-08-07, from: Michael Tsai

Fatbobman: Swift&#8217;s concurrency model introduces numerous keywords, some of which are similar in naming and purpose, often causing confusion among developers. This article examines several keywords related to cross-isolation domain passing in Swift concurrency: Sendable, @unchecked Sendable, @Sendable, sending, and nonsending, helping you understand their respective roles and use cases. There&#8217;s a great summary table. [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/07/sendable-unchecked-sendable-sendable-sending-and-nonsending/>

---

## Screens Rejected From the App Store

date: 2025-08-07, from: Michael Tsai

Luc Vandal: I never thought Screens would get rejected for actually asking users to opt in to share anonymous statistics data. [&#8230;] This has been in place for almost a year now. Mac and Vision went through right away. [&#8230;] You want to be transparent to your users and your app gets rejected for it. [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/07/screens-rejected-from-the-app-store/>

---

## Apple Still Investigating Cellular MacBook

date: 2025-08-07, from: Michael Tsai

Tim Hardwick: Recent reports have suggested Apple is actively considering bringing cellular connectivity to the Mac lineup as early as next year, but arch rival Microsoft isn&#8217;t waiting around to find out &#x2013; the company is launching its first 5G-enabled Surface laptop this month.[&#8230;]The Surface Laptop&#8217;s 5G capabilities come as Apple has begun deploying its [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/07/apple-still-investigating-cellular-macbook/>

---

## Apple Announces American Manufacturing Program

date: 2025-08-07, from: Michael Tsai

Apple (Hacker News): Apple today announced a new $100 billion commitment to America, a significant acceleration of its U.S. investment that now totals $600 billion over the next four years. Today&#8217;s announcement includes the ambitious new American Manufacturing Program (AMP), dedicated to bringing even more of Apple&#8217;s supply chain and advanced manufacturing to the U.S. [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/07/apple-announces-american-manufacturing-program/>

---

## CCC fordert Innenminister Dobrindt auf, sein ‚ÄûSicherheitspaket‚Äú zur√ºckzunehmen

date: 2025-08-07, updated: 2025-08-08, from: Chaos Computer Club Updates

Die j√ºngsten Pl√§ne des Bundesinnenministers mit einem ‚ÄûSicherheitspaket 2.0‚Äú sehen folgenschwere neue √úberwachungswerkzeuge f√ºr die Polizeiarbeit vor. Darunter finden sich eine umlackierte Rasterfahndung in einer polizeilichen ‚ÄûSuperdatenbank‚Äú sowie die Einf√ºhrung experimenteller automatischer Biometriedatenvergleiche. Ein breites zivilgesellschaftliches B√ºndnis, darunter der CCC, fordert daher in einem offenen Brief das sofortige Ende dieser Vorhaben. 

<br> 

<https://www.ccc.de/de/updates/2025/unsicherheitspaket-dobrindt>

---

**@Robert's feed at BlueSky** (date: 2025-08-07, from: Robert's feed at BlueSky)

Exciting üòé

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvtk25i2h222>

---

## Xyber Hydra NAS is basically a GMK NucBox G9 with more RAM and better cooling (tiny NAS with four M.2 slots and Intel N150)

date: 2025-08-07, from: Liliputing

<p>The¬†GMK NucBox G9 is an impressively compact computer that&#8217;s designed for use as a network-attached storage device, thanks to its M.2 2280 slots for PCIe 3.0 x2 storage and dual 2.5 Gb Ethernet ports. When Ian reviewed the mini PC earlier this year he found that it offered decent performance, but that it could get [&#8230;]</p>
<p>The post <a href="https://liliputing.com/xyber-hydra-nas-is-basically-a-gmk-nucbox-g9-with-more-ram-and-better-cooling-tiny-nas-with-four-m-2-slots-and-intel-n150/">Xyber Hydra NAS is basically a GMK NucBox G9 with more RAM and better cooling (tiny NAS with four M.2 slots and Intel N150)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/xyber-hydra-nas-is-basically-a-gmk-nucbox-g9-with-more-ram-and-better-cooling-tiny-nas-with-four-m-2-slots-and-intel-n150/>

---

## A CBP Agent Wore Meta Smart Glasses to an Immigration Raid in Los Angeles

date: 2025-08-07, from: 404 Media Group

Video obtained and verified by 404 Media shows a CBP official wearing Meta's AI glasses, which are capable of recording and connecting with AI. ‚ÄúI think it should be seen in the context of an agency that is really encouraging its agents to actively intimidate and terrorize people," one expert said. 

<br> 

<https://www.404media.co/a-cbp-agent-wore-meta-smart-glasses-to-an-immigration-raid-in-los-angeles/>

---

## Previewing GPT-5 at OpenAI's office

date: 2025-08-07, updated: 2025-08-07, from: Simon Willison‚Äôs Weblog

<p>A couple of weeks ago I was invited to OpenAI's headquarters for a "preview event", for which I had to sign both an NDA and a video release waiver. I suspected it might relate to either GPT-5 or the OpenAI open weight models... and <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">GPT-5 it was</a>!</p>
<p>OpenAI had invited five developers: <a href="https://clairevo.com/">Claire Vo</a>, <a href="https://www.youtube.com/@t3dotgg">Theo Browne</a>, <a href="https://x.com/benhylak">Ben Hylak</a>, <a href="https://www.swyx.io/">Shawn @swyx Wang</a>, and myself. We were all given early access to the new models and asked to spend a couple of hours (of paid time) experimenting with them, while being filmed by a professional camera crew.</p>
<p>The resulting video is <a href="https://www.youtube.com/watch?v=-gXmWYQtv5o">now up on YouTube</a>. Unsurprisingly most of my edits related to <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">SVGs of pelicans</a>.</p>
<p><lite-youtube videoid="-gXmWYQtv5o" js-api="js-api"
  title=" Surprising developers with GPT-5 "
  playlabel="Play:  Surprising developers with GPT-5 "
> </lite-youtube></p>

    <p>Tags: <a href="https://simonwillison.net/tags/youtube">youtube</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/7/previewing-gpt-5/#atom-everything>

---

## GPT-5 hot take

date: 2025-08-07, from: Gary Marcus blog

It&#8217;s here, finally, but not everything people dreamt it would be 

<br> 

<https://garymarcus.substack.com/p/gpt-5-hot-take>

---

## GPT-5: Key characteristics, pricing and model card

date: 2025-08-07, updated: 2025-08-07, from: Simon Willison‚Äôs Weblog

<p>I've had preview access to the new GPT-5 model family for the past two weeks (see <a href="https://simonwillison.net/2025/Aug/7/previewing-gpt-5/">related video</a>) and have been using GPT-5 as my daily-driver. It's my new favorite model. It's still an LLM - it's not a dramatic departure from what we've had before - but it rarely screws up and generally feels competent or occasionally impressive at the kinds of things I like to use models for.</p>
<p>I've collected a lot of notes over the past two weeks, so I've decided to break them up into <a href="https://simonwillison.net/series/gpt-5/">a series of posts</a>. This first one will cover key characteristics of the models, how they are priced and what we can learn from the <a href="https://openai.com/index/gpt-5-system-card/">GPT-5 system card</a>.</p>
<ul>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#key-model-characteristics">Key model characteristics</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#position-in-the-openai-model-family">Position in the OpenAI model family</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#pricing-is-aggressively-competitive">Pricing is aggressively competitive</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#more-notes-from-the-system-card">More notes from the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#prompt-injection-in-the-system-card">Prompt injection in the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#thinking-traces-in-the-api">Thinking traces in the API</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#and-some-svgs-of-pelicans">And some SVGs of pelicans</a></li>
</ul>

<h4 id="key-model-characteristics">Key model characteristics</h4>
<p>Let's start with the fundamentals. GPT-5 in ChatGPT is a weird hybrid that switches between different models. Here's what the system card says about that (my highlights in bold):</p>
<blockquote>
<p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and <strong>a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent</strong> (for example, if you say ‚Äúthink hard about this‚Äù in the prompt). [...] Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model.</p>
</blockquote>
<p>GPT-5 in the API is simpler: it's available as three models - <strong>regular</strong>, <strong>mini</strong> and <strong>nano</strong> - which can each be run at one of four reasoning levels: minimal (a new level not previously available for other OpenAI reasoning models), low, medium or high.</p>
<p>The models have an input limit of 272,000 tokens and an output limit (which includes invisible reasoning tokens) of 128,000 tokens. They support text and image for input, text only for output.</p>
<p>I've mainly explored full GPT-5. My verdict: it's just <strong>good at stuff</strong>. It doesn't feel like a dramatic leap ahead from other LLMs but it exudes competence - it rarely messes up, and frequently impresses me. I've found it to be a very sensible default for everything that I want to do. At no point have I found myself wanting to re-run a prompt against a different model to try and get a better result.</p>

<p>Here are the OpenAI model pages for <a href="https://platform.openai.com/docs/models/gpt-5">GPT-5</a>, <a href="https://platform.openai.com/docs/models/gpt-5-mini">GPT-5 mini</a> and <a href="https://platform.openai.com/docs/models/gpt-5-nano">GPT-5 nano</a>. Knowledge cut-off is September 30th 2024 for GPT-5 and May 30th 2024 for GPT-5 mini and nano.</p>

<h4 id="position-in-the-openai-model-family">Position in the OpenAI model family</h4>
<p>The three new GPT-5 models are clearly intended as a replacement for most of the rest of the OpenAI line-up. This table from the system card is useful, as it shows how they see the new models fitting in:</p>
<table>
<thead>
<tr>
<th>Previous model</th>
<th>GPT-5 model</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o</td>
<td>gpt-5-main</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>gpt-5-main-mini</td>
</tr>
<tr>
<td>OpenAI o3</td>
<td>gpt-5-thinking</td>
</tr>
<tr>
<td>OpenAI o4-mini</td>
<td>gpt-5-thinking-mini</td>
</tr>
<tr>
<td>GPT-4.1-nano</td>
<td>gpt-5-thinking-nano</td>
</tr>
<tr>
<td>OpenAI o3 Pro</td>
<td>gpt-5-thinking-pro</td>
</tr>
</tbody>
</table>
<p>That "thinking-pro" model is currently only available via ChatGPT where it is labelled as "GPT-5 Pro" and limited to the $200/month tier. It uses "parallel test time compute".</p>
<p>The only capabilities not covered by GPT-5 are audio input/output and image generation. Those remain covered by models like <a href="https://platform.openai.com/docs/models/gpt-4o-audio-preview">GPT-4o Audio</a> and <a href="https://platform.openai.com/docs/models/gpt-4o-realtime-preview">GPT-4o Realtime</a> and their mini variants and the <a href="https://platform.openai.com/docs/models/gpt-image-1">GPT Image 1</a> and DALL-E image generation models.</p>
<h4 id="pricing-is-aggressively-competitive">Pricing is aggressively competitive</h4>
<p>The pricing is <em>aggressively competitive</em> with other providers.</p>
<ul>
<li>GPT-5: $1.25/million for input, $10/million for output</li>
<li>GPT-5 Mini: $0.25/m input, $2.00/m output</li>
<li>GPT-5 Nano: $0.05/m input, $0.40/m output</li>
</ul>
<p>GPT-5 is priced at half the input cost of GPT-4o, and maintains the same price for output. Those invisible reasoning tokens count as output tokens so you can expect most prompts to use more output tokens than their GPT-4o equivalent (unless you set reasoning effort to "minimal").</p>
<p>The discount for token caching is significant too: 90% off on input tokens that have been used within the previous few minutes. This is particularly material if you are implementing a chat UI where the same conversation gets replayed every time the user adds another prompt to the sequence.</p>
<p>Here's a comparison table I put together showing the new models alongside the most comparable models from OpenAI's competition:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Input $/m</th>
<th>Output $/m</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude Opus 4.1</td>
<td>15.00</td>
<td>75.00</td>
</tr>
<tr>
<td>Claude Sonnet 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Grok 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&gt;200,000)</td>
<td>2.50</td>
<td>15.00</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>2.50</td>
<td>10.00</td>
</tr>
<tr>
<td>GPT-4.1</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>o3</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&lt;200,000)</td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td><strong>GPT-5</strong></td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td>o4-mini</td>
<td>1.10</td>
<td>4.40</td>
</tr>
<tr>
<td>Claude 3.5 Haiku</td>
<td>0.80</td>
<td>4.00</td>
</tr>
<tr>
<td>GPT-4.1 mini</td>
<td>0.40</td>
<td>1.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td>0.30</td>
<td>2.50</td>
</tr>
<tr>
<td>Grok 3 Mini</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr>
<td><strong>GPT-5 Mini</strong></td>
<td>0.25</td>
<td>2.00</td>
</tr>
<tr>
<td>GPT-4o mini</td>
<td>0.15</td>
<td>0.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash-Lite</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>GPT-4.1 Nano</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Lite</td>
<td>0.06</td>
<td>0.24</td>
</tr>
<tr>
<td><strong>GPT-5 Nano</strong></td>
<td>0.05</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Micro</td>
<td>0.035</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>(Here's a good example of a GPT-5 failure: I tried to get it to <a href="https://chatgpt.com/share/6894d804-bca4-8006-ac46-580bf4a9bf5f">output that table sorted itself</a> but it put Nova Micro as more expensive than GPT-5 Nano, so I prompted it to "construct the table in Python and sort it there" and that fixed the issue.)</p>
<h4 id="more-notes-from-the-system-card">More notes from the system card</h4>
<p>As usual, <a href="">the system card</a> is vague on what went into the training data. Here's what it says:</p>
<blockquote>
<p>Like OpenAI‚Äôs other models, the GPT-5 models were trained on diverse datasets, including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. [...] We use advanced data filtering processes to reduce personal information from training data.</p>
</blockquote>
<p>I found this section interesting, as it reveals that writing, code and health are three of the most common use-cases for ChatGPT. This explains why so much effort went into health-related questions,  for both GPT-5 and the recently released OpenAI open weight models.</p>
<blockquote>
<p>We‚Äôve made significant advances in <strong>reducing hallucinations, improving instruction following, and minimizing sycophancy</strong>, and have leveled up GPT-5‚Äôs performance in <strong>three of ChatGPT‚Äôs most common uses: writing, coding, and health</strong>. All of the GPT-5 models additionally feature <strong>safe-completions, our latest approach to safety training</strong> to prevent disallowed content.</p>
</blockquote>
<p>Safe-completions is later described like this:</p>
<blockquote>
<p>Large language models such as those powering ChatGPT have <strong>traditionally been trained to
either be as helpful as possible or outright refuse a user request</strong>, depending on whether the
prompt is allowed by safety policy. [...] Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology
or cybersecurity), where a user request can be completed safely at a high level, but may lead
to malicious uplift if sufficiently detailed or actionable. <strong>As an alternative, we introduced safe-
completions: a safety-training approach that centers on the safety of the assistant‚Äôs output rather
than a binary classification of the user‚Äôs intent</strong>. Safe-completions seek to maximize helpfulness
subject to the safety policy‚Äôs constraints.</p>
</blockquote>
<p>So instead of straight up refusals, we should expect GPT-5 to still provide an answer but moderate that answer to avoid it including "harmful" content.</p>
<p>OpenAI have a paper about this which I haven't read yet (I didn't get early access): <a href="https://openai.com/index/gpt-5-safe-completions/">From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</a>.</p>
<p>Sycophancy gets a mention, unsurprising given <a href="https://simonwillison.net/2025/May/2/what-we-missed-with-sycophancy/">their high profile disaster in April</a>. They've worked on this in the core model:</p>
<blockquote>
<p>System
prompts, while easy to modify, have a more limited impact on model outputs relative to changes in
post-training. For GPT-5, we post-trained our models to reduce sycophancy. Using conversations
representative of production data, we evaluated model responses, then assigned a score reflecting
the level of sycophancy, which was used as a reward signal in training.</p>
</blockquote>
<p>They claim impressive reductions in hallucinations. In my own usage I've not spotted a single hallucination yet, but that's been true for me for Claude 4 and o3 recently as well - hallucination is so much less of a problem with this year's models.</p>
<p><em><strong>Update</strong>: I have had some reasonable pushback against this point, so I should clarify what I mean here. When I use the term "hallucination" I am talking about instances where the model confidently states a real-world fact that is untrue - like the incorrect winner of a sporting event. I'm not talking about the models making other kinds of mistakes - they make mistakes all the time!</em></p>
<p><em>Someone <a href="https://news.ycombinator.com/item?id=44829896">pointed out</a> that it's likely I'm avoiding hallucinations through the way I use the models, and this is entirely correct: as an experienced LLM user I instinctively stay clear of prompts that are likely to trigger hallucinations, like asking a non-search-enabled model for URLs or paper citations. This means I'm much less likely to encounter hallucinations in my daily usage.</em></p>


<blockquote>
<p>One of our focuses when training the GPT-5 models was to reduce the frequency of factual
hallucinations. While ChatGPT has browsing enabled by default, many API queries do not use
browsing tools. Thus, we focused both on training our models to browse effectively for up-to-date
information, and on reducing hallucinations when the models are relying on their own internal
knowledge.</p>
</blockquote>
<p>The section about deception also incorporates the thing where models sometimes pretend they've completed a task that defeated them:</p>
<blockquote>
<p>We placed gpt-5-thinking in a variety of tasks that were partly or entirely infeasible to accomplish,
and <strong>rewarded the model for honestly admitting it can not complete the task</strong>. [...]</p>
<p>In tasks where the agent is required to use tools, such as a web browsing
tool, in order to answer a user‚Äôs query, previous models would hallucinate information when
the tool was unreliable. We simulate this scenario by purposefully disabling the tools or by
making them return error codes.</p>
</blockquote>
<h4 id="prompt-injection-in-the-system-card">Prompt injection in the system card</h4>
<p>There's a section about prompt injection, but it's pretty weak sauce in my opinion.</p>
<blockquote>
<p>Two external red-teaming groups conducted a two-week prompt-injection assessment targeting
system-level vulnerabilities across ChatGPT‚Äôs connectors and mitigations, rather than model-only
behavior.</p>
</blockquote>
<p>Here's their chart showing how well the model scores against the rest of the field. It's an impressive result in comparison - 56.8 attack success rate for gpt-5-thinking, where Claude 3.7 scores in the 60s (no Claude 4 results included here) and everything else is 70% plus:</p>
<p><img src="https://static.simonwillison.net/static/2025/prompt-injection-chart.jpg" alt="A bar chart titled &quot;Behavior Attack Success Rate at k Queries&quot; shows attack success rates (in %) for various AI models at k=1 (dark red) and k=10 (light red). For each model, the total height of the stacked bar represents the k=10 success rate (labeled above each bar), while the lower dark red section represents the k=1 success rate (estimated). From left to right: Llama 3.3 70B ‚Äì k=10: 92.2%, k=1: ~47%; Llama 3.1 405B ‚Äì k=10: 90.9%, k=1: ~38%; Gemini Flash 1.5 ‚Äì k=10: 87.7%, k=1: ~34%; GPT-4o ‚Äì k=10: 86.4%, k=1: ~28%; OpenAI o3-mini-high ‚Äì k=10: 86.4%, k=1: ~41%; Gemini Pro 1.5 ‚Äì k=10: 85.5%, k=1: ~34%; Gemini 2.5 Pro Preview ‚Äì k=10: 85.0%, k=1: ~28%; Gemini 2.0 Flash ‚Äì k=10: 85.0%, k=1: ~33%; OpenAI o3-mini ‚Äì k=10: 84.5%, k=1: ~40%; Grok 2 ‚Äì k=10: 82.7%, k=1: ~34%; GPT-4.5 ‚Äì k=10: 80.5%, k=1: ~28%; 3.5 Haiku ‚Äì k=10: 76.4%, k=1: ~17%; Command-R ‚Äì k=10: 76.4%, k=1: ~28%; OpenAI o4-mini ‚Äì k=10: 75.5%, k=1: ~17%; 3.5 Sonnet ‚Äì k=10: 75.0%, k=1: ~13%; OpenAI o1 ‚Äì k=10: 71.8%, k=1: ~18%; 3.7 Sonnet ‚Äì k=10: 64.5%, k=1: ~17%; 3.7 Sonnet: Thinking ‚Äì k=10: 63.6%, k=1: ~17%; OpenAI o3 ‚Äì k=10: 62.7%, k=1: ~13%; gpt-5-thinking ‚Äì k=10: 56.8%, k=1: ~6%. Legend shows dark red = k=1 and light red = k=10." style="max-width: 100%;" /></p>
<p>On the one hand, a 56.8% attack rate is cleanly a big improvement against all of those other models.</p>
<p>But it's also a strong signal that prompt injection continues to be an unsolved problem! That means that more than half of those k=10 attacks (where the attacker was able to try up to ten times) got through.</p>
<p>Don't assume prompt injection isn't going to be a problem for your application just because the models got better.</p>
<h4 id="thinking-traces-in-the-api">Thinking traces in the API</h4>
<p>I had initially thought that my biggest disappointment with GPT-5 was that there's no way to get at those thinking traces via the API... but that turned out <a href="https://bsky.app/profile/sophiebits.com/post/3lvtceih7222r">not to be true</a>. The following <code>curl</code> command demonstrates that the responses API <code>"reasoning": {"summary": "auto"}</code> is available for the new GPT-5 models:</p>

<pre><code>curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $(llm keys get openai)" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "input": "Give me a one-sentence fun fact about octopuses.",
    "reasoning": {"summary": "auto"}
  }'</code></pre>

<p>Here's <a href="https://gist.github.com/simonw/1d1013ba059af76461153722005a039d">the response</a> from that API call.</p>

<p>Without that option the API will often provide a lengthy delay while the model burns through thinking tokens until you start getting back visible tokens for the final response.</p>
<p>OpenAI offer a new <code>reasoning_effort=minimal</code> option which turns off most reasoning so that tokens start to stream back to you as quickly as possible.</p>
<h4 id="and-some-svgs-of-pelicans">And some SVGs of pelicans</h4>
<p>Naturally I've been running <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">my "Generate an SVG of a pelican riding a bicycle" benchmark</a>. I'll actually spend more time on this in a future post - I have some fun variants I've been exploring - but for the moment here's <a href="https://gist.github.com/simonw/c98873ef29e621c0fe2e0d4023534406">the pelican</a> I got from GPT-5 running at its default "medium" reasoning effort:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-pelican.png" alt="The bicycle is really good, spokes on wheels, correct shape frame, nice pedals. The pelican has a pelican beak and long legs stretching to the pedals." style="max-width: 100%;" /></p>
<p>It's pretty great! Definitely recognizable as a pelican, and one of the best bicycles I've seen yet.</p>
<p>Here's <a href="https://gist.github.com/simonw/9b5ecf61a5fb0794729aa0023aaa504d">GPT-5 mini</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-mini-pelican.png" alt="Blue background with clouds. Pelican has two necks for some reason. Has a good beak though. More gradents and shadows than the GPT-5 one." style="max-width: 100%;" /></p>
<p>And <a href="https://gist.github.com/simonw/3884dc8b186b630956a1fb0179e191bc">GPT-5 nano</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-nano-pelican.png" alt="Bicycle is two circles and some randomish black lines. Pelican still has an OK beak but is otherwise very simple." style="max-width: 100%;" /></p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/7/gpt-5/#atom-everything>

---

## Alan Sugar of Amstrad Speaks to Practical Computing (1985)

date: 2025-08-07, from: Computer ads from the Past

A Quick Look Behind the Scenes at Amstrad. 

<br> 

<https://computeradsfromthepast.substack.com/p/practical-computing-interviewed-alan>

---

## A program to list MySQL or MariaDB default option files

date: 2025-08-07, from: Ocelot SQL GUI blog

I&#8217;ve written an open-source Linux program, pgoptionfiles.c, that tells what a MySQL or MariaDB Connector C library will choose for default option files. In my last post I&#8217;d mentioned how choices can differ and can be hard to know. I said ptrace() was a possible help, and pgoptionfiles proves that ptrace() works. First it forks&#8230; <a class="continue" href="https://ocelot.ca/blog/blog/2025/08/07/a-program-to-list-mysql-or-mariadb-default-option-files/">Continue Reading A program to list MySQL or MariaDB default option files</a> 

<br> 

<https://ocelot.ca/blog/blog/2025/08/07/a-program-to-list-mysql-or-mariadb-default-option-files/>

---

## GPT-5: It Just Does Stuff

date: 2025-08-07, from: One Useful Thing

Putting the AI in Charge 

<br> 

<https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff>

---

## AI-Assisted Search and Rescue

date: 2025-08-07, updated: 2025-08-07, from: One Foot Tsunami

 

<br> 

<https://onefoottsunami.com/2025/08/07/ai-assisted-search-and-rescue/>

---

## Samsung Galaxy Tab S10 Lite leaks point to a cheaper 10 inch tablet

date: 2025-08-07, from: Liliputing

<p>Samsung&#8217;s Android tablets are largely divided into the Galaxy Tab S line of premium devices and Galaxy Tab A line of budget devices. But every few years the company puts out a cheaper Galaxy S series device that offers some premium features like an S Pen while cutting some corners to keep the prices low. [&#8230;]</p>
<p>The post <a href="https://liliputing.com/samsung-galaxy-tab-s10-lite-leaks-point-to-a-cheaper-10-inch-tablet/">Samsung Galaxy Tab S10 Lite leaks point to a cheaper 10 inch tablet</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/samsung-galaxy-tab-s10-lite-leaks-point-to-a-cheaper-10-inch-tablet/>

---

## Archivists Let You Now Read Some of the First Ever Reviews of Mario and Zelda

date: 2025-08-07, from: 404 Media Group

Preservationists at the Video Game History Foundation purchased the rights to Computer Entertainer, the first video game magazine ever written and uploaded it for free. 

<br> 

<https://www.404media.co/computer-entertainer-magazine-archived-online-video-game-history-foundation/>

---

## More than 130,000 Claude, Grok, ChatGPT, and Other LLM Chats Readable on Archive.org

date: 2025-08-07, from: 404 Media Group

The issue of publicly saving shared LLM chats is bigger than just Google. 

<br> 

<https://www.404media.co/more-than-130-000-claude-grok-chatgpt-and-other-llm-chats-readable-on-archive-org/>

---

## Why Less Writing Gets More Results 

date: 2025-08-07, from: Guy Kawasaki blog

What if everything you&#8217;ve been taught about effective writing is wrong? 

<br> 

<https://guykawasaki.substack.com/p/why-less-writing-gets-more-results>

---

## Prison inmates can take college classes, but often with no internet and limited tech

date: 2025-08-07, from: The Markup blog

Students regularly face dead ends, even with access to technology and research resources 

<br> 

<https://themarkup.org/machine-learning/2025/08/07/prison-education>

---

## Infinite Pixels

date: 2025-08-07, from: Eric Meyer blog

In which I push browser engines to their finite limits using infinite values. 

<br> 

<https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/>

---

## The Emperor‚Äôs New Trade Deal

date: 2025-08-07, from: Paul Krugman

Tariffs are bad. A deluded president is worse. 

<br> 

<https://paulkrugman.substack.com/p/the-emperors-new-trade-deal>

---

## Come Back, Gil Amelio

date: 2025-08-07, updated: 2025-08-07, from: Tedium site

If Tim Cook is busy having Apple make unnecessary ornaments as appeasement for political leaders, why not have Gil Amelio lead the company instead? 

<br> 

<https://feed.tedium.co/link/15204/17113041/tim-cook-apple-trump-gift>

---

**@Robert's feed at BlueSky** (date: 2025-08-07, from: Robert's feed at BlueSky)

üëá

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvriwdn6ds2z>

---

**@Robert's feed at BlueSky** (date: 2025-08-07, from: Robert's feed at BlueSky)

Looks fun.

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvrirljxek2z>

---

## Lilbits: 100% tariffs on semiconductor chips, Copilot for PC gaming, and better touchpad support for Android

date: 2025-08-06, from: Liliputing

<p>President Trump has announced plans to impose tariffs of 100% on semiconductors entering the United States, which could dramatically drive up the price of computers, phones, cars, and a wide range of products that rely on those chips. There¬†is a carveout for companies that have committed to manufacturing chips in the United States. With Apple [&#8230;]</p>
<p>The post <a href="https://liliputing.com/lilbits-100-tariffs-on-semiconductor-chips-copilot-for-pc-gaming-and-better-touchpad-support-for-android/">Lilbits: 100% tariffs on semiconductor chips, Copilot for PC gaming, and better touchpad support for Android</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/lilbits-100-tariffs-on-semiconductor-chips-copilot-for-pc-gaming-and-better-touchpad-support-for-android/>

---

## Daily Deals (8-06-2025)

date: 2025-08-06, from: Liliputing

<p>Best Buy is running a sale on Amazon Fire tablets and Kindle eReaders that makes them cheaper to buy from Best Buy than Amazon at the moment. In fact, the Kindle Colorsoft Signature Edition is on sale for $180, which is the lowest price ever for Amazon&#8217;s first Kindle with an E Ink color display. [&#8230;]</p>
<p>The post <a href="https://liliputing.com/daily-deals-8-06-2025/">Daily Deals (8-06-2025)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/daily-deals-8-06-2025/>

---

## A Provoked Attack

date: 2025-08-06, updated: 2025-08-06, from: One Foot Tsunami

 

<br> 

<https://onefoottsunami.com/2025/08/06/a-provoked-attack/>

---

## ICE Is Buying Mobile Iris Scanning Tech for Its Deportation Arm

date: 2025-08-06, from: 404 Media Group

MORIS and I.R.I.S. was designed for Sheriff's Offices to identify known persons with their iris. Now ICE says it plans to buy the tech. 

<br> 

<https://www.404media.co/ice-is-buying-mobile-iris-scanning-tech-for-its-deportation-arm/>

---

## Trump Is Launching an AI Search Engine Powered by Perplexity

date: 2025-08-06, from: 404 Media Group

America‚Äôs scandalous president is teaming up with its most disreputable AI company to make a search engine. 

<br> 

<https://www.404media.co/trump-is-launching-an-ai-search-engine-powered-by-perplexity/>

---

## Jules, our asynchronous coding agent, is now available for everyone

date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://blog.google/technology/google-labs/jules-now-available/">Jules, our asynchronous coding agent, is now available for everyone</a></strong></p>
I wrote about the Jules beta <a href="https://simonwillison.net/2025/May/19/jules/">back in May</a>. Google's version of the OpenAI Codex PR-submitting hosted coding tool graduated from beta today.</p>
<p>I'm mainly linking to this now because I like the new term they are using in this blog entry: <strong>Asynchronous coding agent</strong>. I like it so much I <a href="https://simonwillison.net/tags/asynchronous-coding-agents/">gave it a tag</a>.</p>
<p>I continue to avoid the term "agent" as infuriatingly vague, but I can grudgingly accept it when accompanied by a prefix that clarifies the type of agent we are talking about. "Asynchronous coding agent" feels just about obvious enough to me to be useful.</p>
<p>... I just ran a Google search for <code>"asynchronous coding agent" -jules</code> and came up with a few more notable examples of this name being used elsewhere:</p>
<ul>
<li><a href="https://blog.langchain.com/introducing-open-swe-an-open-source-asynchronous-coding-agent/">Introducing Open SWE: An Open-Source Asynchronous Coding Agent</a> is an announcement from LangChain just this morning of their take on this pattern. They provide a hosted version (bring your own API keys) or you can run it yourself with <a href="https://github.com/langchain-ai/open-swe">their MIT licensed code</a>.</li>
<li>The press release for GitHub's own version of this <a href="https://github.com/newsroom/press-releases/coding-agent-for-github-copilot">GitHub Introduces Coding Agent For GitHub Copilot</a> states that "GitHub Copilot now includes an asynchronous coding agent".</li>
</ul>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=44813854">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/github">github</a>, <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/agent-definitions">agent-definitions</a>, <a href="https://simonwillison.net/tags/asynchronous-coding-agents">asynchronous-coding-agents</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/6/asynchronous-coding-agents/#atom-everything>

---

## Xcode 26 Beta 5

date: 2025-08-06, from: Michael Tsai

Apple: Xcode 26 beta 5 requires a Mac running macOS Sequoia 15.5 or later. The download is back to being a .xip file, and there&#8217;s a slightly smaller Apple Silicon&#x2013;only version. Again, the release notes don&#8217;t actually show what&#8217;s new in this build. The span property of UTF8View does not support the small string representation [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/06/xcode-26-beta-5/>

---

## iOS 26 Developer Beta 5

date: 2025-08-06, from: Michael Tsai

Juli Clover: Apple today provided developers with the fifth betas of iOS 26 and iPadOS 26 for testing purposes, with the updates coming a week after Apple seeded the fourth betas. Juli Clover: Apple is continuing to refine button placement, animations, and design in preparation for launching iOS 26 in September.[&#8230;]Apple added a toggle in [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/06/ios-26-developer-beta-5/>

---

## uBlock Origin Lite for Safari

date: 2025-08-06, from: Michael Tsai

PseudorandomNoise (Hacker News): TLDR uBO Lite is available in Test Flight today for all the Cupertino OS&#8217;s Jen Simmons (Hacker News, Reddit): Over the years, I&#8217;ve heard a lot of developers &#38; other people wish that uBlock Origin was available for Safari. Now it is! Download for Safari 18.6 and Safari 26 beta. VastTension6022: I [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/06/ublock-origin-lite-for-safari/>

---

## Disk Utility: Erase Process Has Failed

date: 2025-08-06, from: Michael Tsai

Especially with macOS Sequoia, I get this error almost every time I connect a hard drive or SSD and try to erase it. Disk Utility reports: The calling process or user lacks the proper privileges to perform this operation The workaround is to quit and relaunch Disk Utility. 

<br> 

<https://mjtsai.com/blog/2025/08/06/disk-utility-erase-process-has-failed/>

---

## My video chat with Gary Ginsberg, friend and confidante of JFK Jr, and a producer of the new CNN documentary American Prince, JFK Jr. 

date: 2025-08-06, from: Tina Brown

Plus, his fix for the divided Democratic Party-- and the light-saber battle between Trump and Rupert Murdoch 

<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://api.substack.com/feed/podcast/170281160/ea4ce0e7696c27934204d2ce6fdf50bf.mp3"></source>
</audio> <a href="https://api.substack.com/feed/podcast/170281160/ea4ce0e7696c27934204d2ce6fdf50bf.mp3" target="_blank">download audio/mpeg</a><br> 

<https://tinabrown.substack.com/p/my-video-chat-with-gary-ginsberg>

---

**@Robert's feed at BlueSky** (date: 2025-08-06, from: Robert's feed at BlueSky)

üìå

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvqrdh2mhc27>

---

## CM5 MINIMA is a tiny $65 Raspberry Pi CM5 carrier board with M.2, Ethernet, and HDMI

date: 2025-08-06, from: Liliputing

<p>The credit card-sized Raspberry Pi Model B line of computers may be small, at just 85 x 56mm. But the Raspberry Compute Module 5 is even smaller, measuring just 55 x 40mm. Unfortunately this tiny computer-on-a-module isn&#8217;t much use on its own, because it lacks the full-sized ports you&#8217;d need to connect a power source, [&#8230;]</p>
<p>The post <a href="https://liliputing.com/cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m-2-ethernet-and-hdmi/">CM5 MINIMA is a tiny $65 Raspberry Pi CM5 carrier board with M.2, Ethernet, and HDMI</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m-2-ethernet-and-hdmi/>

---

## Tom MacWright: Observable Notebooks 2.0

date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://macwright.com/2025/07/31/observable-notebooks-2">Tom MacWright: Observable Notebooks 2.0</a></strong></p>
Observable announced <a href="https://observablehq.com/notebook-kit/">Observable Notebooks 2.0</a> last week - the latest take on their JavaScript notebook technology, this time with an <a href="https://observablehq.com/notebook-kit/kit">open file format</a> and a brand new <a href="https://observablehq.com/notebook-kit/desktop">macOS desktop app</a>.</p>
<p>Tom MacWright worked at Observable during their first iteration and here provides thoughtful commentary from an insider-to-outsider perspective on how their platform has evolved over time.</p>
<p>I particularly appreciated this aside on the downsides of evolving your own not-quite-standard language syntax:</p>
<blockquote>
<p>Notebook Kit and Desktop <a href="https://observablehq.com/notebook-kit/#vanilla-java-script">support vanilla JavaScript</a>, which is excellent and cool. The Observable changes to JavaScript were always tricky and meant that we struggled to use off-the-shelf parsers, and users couldn't use standard JavaScript tooling like eslint. This is stuff like the <code>viewof</code> operator which meant that <a href="https://observablehq.com/@observablehq/observable-javascript">Observable was not JavaScript</a>. [...] <em>Sidenote</em>: I now work on <a href="https://www.val.town/">Val Town</a>, which is also a platform based on writing JavaScript, and when I joined it <em>also</em> had a tweaked version of JavaScript. We used the <code>@</code> character to let you 'mention' other vals and implicitly import them. This was, like it was in Observable, not worth it and we switched to standard syntax: don't mess with language standards folks!</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/observable">observable</a>, <a href="https://simonwillison.net/tags/tom-macwright">tom-macwright</a>, <a href="https://simonwillison.net/tags/val-town">val-town</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/6/observable-notebooks-20/#atom-everything>

---

## Fruit Jam credit card-sized PC with an RP2350B chip launches for $40

date: 2025-08-06, from: Liliputing

<p>Earlier this year Adafruit introduced a credit card-sized computer called the Fruit Jam. It&#8217;s the size of a typical Raspberry Pi Model B, but it&#8217;s powered by a low-power Raspberry Pi RP2350B microcontroller. This weekend the company announced that the Fruit Jam was available for purchase for $40. Only a few units were available at [&#8230;]</p>
<p>The post <a href="https://liliputing.com/fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40/">Fruit Jam credit card-sized PC with an RP2350B chip launches for $40</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40/>

---

## Home Depot and Lowe's Share Data From Hundreds of AI Cameras With Cops

date: 2025-08-06, from: 404 Media Group

Home improvement stores are finding ways to share data from their Flock license plate reader cameras with law enforcement, according to public records. 

<br> 

<https://www.404media.co/home-depot-and-lowes-share-data-from-hundreds-of-ai-cameras-with-cops/>

---

## Constitution Sections on Due Process and Foreign Gifts Just Vanished from Congress' Website

date: 2025-08-06, from: 404 Media Group

Part of Article I Section 8, and all of Sections 9 and 10, which address things like habeas corpus, nobility, and militias, are gone from Congress's website for the Constitution.  

<br> 

<https://www.404media.co/constitution-sections-on-due-process-and-foreign-gifts-just-vanished-from-congress-website/>

---

## Million-Year-Old Evidence of Epic Journey Near ‚ÄòHobbit‚Äô Island Discovered by Scientists

date: 2025-08-06, from: 404 Media Group

Stone tools found on the Indonesian island of Sulawesi reveal a long-lost population of human relatives; their identity, and how they crossed the sea, is a mystery. 

<br> 

<https://www.404media.co/indonesian-island-sulawesi-early-humans-hobbits-calio/>

---

## Fairness Isn‚Äôt Just a Feature‚ÄîIt‚Äôs the Future

date: 2025-08-06, from: Purism News and Events

<p>The European Union is making bold moves to reshape the digital landscape‚Äîand if you're a consumer, developer, or platform operator, you‚Äôll want to pay attention. Two major regulatory shifts are now underway: the proposed Digital Fairness Act and the activation of key provisions in the EU AI Act. Together, they signal a new era of transparency, accountability, and ethical design in the digital economy.</p>
<p>The post <a rel="nofollow" href="https://puri.sm/posts/fairness-isnt-just-a-feature-its-the-future/">Fairness Isn‚Äôt Just a Feature‚ÄîIt‚Äôs the Future</a> appeared first on <a rel="nofollow" href="https://puri.sm/">Purism</a>.</p>
 

<br> 

<https://puri.sm/posts/fairness-isnt-just-a-feature-its-the-future/>

---

## AI and Data Privacy Under Scrutiny

date: 2025-08-06, from: Purism News and Events

<p>The promise of AI is seductive: instant answers, personalized insights, and a frictionless interface with the digital world. But beneath the surface of convenience lies a growing privacy crisis‚Äîone that‚Äôs now impossible to ignore.</p>
<p>The post <a rel="nofollow" href="https://puri.sm/posts/ai-and-data-privacy-under-scrutiny/">AI and Data Privacy Under Scrutiny</a> appeared first on <a rel="nofollow" href="https://puri.sm/">Purism</a>.</p>
 

<br> 

<https://puri.sm/posts/ai-and-data-privacy-under-scrutiny/>

---

## The Case for Optimism

date: 2025-08-06, from: Guy Kawasaki blog

It&#8217;s a strategic advantage. 

<br> 

<https://guykawasaki.substack.com/p/the-case-for-optimism>

---

## Watch This Guy‚Äôs Interactive Wooden Pixel Machine Make Art in Real Time

date: 2025-08-06, from: 404 Media Group

Kilopixel by Ben Holmen turns a CNC machine and a thousand wooden blocks into pixel art. 

<br> 

<https://www.404media.co/kilopixel-live-stream-interactive-pixel-art/>

---

## Join me today at 12pET for a video chat with the great political/media insider Gary Ginsberg on the new doc about his friend JFK Jr, and the ongoing smackdown between Trump and the media

date: 2025-08-06, from: Tina Brown

There is much to discuss with Ginsberg, who is consulting producer on American Prince: JFK Jr., a new documentary on CNN. 

<br> 

<https://tinabrown.substack.com/p/join-me-today-at-12pet-for-a-video>

---

## Podcast: Google Is Exposing Peoples‚Äô ChatGPT Secrets

date: 2025-08-06, from: 404 Media Group

Shared ChatGPT indexed by Google; how Wikipedia is fighting AI slop; and the history of how we got to Steam censorship. 

<br> 

<https://www.404media.co/podcast-google-is-exposing-peoples-chatgpt-secrets/>

---

## Quoting Artificial Analysis

date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs Weblog

<blockquote cite="https://x.com/artificialanlys/status/1952887733803991070"><p><strong>gpt-oss-120b is the most intelligent American open weights model, comes behind DeepSeek R1 and Qwen3 235B in intelligence but offers efficiency benefits</strong> [...]</p>
<p>We‚Äôre seeing the 120B beat o3-mini but come in behind o4-mini and o3. The 120B is the most intelligent model that can be run on a single H100 and the 20B is the most intelligent model that can be run on a consumer GPU. [...]</p>
<p>While the larger gpt-oss-120b does not come in above DeepSeek R1 0528‚Äôs score of 59 or Qwen3 235B 2507s score of 64, it is notable that it is significantly smaller in both total and active parameters than both of those models.</p></blockquote>
<p class="cite">&mdash; <a href="https://x.com/artificialanlys/status/1952887733803991070">Artificial Analysis</a>, see also their <a href="https://artificialanalysis.ai/models/open-source">updated leaderboard</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/evals">evals</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/deepseek">deepseek</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gpt-oss">gpt-oss</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/6/artificial-analysis/#atom-everything>

---

## About That Stock Market

date: 2025-08-06, from: Paul Krugman

Policy has gone mad; why aren&#8217;t stocks down? 

<br> 

<https://paulkrugman.substack.com/p/about-that-stock-market>

---

## AI interdimensional phone entertains party guests like it‚Äôs 1999

date: 2025-08-06, from: Raspberry Pi News (.com)

<p>This AI-powered landline phone talks to guests, gives them clues, and sends them on a mystery tour around the house. </p>
<p>The post <a href="https://www.raspberrypi.com/news/ai-interdimensional-phone-entertains-party-guests-like-its-1999/">AI interdimensional phone entertains party guests like it&#8217;s 1999</a> appeared first on <a href="https://www.raspberrypi.com">Raspberry Pi</a>.</p>
 

<br> 

<https://www.raspberrypi.com/news/ai-interdimensional-phone-entertains-party-guests-like-its-1999/>

---

## No, AI is not Making Engineers 10x as Productive

date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">No, AI is not Making Engineers 10x as Productive</a></strong></p>
Colton Voege on "curing your AI 10x engineer imposter syndrome".</p>
<p>There's a lot of rhetoric out there suggesting that if you can't 10x your productivity through tricks like running a dozen Claude Code instances at once you're falling behind. Colton's piece here is a pretty thoughtful exploration of why that likely isn't true. I found myself agreeing with quite a lot of this article.</p>
<p>I'm a pretty huge proponent for AI-assisted development, but I've never found those 10x claims convincing. I've estimated that LLMs make me 2-5x more productive on the parts of my job which involve typing code into a computer, which is itself a small portion of that I do as a software engineer.</p>
<p>That's not too far from this article's assumptions. From the article:</p>
<blockquote>
<p>I wouldn't be surprised to learn AI helps many engineers do certain tasks 20-50% faster, but the nature of software bottlenecks mean this doesn't translate to a 20% productivity increase and certainly not a 10x increase.</p>
</blockquote>
<p>I think that's an under-estimation - I suspect engineers that really know how to use this stuff effectively will get more than a 0.2x increase - but I do think all of the <em>other stuff</em> involved in building software makes the 10x thing unrealistic in most cases.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=44798189">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/6/not-10x/#atom-everything>

---

## OpenFlexure: What It Takes to get Medical Device Certification for an Open Source Microscope

date: 2025-08-06, updated: 2025-08-06, from: nlnet feed

 

<br> 

<https://nlnet.nl/news/2025/20250806-openflexure-interview.html>

---

## GHC 9.10.3-rc2 is now available

date: 2025-08-06, from: Glasgow Haskell Compiler

<h1>GHC 9.10.3-rc2 is now available</h1>
<h4 class="text-muted">wz1000 - 2025-08-06</h4>

<p>The GHC developers are very pleased to announce the availability
of the second release candidate for GHC 9.10.3. Binary distributions, source
distributions, and documentation are available at <a href="https://downloads.haskell.org/ghc/9.10.3-rc2">downloads.haskell.org</a> and
via <a href="https://www.haskell.org/ghcup/">GHCup</a>.</p>
<p>GHC 9.10.3 is a bug-fix release fixing over 50 issues of a variety of
severities and scopes. A full accounting of these fixes can be found in the
<a href="https://gitlab.haskell.org/ghc/ghc/-/blob/ghc-9.10/docs/users_guide/9.10.3-notes.rst?ref_type=heads&amp;plain=1">release notes</a>. As always, GHC‚Äôs release status, including planned future
releases, can be found on the GHC Wiki <a href="https://gitlab.haskell.org/ghc/ghc/-/wikis/GHC-status">status</a>.</p>
<p>The changes from the first release candidate are:</p>
<ul>
<li>Bumping the text submodule to 2.1.3</li>
<li>Reverting a bug fix (<a href="https://gitlab.haskell.org/ghc/ghc/merge_requests/14291">!14291</a>) that restricted previously allowed namespace specifiers (<a href="https://gitlab.haskell.org/ghc/ghc/issues/26250">#26250</a>)</li>
<li>Reverting the bump of the deepseq submodule to 1.5.2.0 (<a href="https://gitlab.haskell.org/ghc/ghc/issues/26251">#26251</a>)</li>
</ul>
<p>This release candidate will have a two-week testing period. If all goes well
the final release will be available the week of 19 August 2025.</p>
<p>We would like to thank Well-Typed, Tweag I/O, Juspay, QBayLogic, Channable,
Serokell, SimSpace, the Haskell Foundation, and other anonymous contributors
whose on-going financial and in-kind support has facilitated GHC maintenance
and release management over the years. Finally, this release would not have
been possible without the hundreds of open-source contributors whose work
comprise this release.</p>
<p>As always, do give this release a try and open a <a href="https://gitlab.haskell.org/ghc/ghc/-/issues/new">ticket</a> if you see
anything amiss.</p>
 

<br> 

<http://haskell.org/ghc/blog/20250806-ghc-9.10.3-rc2-released.html>

---

## Out-Fibbing CPython with the Plush Interpreter

date: 2025-08-06, from: Pointers gone wild blog

 

<br> 

<https://pointersgonewild.com/2025-08-06-out-fibbing-cpython-with-the-plush-interpreter/>

---

## Available today: gpt-oss-20B Model on Windows with GPU Acceleration ‚Äì further pushing the boundaries on the edge

date: 2025-08-05, from: Windows Developer Blog

<p>With OpenAI‚Äôs release of gpt-oss models today, we are thrilled to bring GPU optimized gpt-oss-20B model variants to Windows devices.</p>
<p>This milestone brings powerful, open-source reasoning models to Windows developers, with support for local inferen</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/">Available today: gpt-oss-20B Model on Windows with GPU Acceleration ‚Äì further pushing the boundaries on the edge</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
 

<br> 

<https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/>

---

## OpenAI's new open weight (Apache 2) models are really good

date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs Weblog

<p>The long promised <a href="https://openai.com/index/introducing-gpt-oss/">OpenAI open weight models are here</a>, and they are <em>very</em> impressive. They're available under proper open source licenses - Apache 2.0 - and come in two sizes, 120B and 20B.</p>
<p>OpenAI's own benchmarks are eyebrow-raising - emphasis mine:</p>
<blockquote>
<p>The <strong>gpt-oss-120b</strong> model achieves <strong>near-parity with OpenAI o4-mini</strong> on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The <strong>gpt-oss-20b</strong> model delivers <strong>similar results to OpenAI o3‚Äëmini</strong> on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure.</p>
</blockquote>
<p>o4-mini and o3-mini are <em>really good</em> proprietary models - I was not expecting the open weights releases to be anywhere near that class, especially given their small sizes. That gpt-oss-20b model should run quite comfortably on a Mac laptop with 32GB of RAM.</p>
<p>Both models are mixture-of-experts:</p>
<blockquote>
<p>gpt-oss-120b activates 5.1B parameters per token, while gpt-oss-20b activates 3.6B. The models have 117b and 21b total parameters respectively.</p>
</blockquote>
<p>Something that surprised me even more about the benchmarks was the scores for general knowledge based challenges. I can just about believe they managed to train a strong reasoning model that fits in 20B parameters, but these models score highly on benchmarks like "GPQA Diamond (without tools) PhD-level science questions" too:</p>
<ul>
<li>o3 ‚Äî 83.3%</li>
<li>o4-mini ‚Äî 81.4%</li>
<li>gpt-oss-120b ‚Äî 80.1%</li>
<li>o3-mini ‚Äî 77%</li>
<li>gpt-oss-20b ‚Äî 71.5%</li>
</ul>
<p>A lot of these benchmarks are edging towards saturated.</p>
<ul>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#running-gpt-oss-20b-on-my-mac-with-lm-studio">Running gpt-oss-20b on my Mac with LM Studio</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-low">Pelican on reasoning=low</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-medium">Pelican on reasoning=medium</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-high">Pelican on reasoning=high</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#space-invaders-with-gpt-oss-20b">Space invaders with gpt-oss-20b</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#trying-gpt-oss-120b-via-api-providers">Trying gpt-oss-120b via API providers</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#llama-cpp-is-coming-very-shortly">llama.cpp is coming very shortly</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#gpt-oss-20b-in-ollama">gpt-oss:20b in Ollama</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#the-model-card">Training details from the model card</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#openai-harmony-a-new-format-for-prompt-templates">OpenAI Harmony, a new format for prompt templates</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#the-open-question-for-me-how-good-is-tool-calling-">The open question for me: how good is tool calling?</a></li>
  <li><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#china">Competing with the Chinese open models</a></li>
</ul>
<h4 id="running-gpt-oss-20b-on-my-mac-with-lm-studio">Running gpt-oss-20b on my Mac with LM Studio</h4>
<p>There are already a bunch of different ways to run these models - OpenAI partnered with numerous organizations in advance of the release.</p>
<p>I decided to start with <a href="https://lmstudio.ai/">LM Studio</a>.</p>
<p>I had to update to the most recent version of the app, then install the new model from <a href="https://lmstudio.ai/models/openai/gpt-oss-20b">their openai/gpt-oss-20b</a> page.</p>
<p>First impressions: this is a <em>really good</em> model, and it somehow runs using just 11.72GB of my system RAM.</p>
<p>The model supports three reasoning efforts: low, medium and high. LM Studio makes those available via a dropdown.</p>
<p>Let's try "Generate an SVG of a pelican riding a bicycle":</p>
<h4 id="pelican-on-reasoning-low">Pelican on reasoning=low</h4>
<p>I started <a href="https://gist.github.com/simonw/b71394cc85fe0f048e376392e41586da">with low</a>. It thought for 0.07 seconds and then output this (at 39 tokens a second):</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-20-low.png" alt="" style="max-width: 100%;" /></p>
<p>Except... it output invalid SVG. One of the path elements looked like this:</p>
<pre><code>&lt;!-- Frame --&gt;
&lt;path d="
    M150,250          &lt;!-- rear wheel center --&gt;
    L300,120          &lt;!-- top tube to front --&gt;
    L450,250          &lt;!-- chain stays back to front --&gt;
    L300,350          &lt;!-- seat stays down --&gt;
    Z"
    fill="#e0e0e0" stroke="#555" stroke-width="4"/&gt;
</code></pre>
<p>But you can't put comments inside attributes like that. I fixed this to get the above image.</p>
<h4 id="pelican-on-reasoning-medium">Pelican on reasoning=medium</h4>
<p>I tried again <a href="https://gist.github.com/simonw/642e9e371387fc59a5aad25dcd41b0cc">on medium</a>. This time it thought for 4.44 seconds and gave me this, at 55 tokens/second:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-20-medium.png" alt="" style="max-width: 100%;" /></p>
<h4 id="pelican-on-reasoning-high">Pelican on reasoning=high</h4>
<p>And then <a href="https://gist.github.com/simonw/164a3f1b4b1a971637af279a0ac49be3">on high</a>. My first attempt here failed because it hit the default LM Studio 4096 context limit - so I reloaded the model and set the new limit to the maximum possible.</p>
<p>It jumped up to using 17.35GB RAM (I imagine it might use a lot more with a longer prompt) and after thinking for 5 minutes and 50 seconds (!) gave me this:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-20-high.png" alt="" style="max-width: 100%;" /></p>
<p>It thought <em>really hard</em> to get there. Here's a snippet of <a href="https://gist.github.com/simonw/164a3f1b4b1a971637af279a0ac49be3 ">that transcript</a>:</p>
<blockquote>
<p>But maybe we want seat below body bottom? Actually seat should be between body and pedals, not above body bottom. Wait, typical bicycle: seat post goes from bottom bracket upward to the seat. The seat sits on top of the seat tube (vertical). So the seat is above bottom bracket. In our representation, the seat is at y=250-255. Body center at 245, body bottom at 265. So body bottom is below seat. That means the pelican's body is below the seat? Actually no: body bottom extends to 265; seat top is at 250; so seat sits above body bottom by about 15 units. That seems plausible because seat should be slightly above body, but body bottom is at 265 &gt; seat top 255. [...]</p>
</blockquote>
<p>I've seen this from other local reasoning models too: they can really overthink this kind of problem! I don't know if there's any pelican SVG that's worth waiting nearly six minutes for.</p>
<h4 id="space-invaders-with-gpt-oss-20b">Space invaders with gpt-oss-20b</h4>
<p>Given how long high took I switched back to medium for my next experiment:</p>
<blockquote>
<p><code>Write an HTML and JavaScript page implementing space invaders</code></p>
</blockquote>
<p>It <a href="https://gist.github.com/simonw/63d7d8c43ae2ac93c214325bd6d607e4">thought for 10.78 seconds</a> and produced this:</p>

<div style="max-width: 100%; margin-bottom: 0.4em">
    <video controls="controls" preload="none" aria-label="Space Invaders" poster="https://static.simonwillison.net/static/2025/space-invaders-gpt-20.jpg" loop="loop" style="width: 100%; height: auto;" muted="muted">
        <source src="https://static.simonwillison.net/static/2025/space-invaders-gpt-20.mp4" type="video/mp4" />
    </video>
</div>

<p>You can <a href="https://tools.simonwillison.net/space-invaders-gpt-oss-20b-mxfp4-medium">play that here</a>.</p>

<p>It's not the best I've seen - I was more impressed <a href="https://simonwillison.net/2025/Jul/29/space-invaders/">by GLM 4.5 Air</a> - but it's very competent for a model that only uses 12GB of my RAM (GLM 4.5 Air used 47GB).</p>
<h4 id="trying-gpt-oss-120b-via-api-providers">Trying gpt-oss-120b via API providers</h4>
<p>I don't quite have the resources on my laptop to run the larger model. Thankfully it's already being hosted by a number of different API providers.</p>
<p>OpenRouter already <a href="https://openrouter.ai/openai/gpt-oss-120b/providers">lists three</a> - Fireworks, Groq and Cerebras. (Update: now also Parasail and Baseten.)</p>
<p>Cerebras is <em>fast</em>, so I decided to try them first.</p>
<p>I installed the <a href="https://github.com/irthomasthomas/llm-cerebras">llm-cerebras</a> plugin and ran the <code>refresh</code> command to ensure it had their latest models:</p>
<div class="highlight highlight-source-shell"><pre>llm install -U llm-cerebras jsonschema
llm cerebras refresh</pre></div>
<p>(Installing jsonschema worked around a warning message.)</p>
<p>Output:</p>
<pre><code>Refreshed 10 Cerebras models:
  - cerebras-deepseek-r1-distill-llama-70b
  - cerebras-gpt-oss-120b
  - cerebras-llama-3.3-70b
  - cerebras-llama-4-maverick-17b-128e-instruct
  - cerebras-llama-4-scout-17b-16e-instruct
  - cerebras-llama3.1-8b
  - cerebras-qwen-3-235b-a22b-instruct-2507
  - cerebras-qwen-3-235b-a22b-thinking-2507
  - cerebras-qwen-3-32b
  - cerebras-qwen-3-coder-480b
</code></pre>
<p>Now:</p>
<div class="highlight highlight-source-shell"><pre>llm -m cerebras-gpt-oss-120b \
  <span class="pl-s"><span class="pl-pds">'</span>Generate an SVG of a pelican riding a bicycle<span class="pl-pds">'</span></span></pre></div>
<p>Cerebras runs the new model at between 2 and 4 thousands tokens per second!</p>
<p>To my surprise this one <a href="https://gist.github.com/simonw/4c685f19f1a93b68eacb627125e36be4">had the same comments-in-attributes bug</a> that we saw with oss-20b earlier. I fixed those and got this pelican:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-120-cerebras.jpg" alt="Yellow and not great pelican, quite a good bicycle if a bit sketchy." style="max-width: 100%;" /></p>
<p>That bug appears intermittently - I've not seen it on some of my other runs of the same prompt.</p>
<p>The <a href="https://github.com/simonw/llm-openrouter">llm-openrouter</a> plugin also provides access to the models, balanced across the underlying providers. You can use that like so:</p>
<div class="highlight highlight-source-shell"><pre>llm install llm-openrouter
llm keys <span class="pl-c1">set</span> openrouter
<span class="pl-c"><span class="pl-c">#</span> Paste API key here</span>
llm -m openrouter/openai/gpt-oss-120b <span class="pl-s"><span class="pl-pds">"</span>Say hi<span class="pl-pds">"</span></span></pre></div>
<h4 id="llama-cpp-is-coming-very-shortly">llama.cpp is coming very shortly</h4>
<p>The <code>llama.cpp</code> <a href="https://github.com/ggml-org/llama.cpp/pull/15091">pull request for gpt-oss</a> was landed less than an hour ago. It's worth browsing through the coded - a <em>lot</em> of work went into supporting this new model, spanning 48 commits to 83 different files. Hopefully this will land in the <a href="https://formulae.brew.sh/formula/llama.cpp">llama.cpp Homebrew package</a> within the next day or so, which should provide a convenient way to run the model via <code>llama-server</code> and friends.</p>
<h4 id="gpt-oss-20b-in-ollama">gpt-oss:20b in Ollama</h4>
<p>Ollama <a href="https://ollama.com/library/gpt-oss">also have gpt-oss</a>, requiring an update to their app.</p>
<p>I fetched that 14GB model like this:</p>
<div class="highlight highlight-source-shell"><pre>ollama pull gpt-oss:20b</pre></div>
<p>Now I can use it with the new Ollama native app, or access it from <a href="https://llm.datasette.io/">LLM</a> like this:</p>
<div class="highlight highlight-source-shell"><pre>llm install llm-ollama
llm -m gpt-oss:20b <span class="pl-s"><span class="pl-pds">'</span>Hi<span class="pl-pds">'</span></span></pre></div>
<p>This also appears to use around 13.26GB of system memory while running a prompt.</p>
<p>Ollama also launched <a href="https://ollama.com/turbo">Ollama Turbo</a> today, offering the two OpenAI models as a paid hosted service:</p>
<blockquote><p>Turbo is a new way to run open models using datacenter-grade hardware. Many new models are too large to fit on widely available GPUs, or run very slowly. Ollama Turbo provides a way to run these models fast while using Ollama's App, CLI, and API. </p></blockquote>
<h4 id="the-model-card">Training details from the model card</h4>
<p>Here are some interesting notes about how the models were trained from <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">the model card</a> (PDF):</p>
<blockquote>
<p><strong>Data</strong>: We train the models on a text-only dataset with trillions of tokens, with a focus on STEM, coding, and general knowledge. To improve the safety of the model, we filtered the data for harmful content in pre-training, especially around hazardous biosecurity knowledge, by reusing the CBRN pre-training filters from GPT-4o. Our model has a knowledge cutoff of June 2024.</p>
<p><strong>Training</strong>: The gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework with expert-optimized Triton kernels. The training run for gpt-oss-120b required 2.1 million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. [...]</p>
</blockquote>
<p>Thunder Compute's article <a href="https://www.thundercompute.com/blog/nvidia-h100-pricing">NVIDIA H100 Pricing (August 2025): Cheapest On-Demand Cloud GPU Rates</a> lists prices from around $2/hour to $11/hour, which would indicate a training cost of the 120b model between $4.2m and $23.1m and the 20b between $420,000 and $2.3m.</p>
<blockquote>
<p>After pre-training, we post-train the models using similar CoT RL techniques as OpenAI o3. This procedure teaches the models how to reason and solve problems using CoT and teaches the model how to use tools. Because of the similar RL techniques, these models have a personality similar to models served in our first-party products like ChatGPT. Our training dataset consists of a wide range of problems from coding, math, science, and more.</p>
</blockquote>
<p>The models have additional special training to help them use web browser and Python (Jupyter notebook) tools more effectively:</p>
<blockquote>
<p>During post-training, we also teach the models to use different agentic tools:</p>
<ul>
<li>A browsing tool, that allows the model to call search and open functions to interact with
the web. This aids factuality and allows the models to fetch info beyond their knowledge
cutoff.</li>
<li>A python tool, which allows the model to run code in a stateful Jupyter notebook environment.</li>
<li>Arbitrary developer functions, where one can specify function schemas in a <code>Developer</code>
message similar to the OpenAI API. The definition of function is done within our harmony
format.</li>
</ul>
</blockquote>
<p>There's a corresponding <a href="https://github.com/openai/gpt-oss?tab=readme-ov-file#python">section about Python tool usage</a> in the <code>openai/gpt-oss</code> repository README.</p>


<h4 id="openai-harmony-a-new-format-for-prompt-templates">OpenAI Harmony, a new format for prompt templates</h4>
<p>One of the gnarliest parts of implementing harnesses for LLMs is handling the prompt template format.</p>
<p>Modern prompts are complicated beasts. They need to model user v.s. assistant conversation turns, and tool calls, and reasoning traces and an increasing number of other complex patterns.</p>
<p><a href="https://github.com/openai/harmony">openai/harmony</a> is a brand new open source project from OpenAI (again, Apache 2) which implements a new response format that was created for the <code>gpt-oss</code> models. It's clearly inspired by their new-ish <a href="https://openai.com/index/new-tools-for-building-agents/">Responses API</a>.</p>
<p>The format is described in the new <a href="https://cookbook.openai.com/articles/openai-harmony">OpenAI Harmony Response Format</a> cookbook document. It introduces some concepts that I've not seen in open weight models before:</p>
<ul>
<li>
<code>system</code>, <code>developer</code>, <code>user</code>, <code>assistant</code> and <code>tool</code> roles - many other models only use user and assistant, and sometimes system and tool.</li>
<li>Three different channels for output: <code>final</code>, <code>analysis</code> and <code>commentary</code>. Only the <code>final</code> channel is default intended to be visible to users. <code>analysis</code> is for chain of thought and <code>commentary</code> is sometimes used for tools.</li>
</ul>
<p>That channels concept has been present in ChatGPT for a few months, starting with the release of o3.</p>
<p>The details of the new tokens used by Harmony caught my eye:</p>
<center>
<table>
  <tbody><tr>
    <th>Token</th>
    <th>Purpose</th>
    <th>ID</th>
  </tr>
  <tr>
    <td>&lt;|start|&gt;</td>
    <td>Start of message header</td>
    <td>200006</td>
  </tr>
  <tr>
    <td>&lt;|end|&gt;</td>
    <td>End of message</td>
    <td>200007</td>
  </tr>
  <tr>
    <td>&lt;|message|&gt;</td>
    <td>Start of message content</td>
    <td>200008</td>
  </tr>
  <tr>
    <td>&lt;|channel|&gt;</td>
    <td>Start of channel info</td>
    <td>200005</td>
  </tr>
  <tr>
    <td>&lt;|constrain|&gt;</td>
    <td>Data type for tool call</td>
    <td>200003</td>
  </tr>
  <tr>
    <td>&lt;|return|&gt;</td>
    <td>Stop after response</td>
    <td>200002</td>
  </tr>
  <tr>
    <td>&lt;|call|&gt;</td>
    <td>Call a tool</td>
    <td>200012</td>
  </tr>
</tbody></table>
</center>
<p>Those token IDs are particularly important. They are part of a new token vocabulary called <code>o200k_harmony</code>, which landed in OpenAI's tiktoken tokenizer library <a href="https://github.com/openai/tiktoken/commit/3591ff175d6a80efbe4fcc7f0e219ddd4b8c52f1">this morning</a>.</p>
<p>In the past I've seen models get confused by special tokens - try pasting <code>&lt;|end|&gt;</code> into a model and see what happens.</p>
<p>Having these special instruction tokens formally map to dedicated token IDs should hopefully be a whole lot more robust!</p>
<p>The Harmony repo itself includes a Rust library and a Python library (wrapping that Rust library) for working with the new format in a much more ergonomic way.</p>
<p>I tried one of their demos using <code>uv run</code> to turn it into a shell one-liner:</p>
<div class="highlight highlight-source-shell"><pre>uv run --python 3.12 --with openai-harmony python -c <span class="pl-s"><span class="pl-pds">'</span></span>
<span class="pl-s">from openai_harmony import *</span>
<span class="pl-s">from openai_harmony import DeveloperContent</span>
<span class="pl-s">enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)</span>
<span class="pl-s">convo = Conversation.from_messages([</span>
<span class="pl-s">    Message.from_role_and_content(</span>
<span class="pl-s">        Role.SYSTEM,</span>
<span class="pl-s">        SystemContent.new(),</span>
<span class="pl-s">    ),</span>
<span class="pl-s">    Message.from_role_and_content(</span>
<span class="pl-s">        Role.DEVELOPER,</span>
<span class="pl-s">        DeveloperContent.new().with_instructions("Talk like a pirate!")</span>
<span class="pl-s">    ),</span>
<span class="pl-s">    Message.from_role_and_content(Role.USER, "Arrr, how be you?"),</span>
<span class="pl-s">])</span>
<span class="pl-s">tokens = enc.render_conversation_for_completion(convo, Role.ASSISTANT)</span>
<span class="pl-s">print(tokens)<span class="pl-pds">'</span></span></pre></div>
<p>Which outputs:</p>
<blockquote>
<p><code>[200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359, 22203, 656, 7788, 17527, 558, 87447, 100594, 25, 220, 1323, 19, 12, 3218, 279, 30377, 289, 25, 14093, 279, 2, 13888, 18403, 25, 8450, 11, 49159, 11, 1721, 13, 21030, 2804, 413, 7360, 395, 1753, 3176, 13, 200007, 200006, 77944, 200008, 2, 68406, 279, 37992, 1299, 261, 96063, 0, 200007, 200006, 1428, 200008, 8977, 81, 11, 1495, 413, 481, 30, 200007, 200006, 173781]</code></p>
</blockquote>
<p>Note those token IDs like <code>200006</code> corresponding to the special tokens listed above.</p>
<h4 id="the-open-question-for-me-how-good-is-tool-calling-">The open question for me: how good is tool calling?</h4>
<p>There's one aspect of these models that I haven't explored in detail yet: <strong>tool calling</strong>. How these work is clearly a big part of the new Harmony format, but the packages I'm using myself (around my own <a href="https://simonwillison.net/2025/May/27/llm-tools/">LLM tool calling</a> support) need various tweaks and fixes to start working with that new mechanism.</p>
<p>Tool calling currently represents my biggest disappointment with local models that I've run on my own machine. I've been able to get them to perform simple single calls, but the state of the art these days is wildly more ambitious than that.</p>
<p>Systems like Claude Code can make dozens if not hundreds of tool calls over the course of a single session, each one adding more context and information to a single conversation with an underlying model.</p>
<p>My experience to date has been that local models are unable to handle these lengthy conversations. I'm not sure if that's inherent to the limitations of my own machine, or if it's something that the right model architecture and training could overcome.</p>
<p>OpenAI make big claims about the tool calling capabilities of these new models. I'm looking forward to seeing how well they perform in practice.</p>

<h4 id="china">Competing with the Chinese open models</h4>

<p>I've been writing a <em>lot</em> about the <a href="https://simonwillison.net/tags/ai-in-china/">flurry of excellent open weight models</a> released by Chinese AI labs over the past few months - all of them very capable and most of them under Apache 2 or MIT licenses.</p>

<p>Just last week <a href="https://simonwillison.net/2025/Jul/30/chinese-models/">I said</a>:</p>

<blockquote>
<p>Something that has become undeniable this month is that the best available open weight models now come from the Chinese AI labs.</p>
<p>I continue to have a lot of love for Mistral, Gemma and Llama but my feeling is that Qwen, Moonshot and Z.ai have positively smoked them over the course of July. [...]</p>
<p>I can't help but wonder if part of the reason for the delay in release of OpenAI's open weights model comes from a desire to be notably better than this truly impressive lineup of Chinese models.</p>
</blockquote>
<p>With the release of the gpt-oss models that statement no longer holds true. I'm waiting for the dust to settle and the independent benchmarks (that are more credible than my ridiculous pelicans) to roll out, but I think it's likely that OpenAI now offer the best available open weights models.</p>

<p><strong>Update</strong>: Independent evaluations are beginning to roll in. Here's <a href="https://x.com/artificialanlys/status/1952887733803991070">Artificial Analysis</a>:</p>
<blockquote><p>gpt-oss-120b is the most intelligent American open weights model, comes behind DeepSeek R1 and Qwen3 235B in intelligence but offers efficiency benefits [...]</p>
<p>While the larger gpt-oss-120b does not come in above DeepSeek R1 0528‚Äôs score of 59 or Qwen3 235B 2507s score of 64, it is notable that it is significantly smaller in both total and active parameters than both of those models.</p></blockquote>
    
        <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/cerebras">cerebras</a>, <a href="https://simonwillison.net/tags/ollama">ollama</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/lm-studio">lm-studio</a>, <a href="https://simonwillison.net/tags/space-invaders">space-invaders</a>, <a href="https://simonwillison.net/tags/gpt-oss">gpt-oss</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/5/gpt-oss/#atom-everything>

---

## The Icebergs Visit Greenland

date: 2025-08-05, updated: 2025-08-05, from: One Foot Tsunami

 

<br> 

<https://onefoottsunami.com/2025/08/05/the-icebergs-visit-greenland/>

---

## The AI economy is full of financial gimmicks.

date: 2025-08-05, from: Dave Karpf's blog

Tech journalists need to start moonlighting as finance journalists. 

<br> 

<https://davekarpf.substack.com/p/the-ai-economy-is-full-of-financial>

---

## Academic Independence Eroding

date: 2025-08-05, from: Guy Kawasaki blog

Brendan Cantwell, Associate Professor of Higher, Adult, and Lifelong Education, Michigan State University. 

<br> 

<https://guykawasaki.substack.com/p/academic-independence-eroding>

---

## PCIe 8.0 to be up to 16 times faster than PCIe 4.0

date: 2025-08-05, from: Liliputing

<p>PCI-SIG has announced plans to release the PCIe 8.0 specification to members by 2028, offering support for data transfer speeds up to 256 GT/s in terms of raw bit rates, and up to 1TB/s of bi-directional speed when used in a x16 configuration. Given how long it takes for new PCIe standards to roll out, [&#8230;]</p>
<p>The post <a href="https://liliputing.com/pcie-8-0-to-be-up-to-16-times-faster-than-pcie-4-0/">PCIe 8.0 to be up to 16 times faster than PCIe 4.0</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/pcie-8-0-to-be-up-to-16-times-faster-than-pcie-4-0/>

---

## macOS Tahoe 26 Developer Beta 5

date: 2025-08-05, from: Michael Tsai

Juli Clover (Mr. Macintosh): Apple today provided developers with the fifth beta of macOS Tahoe 26 for testing purposes, with the update coming two weeks after the fourth beta. There are no updates to the release notes, which still say Beta 4. Mario Guzm&#225;n: THIS IS THE NEW MACINTOSH HD ICON?! WTF Previously: macOS Tahoe [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/05/macos-tahoe-26-developer-beta-5/>

---

## Apple: The First 50 Years (Forthcoming)

date: 2025-08-05, from: Michael Tsai

David Pogue (tweet): In time for Apple&#8217;s 50th anniversary, &#8220;CBS Sunday Morning&#8221; correspondent David Pogue tells the iconic company&#8217;s entire life story: how it was born, nearly died, was born again under Steve Jobs, and became, under CEO Tim Cook, one of the most valuable companies in the world. The 600-page book features 360 full-color [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/05/apple-the-first-50-years-forthcoming/>

---

## SwiftUI DocumentGroups Are Terribly Limited

date: 2025-08-05, from: Michael Tsai

Christian Tietze: This is how little you need to get started[&#8230;][&#8230;]What the system does is provide a launch scene for you when you only declare a DocumentGroup in your SwiftUI.App.body. You can customize this by making the launch scene yourself. WWDC24 &#8220;Evolve Your Document Launch Experience&#8221; contains examples that at least offer to style what&#8217;s [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/05/swiftui-documentgroups-are-terribly-limited/>

---

## Google Loses Appeal Against Epic

date: 2025-08-05, from: Michael Tsai

Mike Scarcella (MacRumors, Slashdot): The San Francisco-based 9th U.S. Circuit Court of Appeals, in a unanimous ruling, rejected, claims from Google that the trial judge made legal errors in the antitrust case that unfairly benefited &#8220;Fortnite&#8221; maker Epic Games, which filed the lawsuit in 2020.[&#8230;]U.S. District Judge James Donato in San Francisco ordered Google in [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/05/google-loses-appeal-against-epic/>

---

## AMD Ryzen 7 H 255 is a mid-range Hawk Point chip for laptops & mini PCs

date: 2025-08-05, from: Liliputing

<p>Over the past few days I&#8217;ve spotted a bunch of mini PCs from Chinese brands that are using a new processor based on slightly older technology. The¬†AMD Ryzen 7 H 255¬†processor is an 8-core, 16-thread chip that&#8217;s made for the Chinese market, but which is showing up in mini PCs shipped to customers globally. The [&#8230;]</p>
<p>The post <a href="https://liliputing.com/amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs/">AMD Ryzen 7 H 255 is a mid-range Hawk Point chip for laptops &#038; mini PCs</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs/>

---

## Claude Opus 4.1

date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://www.anthropic.com/news/claude-opus-4-1">Claude Opus 4.1</a></strong></p>
Surprise new model from Anthropic today - Claude Opus 4.1, which they describe as "a drop-in replacement for Opus 4".</p>
<p>My favorite thing about this model is the version number - treating this as a .1 version increment looks like it's an accurate depiction of the model's capabilities.</p>
<p>Anthropic's own benchmarks show very small incremental gains.</p>
<p>Comparing Opus 4 and Opus 4.1 (I <a href="https://claude.ai/share/c7366629-784a-4088-9fc4-15613aa41a7f">got 4.1 to extract this information from a screenshot</a> of Anthropic's own benchmark scores, then asked it to look up the links, then verified the links myself and fixed a few):</p>
<ul>
<li><strong>Agentic coding</strong> (<a href="https://github.com/SWE-bench/SWE-bench">SWE-bench Verified</a>): From 72.5% to 74.5%</li>
<li><strong>Agentic terminal coding</strong> (<a href="https://github.com/laude-institute/terminal-bench">Terminal-Bench</a>): From 39.2% to 43.3%</li>
<li><strong>Graduate-level reasoning</strong> (<a href="https://github.com/idavidrein/gpqa">GPQA Diamond</a>): From 79.6% to 80.9%</li>
<li><strong>Agentic tool use</strong> (<a href="https://github.com/sierra-research/tau-bench">TAU-bench</a>):</li>
<li>Retail: From 81.4% to 82.4%</li>
<li><strong>Airline: From 59.6% to 56.0%</strong> <em>(decreased)</em></li>
<li><strong>Multilingual Q&amp;A</strong> (<a href="https://huggingface.co/datasets/openai/MMMLU">MMMLU</a>): From 88.8% to 89.5%</li>
<li><strong>Visual reasoning</strong> (<a href="https://mmmu-benchmark.github.io/">MMMU validation</a>): From 76.5% to 77.1%</li>
<li><strong>High school math competition</strong> (<a href="https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions">AIME 2025</a>): From 75.5% to 78.0%</li>
</ul>
<p>Likewise, the <a href="https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf">model card</a> shows only tiny changes to the various safety metrics that Anthropic track.</p>
<p>It's priced the same as Opus 4 - $15/million for input and $75/million for output, making it one of <a href="https://www.llm-prices.com/#sb=input&amp;sd=descending">the most expensive models</a> on the market today.</p>
<p>I had it <a href="https://gist.github.com/simonw/7fead138d31d751d65c7253a1c18751b">draw me this pelican</a> riding a bicycle:</p>
<p><img alt="Pelican is line art, does have a good beak and feet on the pedals, bicycle is very poorly designed and not the right shape." src="https://static.simonwillison.net/static/2025/opus-4.1-pelican.png" /></p>
<p>For comparison I got a fresh new pelican <a href="https://gist.github.com/simonw/96a958e39aaed10e1e47c1aab2d05e20">out of Opus 4</a> which I actually like a little more:</p>
<p><img alt="This one has shaded colors for the different parts of the pelican. Still a bad bicycle." src="https://static.simonwillison.net/static/2025/opus-4-pelican.png" /></p>
<p>I shipped <a href="https://github.com/simonw/llm-anthropic/releases/tag/0.18">llm-anthropic 0.18</a> with support for the new model.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/evals">evals</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/5/claude-opus-41/#atom-everything>

---

## Florida Sues Huge Porn Sites Including XVideos and Bang Bros Over Age Verification Law

date: 2025-08-05, from: 404 Media Group

The lawsuit alleges XVideos, Bang Bros, XNXX, Girls Gone Wild and TrafficFactory are in violation of Florida's law that requires adult platforms to verify visitors are over 18. 

<br> 

<https://www.404media.co/florida-sues-huge-porn-sites-including-xvideos-and-bang-bros-over-age-verification-law/>

---

## Let‚Äôs Do Lunch!

date: 2025-08-05, from: Paul Krugman

A recording from Paul Krugman and Jared Bernstein's live video 

<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://api.substack.com/feed/podcast/170192625/564a4eb93450dfb4145438f847184acb.mp3"></source>
</audio> <a href="https://api.substack.com/feed/podcast/170192625/564a4eb93450dfb4145438f847184acb.mp3" target="_blank">download audio/mpeg</a><br> 

<https://paulkrugman.substack.com/p/lets-do-lunch>

---

## Argon ONE UP hits Kickstarter for $330 and up (Raspberry Pi CM5-powered laptop)

date: 2025-08-05, from: Liliputing

<p>The¬†Argon ONE UP is a laptop with a 14 inch, 1920 x 1200 pixel IPS LCD display, an aluminum body, backlit keyboard, and one thing that sets it apart from most other laptops: the Argon ONE UP is powered by a removable Raspberry Pi CM5 computer module. Argon40 has been making Raspberry Pi accessories like [&#8230;]</p>
<p>The post <a href="https://liliputing.com/argone-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop/">Argon ONE UP hits Kickstarter for $330 and up (Raspberry Pi CM5-powered laptop)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/argone-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop/>

---

## Tell Us What You Really Think Mr. Secretary [Poison Gas Warfare], 1942

date: 2025-08-05, from: National Archives, Text Message blog

In January 1942, shortly after the United States was thrust into World War II by the December 7, 1941, Japanese attack on Pearl Harbor and the subsequent December 11 declaration of war by Germany, officials in the Department of State considered the issue of the U.S. attitude toward the Geneva Protocol for the Prohibition of &#8230; <a href="https://text-message.blogs.archives.gov/2025/08/05/tell-us-what-you-really-think-mr-secretary-poison-gas-warfare-1942/" class="more-link">Continue reading <span class="screen-reader-text">Tell Us What You Really Think Mr. Secretary [Poison Gas Warfare], 1942</span></a> 

<br> 

<https://text-message.blogs.archives.gov/2025/08/05/tell-us-what-you-really-think-mr-secretary-poison-gas-warfare-1942/>

---

## Amores materialistas: el deseo en tiempos de capital

date: 2025-08-05, from: Iv√°n Paredes Res√©ndiz blog, Mexico's cinema

<p>Direcci√≥n:¬†Celine Song.¬† Guion:¬†Celine Song. Elenco:¬†Dakota Johnson, Chris Evans, Pedro Pascal.¬† Pa√≠s:¬†Estados Unidos. ¬†¬† M√°s informaci√≥n de la pel√≠cula:¬†https://www.imdb.com/title/tt30253473/¬†¬† Desde su √≥pera prima¬†Vidas pasadas¬†(2023), Celine Song dej√≥ entrever una sensibilidad particular: la de una cineasta¬†interesada en explorar c√≥mo fuerzas invisibles ‚Äîel tiempo, la¬†distancia, la cultura‚Äî moldean las relaciones humanas. Su segundo largometraje,¬†Amores¬†materialistas, no solo conserva ese [&#8230;]</p>
<p>La entrada <a href="https://www.palomitademaiz.net/resenas-amores-materialistas/">Amores materialistas: el deseo en tiempos de capital</a> se public√≥ primero en <a href="https://www.palomitademaiz.net">Palomita de ma√≠z</a>.</p>
 

<br> 

<https://www.palomitademaiz.net/resenas-amores-materialistas/?utm_source=rss&utm_medium=rss&utm_campaign=resenas-amores-materialistas>

---

## ICE Is About To Go on a Social Media and TV Ad Recruiting Blitz

date: 2025-08-05, from: 404 Media Group

Contracting records reviewed by 404 Media show that ICE wants to target Gen Z, including with ads on Hulu and HBO Max. 

<br> 

<https://www.404media.co/ice-is-about-to-go-on-a-social-media-and-tv-ad-recruiting-blitz/>

---

## Wikipedia Editors Adopt ‚ÄòSpeedy Deletion‚Äô Policy for AI Slop Articles

date: 2025-08-05, from: 404 Media Group

‚ÄúThe ability to quickly generate a lot of bogus content is problematic if we don't have a way to delete it just as quickly.‚Äù 

<br> 

<https://www.404media.co/wikipedia-editors-adopt-speedy-deletion-policy-for-ai-slop-articles/>

---

**@Robert's feed at BlueSky** (date: 2025-08-05, from: Robert's feed at BlueSky)

Thank you Dave. I enjoyed your work. Keep on trucking.

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvnwjox4w226>

---

## Nearly 100,000 ChatGPT Conversations Were Searchable on Google

date: 2025-08-05, from: 404 Media Group

A researcher has scraped a much larger dataset of indexed ChatGPT conversations, exposing contracts and intimate conversations. 

<br> 

<https://www.404media.co/nearly-100-000-chatgpt-conversations-were-searchable-on-google/>

---

## What Epstein Was Afraid Of

date: 2025-08-05, from: Tina Brown

I guess Ghislaine Maxwell must have given up something juicy enough in her session with Deputy AG Todd Blanche to earn her those new relaxed digs in the minimum-security, open-campus Bryan prison camp in Texas. 

<br> 

<https://tinabrown.substack.com/p/what-epstein-was-afraid-of>

---

## Quoting greyduet on r/teachers

date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs Weblog

<blockquote cite="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/"><p>I teach HS Science in the south. I can only speak for my district, but a few teacher work days in the wave of enthusiasm I'm seeing for AI tools is overwhelming. We're getting district approved ads for AI tools by email, Admin and ICs are pushing it on us, and at least half of the teaching staff seems all in at this point.</p>
<p>I was just in a meeting with my team and one of the older teachers brought out a powerpoint for our first lesson and almost everyone agreed to use it after a quick scan - but it was missing important tested material, repetitive, and just totally airy and meaningless. Just slide after slide of the same handful of sentences rephrased with random loosely related stock photos. When I asked him if it was AI generated, he said 'of course', like it was a strange question. [...]</p>
<p>We don't have a leg to stand on to teach them anything about originality, academic integrity/intellectual honesty, or the importance of doing things for themselves when they catch us indulging in it just to save time at work.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/">greyduet on r/teachers</a>, Unpopular Opinion: Teacher AI use is already out of control and it's not ok</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/slop">slop</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/education">education</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/#atom-everything>

---

## The Paranoid Style in American Economics

date: 2025-08-05, from: Paul Krugman

Remember, every accusation is a confession 

<br> 

<https://paulkrugman.substack.com/p/the-paranoid-style-in-american-economics>

---

## <default:div xmlns="http://www.w3.org/1999/xhtml" class="if-your-feed-reader-displays-this-then-it-is-violating-the-Atom-spec-RFC-4287-section-4.2.14"/>

date: 2025-08-05, updated: 2025-08-05, from: Tantek √áelik's blog

 

<br> 

<https://tantek.com/2025/216/t1/finished-skyline50k-ultra>

---

## A Friendly Introduction to SVG

date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://www.joshwcomeau.com/svg/friendly-introduction-to-svg/">A Friendly Introduction to SVG</a></strong></p>
This SVG tutorial by Josh Comeau is fantastic. It's filled with neat interactive illustrations - with a pleasing subtly "click" audio effect as you adjust their sliders - and provides a useful introduction to a bunch of well chosen SVG fundamentals.</p>
<p>I finally understand what all four numbers in the <code>viewport="..."</code> attribute are for!

    <p><small></small>Via <a href="https://lobste.rs/s/ome2lo/friendly_introduction_svg">Lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/svg">svg</a>, <a href="https://simonwillison.net/tags/explorables">explorables</a>, <a href="https://simonwillison.net/tags/josh-comeau">josh-comeau</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/5/a-friendly-introduction-to-svg/#atom-everything>

---

## A Treatise on AI Chatbots Undermining the Enlightenment

date: 2025-08-05, from: Maggie Appleton blog

On chatbot sycophancy, passivity, and the case for more intellectually challenging companions 

<br> 

<https://maggieappleton.com/ai-enlightenment/>

---

## ChatGPT agent's user-agent

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<p>I was exploring how ChatGPT agent works today. I learned some interesting things about how it exposes its identity through HTTP headers, then made a huge blunder in thinking it was leaking its URLs to Bingbot and Yandex... but it turned out <a href="https://simonwillison.net/2025/Aug/4/chatgpt-agents-agent/#cloudflare-crawler-hints">that was a Cloudflare feature</a> that had nothing to do with ChatGPT.</p>

<p>ChatGPT agent is the <a href="https://openai.com/index/introducing-chatgpt-agent/">recently released</a> (and confusingly named) ChatGPT feature that provides browser automation combined with terminal access as a feature of ChatGPT - replacing their previous <a href="https://help.openai.com/en/articles/10421097-operator">Operator research preview</a> which is scheduled for deprecation on August 31st.</p>

<h4 id="investigating-chatgpt-agent-s-user-agent">Investigating ChatGPT agent's user-agent</h4>
<p>I decided to dig into how it works by creating a logged web URL endpoint using <a href="https://simonwillison.net/2024/Aug/8/django-http-debug/">django-http-debug</a>. Then I told ChatGPT agent mode to explore that new page:</p>
<p><img src="https://static.simonwillison.net/static/2025/chatgpt-agent-url.jpg" alt="ChatGPT screenshot. My prompt was &quot;Visit https://simonwillison.net/test-url-context and tell me what you see there&quot; - it said &quot;Worked for 15 seconds&quot; with an arrow, then a screnshot of the webpage content showing &quot;simonwillison.net&quot; with a favicon, heading &quot;This is a heading&quot;, text &quot;Text and text and more text.&quot; and &quot;this came from javascript&quot;. The bot then responds with: The webpage displays a simple layout with a large heading at the top that reads ‚ÄúThis is a heading.‚Äù Below it, there's a short paragraph that says ‚ÄúText and text and more text.‚Äù A final line appears underneath saying ‚Äúthis came from javascript,‚Äù indicating that this last line was inserted via a script. The page contains no interactive elements or instructions‚Äîjust these lines of plain text displayed on a white background." style="max-width: 100%;" /></p>
<p>My logging captured these request headers:</p>
<pre><code>Via: 1.1 heroku-router
Host: simonwillison.net
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Cf-Ray: 96a0f289adcb8e8e-SEA
Cookie: cf_clearance=zzV8W...
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Priority: u=0, i
Sec-Ch-Ua: "Not)A;Brand";v="8", "Chromium";v="138"
Signature: sig1=:1AxfqHocTf693inKKMQ7NRoHoWAZ9d/vY4D/FO0+MqdFBy0HEH3ZIRv1c3hyiTrzCvquqDC8eYl1ojcPYOSpCQ==:
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36
Cf-Ipcountry: US
X-Request-Id: 45ef5be4-ead3-99d5-f018-13c4a55864d3
Sec-Fetch-Dest: document
Sec-Fetch-Mode: navigate
Sec-Fetch-Site: none
Sec-Fetch-User: ?1
Accept-Encoding: gzip, br
Accept-Language: en-US,en;q=0.9
Signature-Agent: "https://chatgpt.com"
Signature-Input: sig1=("@authority" "@method" "@path" "signature-agent");created=1754340838;keyid="otMqcjr17mGyruktGvJU8oojQTSMHlVm7uO-lrcqbdg";expires=1754344438;nonce="_8jbGwfLcgt_vUeiZQdWvfyIeh9FmlthEXElL-O2Rq5zydBYWivw4R3sV9PV-zGwZ2OEGr3T2Pmeo2NzmboMeQ";tag="web-bot-auth";alg="ed25519"
X-Forwarded-For: 2a09:bac5:665f:1541::21e:154, 172.71.147.183
X-Request-Start: 1754340840059
Cf-Connecting-Ip: 2a09:bac5:665f:1541::21e:154
Sec-Ch-Ua-Mobile: ?0
X-Forwarded-Port: 80
X-Forwarded-Proto: http
Sec-Ch-Ua-Platform: "Linux"
Upgrade-Insecure-Requests: 1
</code></pre>
<p>That <strong>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36</strong> user-agent header is the one used by the most recent Chrome on macOS - which is a little odd here as the <strong>Sec-Ch-Ua-Platform : "Linux"</strong> indicates that the agent browser runs on Linux.</p>
<p>At first glance it looks like ChatGPT is being dishonest here by not including its bot identity in the user-agent header. I thought for a moment it might be reflecting my own user-agent, but I'm using Firefox on macOS and it identified itself as Chrome.</p>
<p>Then I spotted this header:</p>
<pre><code>Signature-Agent: "https://chatgpt.com"
</code></pre>
<p>Which is accompanied by a much more complex header called <strong>Signature-Input</strong>:</p>
<pre><code>Signature-Input: sig1=("@authority" "@method" "@path" "signature-agent");created=1754340838;keyid="otMqcjr17mGyruktGvJU8oojQTSMHlVm7uO-lrcqbdg";expires=1754344438;nonce="_8jbGwfLcgt_vUeiZQdWvfyIeh9FmlthEXElL-O2Rq5zydBYWivw4R3sV9PV-zGwZ2OEGr3T2Pmeo2NzmboMeQ";tag="web-bot-auth";alg="ed25519"
</code></pre>
<p>And a <code>Signature</code> header too.</p>
<p>These turn out to come from a relatively new web standard: <a href="https://www.rfc-editor.org/rfc/rfc9421.html">RFC 9421 HTTP Message Signatures</a>' published February 2024.</p>
<p>The purpose of HTTP Message Signatures is to allow clients to include signed data about their request in a way that cannot be tampered with by intermediaries. The signature uses a public key that's provided by the following well-known endpoint:</p>
<pre><code>https://chatgpt.com/.well-known/http-message-signatures-directory
</code></pre>
<p>Add it all together and we now have a rock-solid way to identify traffic from ChatGPT agent: look for the <code>Signature-Agent: "https://chatgpt.com"</code> header and confirm its value by checking the signature in the <code>Signature-Input</code> and <code>Signature</code> headers.</p>
<h4 id="and-then-came-the-crawlers">And then came Bingbot and Yandex</h4>
<p>Just over a minute after it captured that request, my logging endpoint got another request:</p>
<pre><code>Via: 1.1 heroku-router
From: bingbot(at)microsoft.com
Host: simonwillison.net
Accept: */*
Cf-Ray: 96a0f4671d1fc3c6-SEA
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36
Cf-Ipcountry: US
X-Request-Id: 6214f5dc-a4ea-5390-1beb-f2d26eac5d01
Accept-Encoding: gzip, br
X-Forwarded-For: 207.46.13.9, 172.71.150.252
X-Request-Start: 1754340916429
Cf-Connecting-Ip: 207.46.13.9
X-Forwarded-Port: 80
X-Forwarded-Proto: http
</code></pre>
<p>I pasted <code>207.46.13.9</code> into Microsoft's <a href="https://www.bing.com/toolbox/verify-bingbot-verdict">Verify Bingbot</a> tool (after solving a particularly taxing CAPTCHA) and it confirmed that this was indeed a request from Bingbot.</p>
<p>I set up a second URL to confirm... and this time got a visit from Yandex!</p>
<pre><code>Via: 1.1 heroku-router
From: support@search.yandex.ru
Host: simonwillison.net
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Cf-Ray: 96a16390d8f6f3a7-DME
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)
Cf-Ipcountry: RU
X-Request-Id: 3cdcbdba-f629-0d29-b453-61644da43c6c
Accept-Encoding: gzip, br
X-Forwarded-For: 213.180.203.138, 172.71.184.65
X-Request-Start: 1754345469921
Cf-Connecting-Ip: 213.180.203.138
X-Forwarded-Port: 80
X-Forwarded-Proto: http
</code></pre>
<p>Yandex <a href="https://yandex.com/support/webmaster/en/robot-workings/check-yandex-robots.html?lang=en">suggest a reverse DNS lookup</a> to verify, so I ran this command:</p>
<pre><code>dig -x 213.180.203.138 +short
</code></pre>
<p>And got back:</p>
<pre><code>213-180-203-138.spider.yandex.com.
</code></pre>
<p>Which confirms that this is indeed a Yandex crawler.</p>

<p>I tried a third experiment to be sure... and got hits from both Bingbot and YandexBot.</p>

<h4 id="cloudflare-crawler-hints">It was Cloudflare Crawler Hints, not ChatGPT</h4>

<p>So I wrote up and posted about my discovery... and <a href="https://x.com/jatan_loya/status/1952506398270767499">Jatan Loya asked:</a></p>

<blockquote><p>do you have crawler hints enabled in cf?</p></blockquote>

<p>And yeah, it turned out I did. I spotted this in my caching configuration page (and it looks like I must have turned it on myself at some point in the past):</p>

<p><img src="https://static.simonwillison.net/static/2025/cloudflare-crawler-hints.jpg" alt="Screenshot of Cloudflare settings panel showing &quot;Crawler Hints Beta&quot; with description text explaining that Crawler Hints provide high quality data to search engines and other crawlers when sites using Cloudflare change their content. This allows crawlers to precisely time crawling, avoid wasteful crawls, and generally reduce resource consumption on origins and other Internet infrastructure. Below states &quot;By enabling this service, you agree to share website information required for feature functionality and agree to the Supplemental Terms for Crawler Hints.&quot; There is a toggle switch in the on position on the right side and a &quot;Help&quot; link in the bottom right corner." style="max-width: 100%" /></p>

<p>Here's <a href="https://developers.cloudflare.com/cache/advanced-configuration/crawler-hints/">the Cloudflare documentation for that feature</a>.</p>

<p>I deleted my posts on Twitter and Bluesky (since you can't edit those and I didn't want the misinformation to continue to spread) and edited <a href="https://fedi.simonwillison.net/@simon/114972968822349077">my post on Mastodon</a>, then updated this entry with the real reason this had happened.</p>

<p>I also changed the URL of this entry as it turned out Twitter and Bluesky were caching my social media preview for the previous one, which included the incorrect information in the title.</p>

<details><summary>Original "So what's going on here?" section from my post</summary>

<p><em>Here's a section of my original post with my theories about what was going on before learning about Cloudflare Crawler Hints.</em></p>

<h4 id="so-what-s-going-on-here-">So what's going on here?</h4>
<p>There are quite a few different moving parts here.</p>
<ol>
<li>I'm using Firefox on macOS with the 1Password and Readwise Highlighter extensions installed and active. Since I didn't visit the debug pages at all with my own browser I don't think any of these are relevant to these results.</li>
<li>ChatGPT agent makes just a single request to my debug URL ...</li>
<li>... which is proxied through both Cloudflare and Heroku.</li>
<li>Within about a minute, I get hits from one or both of Bingbot and Yandex.</li>
</ol>
<p>Presumably ChatGPT agent itself is running behind at least one proxy - I would expect OpenAI to keep a close eye on that traffic to ensure it doesn't get abused.</p>
<p>I'm guessing that infrastructure is hosted by Microsoft Azure. The <a href="https://openai.com/policies/sub-processor-list/">OpenAI Sub-processor List</a> - though that lists Microsoft Corporation, CoreWeave Inc, Oracle Cloud Platform and Google Cloud Platform under the "Cloud infrastructure" section so it could be any of those.</p>
<p>Since the page is served over HTTPS my guess is that any intermediary proxies should be unable to see the path component of the URL, making the mystery of how Bingbot and Yandex saw the URL even more intriguing.</p>
</details>
    
        <p>Tags: <a href="https://simonwillison.net/tags/bing">bing</a>, <a href="https://simonwillison.net/tags/privacy">privacy</a>, <a href="https://simonwillison.net/tags/search-engines">search-engines</a>, <a href="https://simonwillison.net/tags/user-agents">user-agents</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/cloudflare">cloudflare</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/chatgpt-agents-user-agent/#atom-everything>

---

## Particle Tachyon 5G single-board PC now available for $299

date: 2025-08-04, from: Liliputing

<p>The¬†Particle Tachyon¬†is a single-board computer that&#8217;s about the same size as a Raspberry Pi 5 and it even has a Raspberry Pi-compatible 40-pin GPIO header. But Particle positions the Tachyon as a versatile little PC with the guts of &#8220;a modern smartphone.&#8221; That&#8217;s because it&#8217;s powered by a Qualcomm QCM6490 Dragonwing processor with 8 Kryo [&#8230;]</p>
<p>The post <a href="https://liliputing.com/particle-tachyon-5g-single-board-pc-now-available-for-299/">Particle Tachyon 5G single-board PC now available for $299</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/particle-tachyon-5g-single-board-pc-now-available-for-299/>

---

## Usage charts for my LLM tool against OpenRouter

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://openrouter.ai/apps?url=https%3A%2F%2Fllm.datasette.io%2F">Usage charts for my LLM tool against OpenRouter</a></strong></p>
OpenRouter proxies requests to a large number of different LLMs and provides high level statistics of which models are the most popular among their users.</p>
<p>Tools that call OpenRouter can include <code>HTTP-Referer</code> and <code>X-Title</code> headers to credit that tool with the token usage. My <a href="https://github.com/simonw/llm-openrouter/">llm-openrouter</a> plugin <a href="https://github.com/simonw/llm-openrouter/blob/8e4be78e60337154b063faaa7161dddd91462730/llm_openrouter.py#L99C13-L99C20">does that here</a>.</p>
<p>... which means <a href="https://openrouter.ai/apps?url=https%3A%2F%2Fllm.datasette.io%2F">this page</a> displays aggregate stats across users of that plugin! Looks like someone has been running a lot of traffic through <a href="https://openrouter.ai/qwen/qwen3-14b">Qwen 3 14B</a> recently.</p>
<p><img alt="Screenshot of LLM usage statistics dashboard showing a stacked bar chart from July 5 to August 4, 2025, with a legend on the right displaying &quot;Top models&quot; including Qwen: Qwen3 14B (480M), Google: Gemini 2.5 Flash Lite Preview 06-17 (31.7M), Horizon Beta (3.77M), Google: Gemini 2.5 Flash Lite (1.67M), google/gemini-2.0-flash-exp (1.14M), DeepSeek: DeepSeek V3 0324 (1.11M), Meta: Llama 3.3 70B Instruct (228K), Others (220K), Qwen: Qwen3 Coder (218K), MoonshotAI: Kimi K2 (132K), and Horizon Alpha (75K), with a total of 520M usage shown for August 3, 2025." src="https://static.simonwillison.net/static/2025/llm-usage-openrouter.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/llm-openrouter-usage/#atom-everything>

---

## SuperDuper 4.0 Beta

date: 2025-08-04, from: Michael Tsai

Dave Nanian: Our new trace capability showed quite clearly that the folder we were working on was~/Pictures/Photos Library.photoslibrary/database/search/Spotlight/SpotlightKnowledgeEvents/index.V2/journals/12/cs_defaultAnd that&#8217;s a folder I don&#8217;t have. When the user navigated to it at first, he said it was &#8220;empty&#8221;&#8230;which was weird. But later, he noticed that there was a spinner at the bottom of the Finder window. [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/04/superduper-4-0-beta/>

---

## Logging Privacy Shenanigans

date: 2025-08-04, from: Michael Tsai

Peter Steinberger: If you&#8217;ve ever tried debugging a macOS app using the unified logging system, you&#8217;ve probably encountered the dreaded &#60;private&#62; redaction. Your carefully crafted log messages turn into cryptic puzzles where the most important debugging information is hidden. [&#8230;] You don&#8217;t need to use .mobileconfig files &#x2013; you can simply drop plist files directly [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/04/logging-privacy-shenanigans/>

---

## Device Added to Your Account

date: 2025-08-04, from: Michael Tsai

Riccardo Mori: Whenever I revive one of these devices, if it&#8217;s still able to access iCloud and other Apple ID-related services, I get a notification on all my other Apple devices that a certain device has now access to FaceTime and iMessage. The wording in this notification has changed for the worse in more recent [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/04/device-added-to-your-account/>

---

## AccuWeather to Discontinue Free API

date: 2025-08-04, from: Michael Tsai

AccuWeather (via Hacker News): AccuWeather&#8217;s&#x202F;current Free Limited Trials for Core Weather and MinuteCast&#174; will be retired with the new portal launch. [&#8230;] Once your trial ends, you can keep building with our affordable Starter package, which offers essential API access at a competitive monthly rate. It doesn&#8217;t say what the new plans are. Previously: Weather [&#8230;] 

<br> 

<https://mjtsai.com/blog/2025/08/04/accuweather-to-discontinue-free-api/>

---

## Qwen-Image: Crafting with Native Text Rendering

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://qwenlm.github.io/blog/qwen-image/">Qwen-Image: Crafting with Native Text Rendering</a></strong></p>
Not content with releasing <a href="https://simonwillison.net/2025/Jul/30/chinese-models/">six excellent open weights LLMs in July</a>, Qwen are kicking off August with their first ever image generation model.</p>
<p>Qwen-Image is a 20 billion parameter MMDiT (Multimodal Diffusion Transformer, originally proposed for Stable Diffusion 3) model under an Apache 2.0 license. The <a href="https://huggingface.co/Qwen/Qwen-Image">Hugging Face repo</a> is 53.97GB.</p>
<p>Qwen released a <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf">detailed technical report</a> (PDF) to accompany the model. The model builds on their Qwen-2.5-VL vision LLM, and they also made extensive use of that model to help create some of their their training data:</p>
<blockquote>
<p>In our data annotation pipeline, we utilize a capable image captioner (e.g., Qwen2.5-VL) to generate not only comprehensive image descriptions, but also structured metadata that captures essential image properties and quality attributes.</p>
<p>Instead of treating captioning and metadata extraction as independent tasks, we designed an annotation framework in which the captioner concurrently describes visual content and generates detailed information in a structured format, such as JSON. Critical details such as object attributes, spatial relationships, environmental context, and verbatim transcriptions of visible text are captured in the caption, while key image properties like type, style, presence of watermarks, and abnormal elements (e.g., QR codes or facial mosaics) are reported in a structured format.</p>
</blockquote>
<p>They put a <em>lot</em> of effort into the model's ability to render text in a useful way. 5% of the training data (described as "billions of image-text pairs") was data "synthesized through controlled text rendering techniques", ranging from simple text through text on an image background up to much more complex layout examples:</p>
<blockquote>
<p>To improve the model‚Äôs capacity to follow complex, structured prompts involving layout-sensitive content, we propose a synthesis strategy based on programmatic editing of pre-defined templates, such as PowerPoint slides or User Interface Mockups. A comprehensive rule-based system is designed to automate the substitution of placeholder text while maintaining the integrity of layout structure, alignment, and formatting.</p>
</blockquote>
<p>I tried the model out using the <a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced">ModelScope demo</a> - I signed in with GitHub and verified my account via a text message to a phone number. Here's what I got for "A raccoon holding a sign that says "I love trash" that was written by that raccoon":</p>
<p><img alt="A great photo of a raccoon holding a cardboard sign, the text I love trash is written on it in marker, the raccoon has chosen to draw the o in love as a heart filled with red marker pen." src="https://static.simonwillison.net/static/2025/qwen-trash.jpg" /></p>
<p>The raccoon has very neat handwriting!</p>
<p><strong>Update</strong>: A version of the model exists that can edit existing images but it's <a href="https://github.com/QwenLM/Qwen-Image/issues/3#issuecomment-3151573614">not yet been released</a>:</p>
<blockquote>
<p>Currently, we have only open-sourced the text-to-image foundation model, but the editing model is also on our roadmap and planned for future release.</p>
</blockquote>

    <p><small></small>Via <a href="https://x.com/Alibaba_Qwen/status/1952398250121756992">@Alibaba_Qwen</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/stable-diffusion">stable-diffusion</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/training-data">training-data</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/qwen-image/#atom-everything>

---

## Modos Paper Dev Kit cranks E Ink monitor refresh rates up to 75 Hz (crowdfunding)

date: 2025-08-04, from: Liliputing

<p>E Ink displays are often used in eBook readers or digital signage thanks to their low power consumption and paper-like qualities. But most devices with E Ink displays have low screen refresh rates that make them awkward fits for video playback or other high-motion graphics. While we&#8217;ve seen a few smartphones, tablets, and monitors with [&#8230;]</p>
<p>The post <a href="https://liliputing.com/modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding/">Modos Paper Dev Kit cranks E Ink monitor refresh rates up to 75 Hz (crowdfunding)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding/>

---

## Quoting @himbodhisattva

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<blockquote cite="https://x.com/himbodhisattva/status/1525182881726730240"><p>for services that wrap GPT-3, is it possible to do the equivalent of sql injection? like, a prompt-injection attack? make it think it's completed the task and then get access to the generation, and ask it to repeat the original instruction?</p></blockquote>
<p class="cite">&mdash; <a href="https://x.com/himbodhisattva/status/1525182881726730240">@himbodhisattva</a>, coining the term prompt injection on 13th May 2022, four months before <a href="https://simonwillison.net/2022/Sep/12/prompt-injection/">I did</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/himbodhisattva/#atom-everything>

---

## I Saved a PNG Image To A Bird

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://www.youtube.com/watch?v=hCQCP-5g5bo">I Saved a PNG Image To A Bird</a></strong></p>
Benn Jordan provides one of the all time great YouTube video titles, and it's justified. He drew an image in an audio spectrogram, played that sound to a talented starling (internet celebrity <a href="https://www.tiktok.com/@farijuana_bird/video/7452882774991572254">"The Mouth"</a>) and recorded the result that the starling almost perfectly imitated back to him.</p>
<blockquote>
<p>Hypothetically, if this were an audible file transfer protocol that used a 10:1 data compression ratio, that's nearly 2 megabytes of information per second. While there are a lot of caveats and limitations there, the fact that you could set up a speaker in your yard and conceivably store any amount of data in songbirds is crazy.</p>
</blockquote>
<p>This video is full of so much more than just that. Fast forward to <a href="https://www.youtube.com/watch?v=hCQCP-5g5bo&amp;t=358s">5m58s</a> for footage of a nest full of brown pelicans showing the sounds made by their chicks!


    <p>Tags: <a href="https://simonwillison.net/tags/audio">audio</a>, <a href="https://simonwillison.net/tags/youtube">youtube</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/i-saved-a-png-image-to-a-bird/#atom-everything>

---

## Your Own Newspaper, Or Not

date: 2025-08-04, from: Chris Coyier blog

You&#8217;ve likely heard me go on about how much I like an encourage using an RSS reader. Molly White frames it nicely: What if you could take all your favorite newsletters, ditch the data collection, and curate your own newspaper? It could include independent journalists, bloggers, mainstream media, worker-owned media collectives, and just about anyone [&#8230;] 

<br> 

<https://chriscoyier.net/2025/08/04/your-own-newspaper-or-not/>

---

## Superman: el nuevo punk rock

date: 2025-08-04, from: Iv√°n Paredes Res√©ndiz blog, Mexico's cinema

<p>Direcci√≥n: James Gunn. Guion: James Gunn. Elenco: David Corenswet, Rachel Brosnahan, Nicholas Hoult, Edi Gathegi, Nathan Fillion, Isabela Merced, Mar√≠a Gabriela de Far√≠a, Anthony Carrigan, Skyler Gisondo, Sara Sampaio. Pa√≠s: Estados Unidos. ¬†¬† M√°s informaci√≥n de la pel√≠cula: https://www.imdb.com/title/tt5950044 Superman, como h√©roe de c√≥mics, es una figura que necesita de una constante actualizaci√≥n para mantenerse [&#8230;]</p>
<p>La entrada <a href="https://www.palomitademaiz.net/resenas-superman/">Superman: el nuevo punk rock</a> se public√≥ primero en <a href="https://www.palomitademaiz.net">Palomita de ma√≠z</a>.</p>
 

<br> 

<https://www.palomitademaiz.net/resenas-superman/?utm_source=rss&utm_medium=rss&utm_campaign=resenas-superman>

---

## Public Broadcasting's Democratic Value

date: 2025-08-04, from: Guy Kawasaki blog

Stephanie A. (Sam) Martin, Frank and Bethine Church Endowed Chair of Public Affairs, Boise State University. 

<br> 

<https://guykawasaki.substack.com/p/public-broadcastings-democratic-value>

---

## Slopocalypse Now

date: 2025-08-04, from: Gary Marcus blog

The Horror 

<br> 

<https://garymarcus.substack.com/p/slopocalypse-now>

---

## Quoting Nick Turley

date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs Weblog

<blockquote cite="https://x.com/nickaturley/status/1952385556664520875"><p>This week, ChatGPT is on track to reach 700M weekly active users ‚Äî up from 500M at the end of March and 4√ó since last year.</p></blockquote>
<p class="cite">&mdash; <a href="https://x.com/nickaturley/status/1952385556664520875">Nick Turley</a>, Head of ChatGPT, OpenAI</p>

    <p>Tags: <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/4/nick-turley/#atom-everything>

---

**@IIIF Mastodon feed** (date: 2025-08-04, from: IIIF Mastodon feed)

<p>The Glycerine Framework has been extended: Glycerine server provides the back-end data model and APIs for integration into existing DAMS and CMS platforms, Glycerine editor allows for annotations, and the Glycerine viewer is embeddable into an existing CMS. </p><p>If you&#39;re interested in learning more, the <a href="https://glammr.us/tags/IIIF" class="mention hashtag" rel="tag">#<span>IIIF</span></a> Consortium and the Glycerine team are hosting (45 min.) showcases &amp; demos in different time zones on August 12 and 13. Free registration on Eventbrite: <a href="https://www.eventbrite.com/o/iiif-consortium-19836883937" target="_blank" rel="nofollow noopener" translate="no"><span class="invisible">https://www.</span><span class="ellipsis">eventbrite.com/o/iiif-consorti</span><span class="invisible">um-19836883937</span></a></p> 

<br> 

<https://glammr.us/@IIIF/114971144220041722>

---

## R2-D2 VEX robot

date: 2025-08-04, from: Raspberry Pi News (.com)

<p>Raspberry¬†Pi 3 and a VEX Robotics kit transform a toy version of an iconic film character into a working robot.</p>
<p>The post <a href="https://www.raspberrypi.com/news/r2-d2-vex-robot/">R2-D2 VEX robot¬†</a> appeared first on <a href="https://www.raspberrypi.com">Raspberry Pi</a>.</p>
 

<br> 

<https://www.raspberrypi.com/news/r2-d2-vex-robot/>

---

## The Anti-Porn Crusade That Censored Steam and Itch.io Started 30 Years Ago

date: 2025-08-04, from: 404 Media Group

Keywords and tags have never been a useful metric for distilling nuance. Pushing for regulations based on them is repeating a 30-year history of porn panic online. 

<br> 

<https://www.404media.co/steam-itchio-collective-shout-nsfw-games-campaign/>

---

## The Microsoft Smurface

date: 2025-08-04, updated: 2025-08-04, from: One Foot Tsunami

 

<br> 

<https://onefoottsunami.com/2025/08/04/the-microsoft-smurface/>

---

## Should Lyft and Uber charge more if your battery is low? California may soon ban that

date: 2025-08-04, from: The Markup blog

California lawmakers want to ban companies from using data about consumers‚Äô devices like battery life, model and geolocation to set fluctuating prices. Proponents say such ‚Äúsurveillance pricing‚Äù is discriminatory. 

<br> 

<https://themarkup.org/artificial-intelligence/2025/08/04/california-surveillance-pricing-ban>

---

## AI, Open Science, and the Future of Research Integrity: An Interview with Alison Mudditt of PLOS

date: 2025-08-04, from: Authors Union blogs

Below is an interview with&#160;Alison Mudditt, CEO of PLOS (Public Library of Science) discussing the impact of AI on publishing [&#8230;] 

<br> 

<https://www.authorsalliance.org/2025/08/04/ai-open-science-and-the-future-of-research-integrity-an-interview-with-alison-mudditt-of-plos/>

---

## Trump is Getting Desperate

date: 2025-08-04, from: Paul Krugman

We're in an extremely dangerous moment 

<br> 

<https://paulkrugman.substack.com/p/trump-is-getting-desperate-and-dangerous>

---

## New wave of projects to create digital commons

date: 2025-08-04, updated: 2025-08-04, from: nlnet feed

 

<br> 

<https://nlnet.nl/news/2025/20250804-announcement-grants-CommonsFund.html>

---

## Making my GitHub heatmap widget

date: 2025-08-04, from: Lean Rada's blog


<p><em>For RSS readers: This article contains interactive content available on the <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">original post on leanrada.com</a>.</em></p>

<p>This post is about how I made the GitHub heatmap widget on my site.</p>

<p>Here‚Äôs the raw, live WebComponent <code>&lt;gh-contribs&gt;</code> by the way:</p>
<card-box>
  <pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</card-box>
<h2>Scraping the data</h2>

<p>First, I had to scrape the heatmap data. I don‚Äôt know if there‚Äôs a proper API, but I found an endpoint that renders an HTML partial of what I wanted.</p>

<p>As far as I know, the GitHub website today works using partial HTMLs to update its UI without reloading the whole page. I think this endpoint populates the contribution graph section of the profile page.</p>

<p>The endpoint that returns a user‚Äôs contribution graph is <a href="https://github.com/users/Kalabasa/contributions" target="_blank"><code>https://github.com/users/{username}/contributions</code></a>. I presume the user has to have contribution stats public. This undocumented API could also break at any time. üò¨</p>

<figure>
  <img src="https://leanrada.com/notes/github-heatmap-widget/contributions-html.png?ref=rss" loading="lazy" width="932" height="741">
  <figcaption>The response HTML for my github.com/users/Kalabasa/contributions</figcaption>
</figure>

<p>Loading this endpoint gives you an unstyled piece of HTML containing an HTML table of contribution data and other UI. The table cells are invisible because of the lack of styles! When embedded in the profile page, it inherits the appropriate styling in context.</p>

<img alt="styled contribution table" src="https://leanrada.com/notes/github-heatmap-widget/contributions-styled.png?ref=rss" loading="lazy" width="741" height="204">

<p>The first column is the weekday label, and the rest of the cells seem to represent a single day each. The data is encoded in the HTML that presents the data! This reminds me of <a href="https://htmx.org/essays/hateoas/" target="_blank">Hypermedia as the Engine of Application State</a>. html = data.</p>

<pre><code>&lt;tbody&gt;
&lt;tr style="height: 10px"&gt;
  &lt;td class="ContributionCalendar-label" style="position: relative"&gt;
    &lt;span class="sr-only"&gt;Monday&lt;/span&gt;
    &lt;span aria-hidden="true" style="clip-path: None; position: absolute; bottom: -3px"&gt;
      Mon
    &lt;/span&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="0"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-2"
    style="width: 10px"
    data-date="2024-08-05"
    id="contribution-day-component-1-0"
    data-level="2"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="1"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-1"
    style="width: 10px"
    data-date="2024-08-12"
    id="contribution-day-component-1-1"
    data-level="1"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="2"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-2"
    style="width: 10px"
    data-date="2024-08-19"
    id="contribution-day-component-1-2"
    data-level="2"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  <!--...--></code></pre>

<p>What I was looking for here was the <code>data-level</code> attribute on each cell. It contains a coarse integer value that indicates the activity level for the day.</p>

<p>Coupled with the <code>data-date</code> attribute, it became rather easy to scrape this data! Instead of keeping track of columns and rows, I just go through each <code>data-date</code> and <code>data-level</code> as a (date,level) data point.</p>

<p>Here‚Äôs my parse function using <a href="https://cheerio.js.org/" target="_blank">cheerio</a>, a jQuery clone for Node.js.</p>

<pre><code>const res = await fetch(
  "https://github.com/users/Kalabasa/contributions");
let data = parseContribs(await res.text());

/**
 * Parses a GitHub contribution calendar HTML string and extracts contribution data.
 *
 * @param {string} html - The HTML string containing the GitHub contribution calendar.
 * @returns {{ date: Date, level: number }[]} Array of contribution objects with date and activity level.
 * @throws {Error} If the contribution calendar table cannot be found in the HTML.
 */
function parseContribs(html) {
  const ch = cheerio.load(html);
  const chTable = ch("table.ContributionCalendar-grid");
  if (!chTable.length) throw new Error("Can't find table.");
  const chDays = chTable.find("[data-date]");
  const data = chDays
    .map((_, el) =&gt; {
      const chDay = ch(el);
      const date = new Date(chDay.attr("data-date"));
      const level = parseInt(chDay.attr("data-level"), 10);
      return { date, level };
    })
    .get();
  return data;
}</code></pre>


This data is scraped at regular intervals, reformatted into a grid, and saved into a compact JSON format for later consumption and rendering by the WebComponent on the client.


<pre><code>// gh-contribs.json
[
  [1,2,1,2,2,1,1],
  [0,1,0,2,0,1,0],
  [1,1,1,1,1,1,0],
  [0,1,0,0,1,1,0],
  [0,0,1,0,0,0]
]</code></pre>

<h2>Rendering the data</h2>

<p>The reason why the data is reformatted into a grid like that is to make the rendering logic straightforward. The data is structured so that it can be directly converted into HTML without thinking in dates and weeks that are in the original data.</p>

<p>Here are the current JSON and WebComponent side by side. Each row in the data gets directly rendered as a column in the component.</p>
<auto-grid>
  <code-block language="js">
    <pre><code>
      <pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Inline script</pre>
    </code></pre>
  </code-block>
  <pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</auto-grid>
<p>As such, <code>&lt;gh-contribs&gt;</code>‚Äôs initialisation logic is really simple:</p>

<pre><code>const contribs = await fetch(
  "path/to/gh-contribs.json"
).then((res) =&gt; res.json());

let htmlString = "";
for (const col of contribs) {
  for (const level of col) {
    htmlString += html`&lt;div data-level="${level}"&gt;${level}&lt;/div&gt;`;
  }
}
this.innerHTML = htmlString;</code></pre>


Add some CSS and it‚Äôs done:


<pre><code>gh-contribs {
  display: grid;
  grid-auto-flow: column;
  grid-template-rows: repeat(7, auto);
  gap: 12px;
  div {
    position: relative;
    width: 18px;
    height: 18px;
    background: #222c2c;
    color: transparent;
    &amp;::after {
      content: "";
      position: absolute;
      inset: 0;
      background: #54f8c1;
    }
    &amp;[data-level="0"]::after {
      opacity: 0;
    }
    &amp;[data-level="1"]::after {
      opacity: 0.3;
    }
    &amp;[data-level="2"]::after {
      opacity: 0.6;
    }
    &amp;[data-level="3"]::after {
      opacity: 1;
    }
  }
}</code></pre>

<h2>Why not use the original HTML?</h2>

<p>Why not just embed the contributions HTML from GitHub? Slice the relevant <code>&lt;tr&gt;</code>s and <code>&lt;td&gt;</code>s‚Ä¶? Why parse the original HTML table, convert it to JSON, then render it as HTML again?</p>

<p>The main reason to do [HTML ‚Üí JSON ‚Üí HTML] is to remain flexible. As you know, that endpoint is undocumented. Also, depending on the HTML structure of the original is risky. Risk of breakage, risk of unwanted content, etc.</p>

<p>This way, I can change how I get the data without refactoring the WebComponent. I could go [<strong>GitHub API</strong> ‚Üí JSON ‚Üí HTML] or [<strong>local git script</strong> ‚Üí JSON ‚Üí HTML] or whatever.</p>

<p>It also works the other end. I actually rewrote this widget recently (from statically-generated HTML into a WebComponent) without having to change the scraper script or the JSON data structure.</p>

<h2>Final touches</h2>

<p>The WebComponent renders just the grid itself for flexibility. This let me use it in different ways, like with an icon and heading as in the home page.</p>
<card-box>
  <h4>
    <img src="https://leanrada.com/icons/github.png?ref=rss" alt="" loading="lazy" width="16" height="16">
    my github<br>heatmap
  </h4>
  <pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</card-box>
<p>Here‚Äôs the <a href="https://leanrada.com/components/gh-contribs/gh-contribs.js?ref=rss" target="_blank">source code for this WebComponent</a> if you‚Äôre interested.</p>
 

<br> 

<https://leanrada.com/notes/github-heatmap-widget/?ref=rss>

---

## Modos Developer Kit Now Live on Crowd Supply!

date: 2025-08-04, from: Modos Blog

The Modos Dev Kit is live! Build with fast-refresh, low-latency e-paper. Now on Crowd Supply. 

<br> 

<https://www.modos.tech/blog/modos-developer-kit-live>

---

## LLVMCGO25 - CARTS: Enabling Event-Driven Task and Data Block Compilation for Distributed HPC

date: 2025-08-04, from: LLVM Blog

<h1 id="llvmcgo25---carts-enabling-event-driven-task-and-data-block-compilation-for-distributed-hpc">LLVMCGO25 - CARTS: Enabling Event-Driven Task and Data Block Compilation for Distributed HPC</h1><p>Hello everyone! I‚Äôm Rafael, a PhD candidate at the University of Delaware. I recently flew from Philadelphia to Las Vegas to attend the CGO conference,where I had the chance to present my project and soak in new ideas about HPC.</p><p>In this blog, I‚Äôll dive into the project I discussed at the conference and share some personal insights and lessons I learned along the way.Although comments aren‚Äôt enabled here, I‚Äôd love to hear from you, feel free to reach out at (<em>rafaelhg at udel dot edu</em>) if you‚Äôre interested in collaborating, have questions, or just want to chat.</p><h2 id="motivation-why-carts">Motivation: Why CARTS?</h2><p>Modern High-Performance Computing (HPC) and AI/ML workloads are pushing our hardware and software to the limits. Some key challenges include:</p><ul><li><strong>Evolving Architectures:</strong> Systems now have complex memory hierarchies that need smart utilization.</li><li><strong>Hardware Heterogeneity:</strong> With multi-core CPUs, GPUs, and specialized accelerators in the mix, resource management gets tricky.</li><li><strong>Performance Pressure:</strong> Large-scale systems demand efficient handling of concurrency, synchronization, and communication.</li></ul><p>These challenges led to the creation of CARTS‚Äîa compiler framework that combines the flexibility of MLIR with the reliability of LLVM to optimize applications for distributed HPC environments.</p><h2 id="a-closer-look-at-arts-and-its-inspirations">A Closer Look at ARTS and Its Inspirations</h2><p>At the heart of CARTS is ARTS. Originally, ARTS stood for the <strong>Abstract Runtime System</strong>.I often get mixed up and mistakenly call it the <strong>Asynchronous Runtime System</strong>. To keep things light,we sometimes joke about it being the <strong>Any Runtime System</strong>.</p><p>ARTS is inspired by the Codelet model, a concept I could talk about all day!The Codelet model breaks a computation into small, independent tasks (or &ldquo;codelets&rdquo;) that can run as soon as their data dependencies are met.If you&rsquo;re curious to learn more about this model (or find it delightfully abstract), I suggest you visit our research group websiteat <a href="https://www.capsl.udel.edu/">CAPSL, University of Delaware</a> and check out the <a href="https://www.capsl.udel.edu/codelets.shtml#B4">Codelet Model website</a>.</p><h3 id="what-does-arts-do">What Does ARTS Do?</h3><p>ARTS is designed to support fine-grained, event-driven task execution in distributed systems. Here‚Äôs a simple breakdown of some key concepts:</p><ul><li><strong>Event-Driven Tasks (EDTs):</strong> These are the basic units of work that can be scheduled independently. Think of an EDT as a small, self-contained task that runs once all its required data is ready.</li><li><strong>DataBlocks:</strong> These represent memory regions holding the data needed by tasks. ARTS tracks these DataBlocks across distributed nodes so that tasks have quick and efficient access to the data they need.</li><li><strong>Events:</strong> These are signals that tell the system when a DataBlock is ready or when a task has finished. They help synchronize tasks without the need for heavy locks.</li><li><strong>Epochs:</strong> These act as synchronization boundaries. An epoch groups tasks together, ensuring that all tasks within the group finish before moving on to the next phase.</li></ul><p>By modeling tasks, DataBlocks, events, and epochs explicitly, ARTS makes it easier to analyze and optimize how tasks are executed across large, distributed systems.</p><h2 id="the-carts-compiler-pipeline">The CARTS Compiler Pipeline</h2><p>Building on ARTS, CARTS creates a task-centric compiler workflow. Here‚Äôs how it works:</p><h3 id="clangpolygeist-from-copenmp-to-mlir">Clang/Polygeist: From C/OpenMP to MLIR</h3><ul><li><strong>Conversion Process:</strong> Using the Polygeist infrastructure, we translate C/OpenMP code into MLIR. This process handles multiple dialects (like OpenMP, SCF, Affine, and Arith).</li><li><strong>Extended Support:</strong> We‚Äôve enhanced it to handle more OpenMP constructs, including OpenMP Tasks</li></ul><h3 id="arts-dialect-simplifying-concurrency">ARTS Dialect: Simplifying Concurrency</h3><ul><li><strong>Custom Language Constructs:</strong> The ARTS dialect converts high-level OpenMP tasks into a form that directly represents EDTs, DataBlocks, events, and epochs.</li><li><strong>Easier Analysis:</strong> This clear representation makes it simpler to analyze and optimize the code.</li></ul><h3 id="optimization-and-transformation-passes">Optimization and Transformation Passes</h3><ul><li><strong>EDT Optimization:</strong> We remove redundant tasks and optimize task structures‚Äîfor example, turning a ‚Äúparallel‚Äù task that contains only one subtask into a ‚Äúsync‚Äù task.</li><li><strong>DataBlock Management:</strong> We analyze memory access patterns to decide which DataBlocks are needed and optimize their usage.</li><li><strong>Event Handling and Classic Optimizations:</strong> We allocate and manage events, applying techniques like dead code elimination and common subexpression elimination to clean up the code.</li></ul><h3 id="lowering-to-llvm-ir-and-runtime-integration">Lowering to LLVM IR and Runtime Integration</h3><ul><li><strong>Conversion to LLVM IR:</strong> The ARTS-enhanced MLIR is converted into LLVM IR. This involves outlining EDT regions into functions and inserting ARTS API calls for task, DataBlock, epoch, and event management.</li><li><strong>Seamless Integration:</strong> The final binary runs on the ARTS runtime, which schedules tasks dynamically based on data readiness.</li></ul><h2 id="looking-ahead-future-directions-for-carts">Looking Ahead: Future Directions for CARTS</h2><p>The journey with CARTS is just beginning. Here‚Äôs a glimpse of what‚Äôs next:</p><ul><li><strong>Comprehensive Benchmarking:</strong> Testing the infrastructure with a variety of benchmarks to validate performance under diverse scenarios.</li><li><strong>Expanded OpenMP Support:</strong> Enhancing support for additional OpenMP constructs such as loops, barriers, and locks.</li><li><strong>Advanced Transformation Passes:</strong> Developing techniques like dependency pruning, task splitting/fusion, and affine transformations to further optimize task management and data locality.</li><li><strong>Memory-Centric Optimizations:</strong> Implementing strategies like cache-aware tiling, data partitioning, and optimized memory layouts to reduce cache misses and enhance data transfer efficiency.</li><li><strong>Feedback-Directed Compilation:</strong> Incorporating runtime profiling data to adapt optimizations dynamically based on actual workload and hardware behavior.</li><li><strong>Domain-Specific Extensions:</strong> Creating specialized operations for domains such as stencil computations and tensor operations to boost performance in targeted HPC applications.</li></ul><h2 id="wrapping-up">Wrapping Up</h2><p>Conferences like CGO are not just about technical presentations, they‚Äôre also about meeting people and sharing ideas. I really enjoyed the mix of technical sessions and informal conversations.One of my favorite moments was meeting a professor at the conference and joking about how we only seem to meet when we‚Äôre away from Newark.It‚Äôs these human connections, along with the valuable feedback on my work, that make attending such events worthwhile. Here are a few personal takeaways:</p><ul><li><strong>Invaluable Feedback:</strong> Presenting work-in-progress at LLVM CGO workshops has taught me that constructive criticism is the fuel for innovation.</li><li><strong>Community Spirit:</strong> Reconnecting with fellow researchers, whether through formal sessions or casual hallway conversations, enriches both our professional and personal lives.I encourage fellow PhD candidates and early-career researchers to take every opportunity to present your work,your ideas might not be 100% polished, but the community is there to help you refine them.</li></ul><p>Presenting CARTS allowed me to share detailed technical insights, discuss the practical challenges of HPC, and even have a few laughs along the way. While the technical details might seem dense at times, Ihope the mix of personal anecdotes and hands-on explanations makes the topic accessible and engaging.If you‚Äôre interested in discussing more about ARTS, the Codelet model, or anything else related to HPC, please drop me an email at (<em>rafaelhg at udel dot edu</em>). I‚Äôd love to chat, collaborate, or simply hang out.</p><h2 id="acknowledgements">Acknowledgements</h2><ul><li>This work is supported by the US DOE Office of Science project ‚ÄúAdvanced Memory to Support Artificial Intelligence for Science‚Äù at PNNL. PNNL is operated by Battelle Memorial Institute under Contract DEAC06-76RL01830.</li><li>Thanks to the LLVM Foundation for the travel award that made attending the CGO conference possible.</li></ul> 

<br> 

<https://blog.llvm.org/posts/2025-03-26-llvmcgo-carts/>

---

## RSoC 2025: Final Report: Unix Domain Sockets, Bulk FD Passing, and Separating File Tables

date: 2025-08-04, from: Redox OS News

Hi everyone! I&rsquo;m Ibuki Omatsu, and as part of my RSoC project, I&rsquo;ve worked on implementing Unix Domain Sockets (UDS) in Redox OS.
Following that, I also worked on implementing bulk file descriptor (FD) passing and separating file tables. In this post, first, I will talk about updates of the UDS implementation, and then I will explain bulk FD passing and the separation of file tables.
Unix Domain Socket First, let&rsquo;s talk about Unix Domain Sockets (UDS). 

<br> 

<https://www.redox-os.org/news/rsoc-2025-fdtbl/>

---

## The ChatGPT sharing dialog demonstrates how difficult it is to design privacy preferences

date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs Weblog

<p>ChatGPT just removed their "make this chat discoverable" sharing feature, after it turned out a material volume of users had inadvertantly made their private chats available via Google search.</p>
<p>Dane Stuckey, CISO for OpenAI, <a href="https://x.com/cryps1s/status/1951041845938499669">on Twitter</a>:</p>
<blockquote>
<p>We just removed a feature from @ChatGPTapp that allowed users to make their conversations discoverable by search engines, such as Google. This was a short-lived experiment to help people discover useful conversations. [...]</p>
<p>Ultimately we think this feature introduced too many opportunities for folks to accidentally share things they didn't intend to, so we're removing the option.</p>
</blockquote>
<p>There's been some media coverage of this issue - here are examples from <a href="https://techcrunch.com/2025/07/31/your-public-chatgpt-queries-are-getting-indexed-by-google-and-other-search-engines/">TechCrunch</a>, <a href="https://www.techradar.com/ai-platforms-assistants/chatgpt/openai-pulls-chat-sharing-tool-after-google-search-privacy-scare">TechRadar</a>, and <a href="https://www.pcmag.com/news/be-careful-what-you-tell-chatgpt-your-chats-could-show-up-on-google-search">PCMag</a>.</p>
<p>It turned out users had shared extremely private conversations and made them discoverable by search engines, which meant that various <code>site:chatgpt.com ...</code> searches were turning up all sorts of potentially embarrassing details.</p>
<p>Here's what that UI looked like before they removed the option:</p>
<p><img src="https://static.simonwillison.net/static/2025/chatgpt-share.jpg" alt="Screenshot of a chat sharing dialog with title &quot;Public link created&quot; and X close button. Text reads &quot;A public link to your chat has been created. Manage previously shared chats at any time via Settings.&quot; Below is an unchecked checkbox labeled &quot;Make this chat discoverable&quot; with subtitle &quot;Allows it to be shown in web searches&quot;. The sharing URL shown is &quot;https://chatgpt.com/share/688b95ef-f986&quot; with a black &quot;Copy link&quot; button. At bottom are three social sharing icons for LinkedIn, Reddit, and X." style="max-width: 100%;" /></p>
<p>I've seen a bunch of commentary, both on Twitter and <a href="https://news.ycombinator.com/item?id=44778764">this Hacker News thread</a>, from people who are baffled that anyone could be confused by such a clear option in the UI.</p>
<p>I think that confusion is warranted. Let's break it down.</p>
<p>Here's the microcopy in question:</p>
<blockquote>
<p><strong>Make this chat discoverable</strong><br />
Allows it to be shown in web searches.</p>
</blockquote>
<p>The first problem here is the choice of terminology. "Discoverable" is not a widely understood term - it's insider jargon. "Allows it to be shown in web searches" is better, but still requires a surprisng depth of understanding from users before they can make an informed decision.</p>
<p>Here's everything a user would need to understand for this to make sense to them:</p>
<ul>
<li>What a URL is, and how it's posssible to create a URL that is semi-public in that it's unguessable by others but can still be read by anyone you share it with. That concept is a pretty tall order just on its own!</li>
<li>What a web search engine is - that in this case it's intended as a generic term for Google, Bing, DuckDuckGo etc.</li>
<li>That "web search" here means "those public search engines other people can use" and not something like "the private search feature you use on this website".</li>
<li>A loose understanding of how search engines work: that they have indexes, and those indexes can selectively include or exclude content.</li>
<li>That sites like ChatGPT get to control whether or not their content is included in those indexes.</li>
<li>That the nature of a "secret URL" is that, once shared and made discoverable, anyone with that link (or who finds it through search) can now view the full content of that page.</li>
</ul>
<p>ChatGPT has over a billion users now. That means there is a giant range of levels of technical expertise among those users. We can't assume that everyone understands the above concepts necessary to understand the implications of checking that box.</p>
<p>And even if they have the pre-requisite knowledge required to understand this, <strong>users don't read</strong>.</p>
<p>When people are using an application they are always looking for the absolute shortest path to achieving their goal. Any dialog box or question that appears is something to be skipped over as quickly as possible.</p>
<p>Sadly, a lot of users may have learned to just say "yes" to any question. This option about making something "discoverable"? Sure, whatever, click the box and keep on going.</p>
<p>I think there's another factor at play here too: the option itself makes almost no sense.</p>
<p>How many people looking for a way to share their chats are going to think "and you know what? Stick this in Google too"?</p>
<p>It's such a tiny fraction of the audience that a logical conclusion, when faced with the above option, could well be that obviously it wouldn't put my chats in Google because who on Earth would ever want that to happen?</p>
<p>I think OpenAI made the right call disabling this feature. The value it can provide for the tiny set of people who decide to use it is massively outweighed by the potential for less discerning users to cause themselves harm by inadvertently sharing their private conversations with the world.</p>
<h4 id="meta-ai-does-this-even-worse">Meta AI does this even worse</h4>
<p>A much worse example of this anti-pattern is Meta AI's decision to provide a "Post to feed" button in their own Meta AI chat app:</p>
<p><img src="https://static.simonwillison.net/static/2025/meta-ai-share.jpg" alt="Sharing dialog has two options: Post to feed - share this conversation to the public feed so anyone can see it and engage. and Share a link: Create a link to share this conversation with specific people." style="max-width: 100%;" /></p>
<p>I think their microcopy here is <em>top notch</em> - the text here uses clear language and should be easy for anyone to understand.</p>
<p>(I took this screenshot today though, so it's possible the text has been recently updated.)</p>
<p>And yet... Futurism, June 14th: <a href="https://futurism.com/meta-ai-embarassing">People Don't Realize Meta's AI App Is Publicly Blasting Their Humiliating Secrets to the World</a>.</p>
<p>Once again, when your users number in the millions some of them are going to randomly click things without understanding the consequences.</p>
<p>The Meta AI iPhone app (fun fact: it can talk to you in the voice of Dame Judi Dench or John Cena) shows that public feed on the homepage when you first open the app, presumably to try and help people get over the blank slate "what is this thing even for" problem. They do not appear keen on losing this feature!</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/design">design</a>, <a href="https://simonwillison.net/tags/privacy">privacy</a>, <a href="https://simonwillison.net/tags/usability">usability</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/meta">meta</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/3/privacy-design/#atom-everything>

---

## High Quality Offline Music

date: 2025-08-03, from: mrusme blog

A brief overview of how to enjoy high quality music without subscribing to a
privacy-invasive and usually lower-quality music streaming service like
Spotify, YouTube Music, Deezer, etc. 

<br> 

<https://xn--gckvb8fzb.com/high-quality-offline-music/>

---

## XBai o4

date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://huggingface.co/MetaStoneTec/XBai-o4">XBai o4</a></strong></p>
Yet <em>another</em> open source (Apache 2.0) LLM from a Chinese AI lab. This model card claims:</p>
<blockquote>
<p><strong>XBai o4</strong> excels in complex reasoning capabilities and has now completely surpassed OpenAI-o3-mini in Medium mode.</p>
</blockquote>
<p>This a 32.8 billion parameter model released by MetaStone AI, a new-to-me lab who released their first model in March - <a href="https://huggingface.co/MetaStoneTec/MetaStone-L1-7B">MetaStone-L1-7B</a>, then followed that with MetaStone-S1 <a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-1.5B">1.5B</a>, <a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-7B">7B</a> and <a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-32B">32B</a> in July and now XBai o4 in August.</p>
<p>The MetaStone-S1 models were accompanied with a with a paper, <a href="https://arxiv.org/abs/2507.01951">Test-Time Scaling with Reflective Generative Model</a>.</p>
<p>There is <em>very</em> little information available on the English-language web about MetaStone AI. Their paper shows a relationship with USTC, <a href="https://en.wikipedia.org/wiki/University_of_Science_and_Technology_of_China">University of Science and Technology of China</a> in Hefei. One of their researchers <a href="https://x.com/WangMagic_/status/1951690465222217872">confirmed on Twitter</a> that their CEO is from <a href="https://en.wikipedia.org/wiki/Kuaishou">KWAI</a> which lead me to <a href="https://www.qbitai.com/2024/07/168071.html">this Chinese language article</a> from July last year about Li Yan, formerly of KWAI and now the founder of Wen Xiaobai and <a href="https://x.com/simonw/status/1951694450369208361">evidently</a> <a href="https://x.com/WangMagic_/status/1951694611191324929">now</a> the CEO of MetaStone. <a href="https://www.wenxiaobai.com">www.wenxiaobai.com</a> is listed as the "official website" linked to from <a href="https://github.com/MetaStone-AI/XBai-o4">the XBai-o4 README</a> on GitHub.</p>
<p>Ivan Fioravanti <a href="https://huggingface.co/mlx-community/models?search=xbai-o4">got it working under MLX</a> in 4bit, 5bit, 6bit, 8bit and 4bit-DWQ sizes. I tried his <a href="https://huggingface.co/mlx-community/XBai-o4-6bit">6bit one</a> (a 24.81GB download) in LM Studio and had it "Generate an SVG of a pelican riding a bicycle". It thought for 39.49 seconds (<a href="https://gist.github.com/simonw/78182fc3409e36f8d22217992967b9d6#response">thinking trace here</a>) and gave me this:</p>
<p><img alt="The pelican is blue and looks a little like a sad elephant. The bicycle looks more like a flat motorbike. The pelican has a blue tail and orange legs." src="https://static.simonwillison.net/static/2025/xbai-o4-pelican.png" /></p>
<p>I also had it "Write an HTML and JavaScript page implementing space invaders", which you can <a href="https://tools.simonwillison.net/space-invaders-xbai-o4-6bit-mlx">try out here</a>. It's not great - the invaders don't fire bullets (it listed that as a "potential improvement") and I don't think it's winnable due to timing issues, but it's an interesting addition to <a href="https://simonwillison.net/tags/space-invaders/">my collection</a>.</p>
<p>LM Studio reported a constant memory usage of 25.54GB and according to Activity Monitor used 87.5% of my GPU and around 39% of a CPU core.</p>
<p>It did a <em>lot</em> of work in the thinking trace - which took 10m35s! - including writing the first draft of much of the code that ended up in the finished result. Here's an extract:</p>
<blockquote>
<p>Let me think of variables:</p>
<p><code>let enemyDirection = 1; // 1 for right, -1 for left</code></p>
<p>In the game loop:
<code>enemies.forEach(enemy =&gt; {&gt;
    enemy.x += enemyDirection * enemy.dx;&gt;
});</code>
Then check if any enemy is out of bounds. If so, reverse direction and move all enemies down.</p>
<p>But how to check if any enemy is at the edge?</p>
<p>Maybe find the max x of all enemies and see if it exceeds <code>canvas.width - enemy.width</code>.</p>
<p>Alternatively, track the leftmost and rightmost positions of enemies. Maybe a bit complex.</p>
<p>For simplicity, perhaps check if any enemy's <code>x + width &gt; canvas.width or x &lt; 0</code>.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/457d55a5c426e0bbea9d47aee225c391">the full transcript</a>.</p>
<p>My initial impressions are that I'm not as impressed with this model for running on my own laptop as I was with <a href="https://simonwillison.net/2025/Jul/31/qwen3-coder-flash/">Qwen3-Coder-30B-A3B-Instruct</a> or <a href="https://simonwillison.net/2025/Jul/29/space-invaders/">GLM-4.5 Air</a>.</p>
<p>But... how extraordinary is it that <em>another</em> Chinese AI lab has been able to produce a competitive model, this time with far less fanfare than we've seen from Qwen and Moonshot AI and Z.ai.

    <p><small></small>Via <a href="https://x.com/ivanfioravanti/status/1951643205985816807">@ivanfioravanti</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/mlx">mlx</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/lm-studio">lm-studio</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/space-invaders">space-invaders</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/3/xbai-o4/#atom-everything>

---

## AI Agents have, so far, mostly been a dud

date: 2025-08-03, from: Gary Marcus blog

Last year, big tech couldn&#8217;t stop talking about how AI &#8220;agents&#8221; would be the next big thing in 2025. It hasn&#8217;t quite turned out that way. 

<br> 

<https://garymarcus.substack.com/p/ai-agents-have-so-far-mostly-been>

---

## From Async/Await to Virtual Threads

date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs Weblog

<p><strong><a href="https://lucumr.pocoo.org/2025/7/26/virtual-threads/">From Async/Await to Virtual Threads</a></strong></p>
Armin Ronacher has long been critical of async/await in Python, both for necessitating <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">colored functions</a> and because of the more subtle challenges they introduce like <a href="https://lucumr.pocoo.org/2020/1/1/async-pressure/">managing back pressure</a>.</p>
<p>Armin <a href="https://lucumr.pocoo.org/2024/11/18/threads-beat-async-await/">argued convincingly</a> for the threaded programming model back in December. Now he's expanded upon that with a description of how virtual threads might make sense in Python.</p>
<p>Virtual threads behave like real system threads but can vastly outnumber them, since they can be paused and scheduled to run on a real thread when needed. Go uses this trick to implement goroutines which can then support millions of virtual threads on a single system.</p>
<p>Python core developer Mark Shannon <a href="https://discuss.python.org/t/add-virtual-threads-to-python/91403">started a conversation</a> about the potential for seeing virtual threads to Python back in May.</p>
<p>Assuming this proposal turns into something concrete I don't expect we will see it in a production Python release for a few more years. In the meantime there are some exciting improvements to the Python concurrency story - most notably <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-pep734">around sub-interpreters</a> - coming up this year in Python 3.14.


    <p>Tags: <a href="https://simonwillison.net/tags/armin-ronacher">armin-ronacher</a>, <a href="https://simonwillison.net/tags/concurrency">concurrency</a>, <a href="https://simonwillison.net/tags/gil">gil</a>, <a href="https://simonwillison.net/tags/go">go</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/threads">threads</a></p> 

<br> 

<https://simonwillison.net/2025/Aug/3/virtual-threads/#atom-everything>

---

## GMK NucBox K12 mini PC with AMD Ryzen 7 H 255 is like a cheaper EVO-T1 (with AMD instead of Intel)

date: 2025-08-03, from: Liliputing

<p>The¬†GMK NucBox K12¬†is a small desktop computer with an OCuLink port for a high-speed connection to an external graphics dock or other accessories, support for up to 128GB of DDR5-5600 memory, up to three SSDs, and dual 2.5 Gb Ethernet ports. If all of that sounds familiar, that&#8217;s because it could also describe the GMK [&#8230;]</p>
<p>The post <a href="https://liliputing.com/gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel/">GMK NucBox K12 mini PC with AMD Ryzen 7 H 255 is like a cheaper EVO-T1 (with AMD instead of Intel)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel/>

---

**@Robert's feed at BlueSky** (date: 2025-08-03, from: Robert's feed at BlueSky)

I am happy this is happening.

[contains quote post or other embedded content] 

<br> 

<https://bsky.app/profile/rsdoiel.bsky.social/post/3lvj2lbommk2b>

---

## GPD Win 5 handheld gaming PC specs & performance preview: Strix Halo processor, 7 inch display, and no keyboard

date: 2025-08-03, from: Liliputing

<p>This week GPD revealed that its next handheld gaming PC would be powered by an AMD Strix Halo processor, bringing discrete-class graphics to a handheld PC for the first time. But at the time the company didn&#8217;t share many other details about the upcoming GPD Win 5, which left questions about battery life and overall [&#8230;]</p>
<p>The post <a href="https://liliputing.com/gpd-win-5-handheld-gaming-pc-specs-revealed-strix-halo-processor-7-inch-display-and-no-keyboard/">GPD Win 5 handheld gaming PC specs &#038; performance preview: Strix Halo processor, 7 inch display, and no keyboard</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/gpd-win-5-handheld-gaming-pc-specs-revealed-strix-halo-processor-7-inch-display-and-no-keyboard/>

---

## The Economics of Smoot-Hawley 2.0, Part I

date: 2025-08-03, from: Paul Krugman

Tariffs will be very high as far as the eye can see. What does that mean? 

<br> 

<https://paulkrugman.substack.com/p/the-economics-of-smoot-hawley-20>

---

## Creating a Toy Programming Language with Actor-Based Parallelism

date: 2025-08-03, from: Pointers gone wild blog

 

<br> 

<https://pointersgonewild.com/2025-08-03-creating-a-toy-language-with-actor-based-parallelism/>

