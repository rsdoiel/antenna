<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8" >
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" >
  <title>libraries</title>
<!--  <link rel="stylesheet" type="text/css"  href="webfonts/fonts.css" media="screen" > -->
  <link rel="stylesheet" type="text/css"  href="css/site.css" media="screen" >
</head>
<body>
<header>
	<img class="logo" 
		src="https://upload.wikimedia.org/wikipedia/commons/9/9c/Antenna_1_-_The_Noun_Project.svg"
		alt="line art showing an antenna"
		height="80" width="60" >
	<h1>The Antenna</h1> 
	<h2>finding signal in the noise</h2>
</header>
<nav>
<ul>
	<li><a href="./">The Antenna</a></li>
	<li><a href="archives/">Archives</a></li>
	<li><a href="about.html">About</a></li>
</ul>
</nav>
<section>
<div class="description-for-items">
Items collected from feeds in <a href="libraries.txt">libraries.txt</a>
</div>
<h1 id="libraries">libraries</h1>
<p>(date: 2024-11-07 08:17:52)</p>
<hr />
<h2
id="paywalls-are-not-the-only-barriers-to-access-accessibility-is-critical-to-equitable-access">Paywalls
are Not the Only Barriers to Access: Accessibility is Critical to
Equitable Access</h2>
<p>date: 2024-11-07, from: Scholarly Kitchen</p>
<p>
Digital accessibility to the scholarly communications process is core to
providing equitable access to the literature.
</p>
<p>
The post
<a href="https://scholarlykitchen.sspnet.org/2024/11/07/paywalls-are-not-the-only-barriers-to-access/">Paywalls
are Not the Only Barriers to Access: Accessibility is Critical to
Equitable Access</a> appeared first on
<a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.
</p>
<p><br></p>
<p><a
href="https://scholarlykitchen.sspnet.org/2024/11/07/paywalls-are-not-the-only-barriers-to-access/"
class="uri">https://scholarlykitchen.sspnet.org/2024/11/07/paywalls-are-not-the-only-barriers-to-access/</a></p>
<hr />
<h2 id="how-good-is-your-matching">How Good Is Your Matching?</h2>
<p>date: 2024-11-07, from: ROR Research ID Blog</p>
<p>The fourth blog post about metadata matching by ROR’s Adam Buttrick
and Crossref’s Dominika Tkaczyk explains how to measure the quality of
different matching strategies with an evaluation dataset and
metrics.</p>
<p><br></p>
<p><a href="https://ror.org/blog/2024-11-06-how-good-is-your-matching/"
class="uri">https://ror.org/blog/2024-11-06-how-good-is-your-matching/</a></p>
<hr />
<h2
id="first-hand-publishing-experiences-researcher-panel-at-ssps-new-directions-seminar">First-Hand
Publishing Experiences: Researcher Panel at SSP’s New Directions
Seminar</h2>
<p>date: 2024-11-06, from: Scholarly Kitchen</p>
<p>
A diverse panel of researchers shared their first-hand publishing
experiences at the 2024New Directions seminar.
</p>
<p>
The post
<a href="https://scholarlykitchen.sspnet.org/2024/11/06/a-diverse-panel-of-researchers-shared-their-first-hand-publishing-experiences-at-the-2024-new-directions-seminar/">First-Hand
Publishing Experiences: Researcher Panel at SSP’s New Directions
Seminar</a> appeared first on
<a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.
</p>
<p><br></p>
<p><a
href="https://scholarlykitchen.sspnet.org/2024/11/06/a-diverse-panel-of-researchers-shared-their-first-hand-publishing-experiences-at-the-2024-new-directions-seminar/"
class="uri">https://scholarlykitchen.sspnet.org/2024/11/06/a-diverse-panel-of-researchers-shared-their-first-hand-publishing-experiences-at-the-2024-new-directions-seminar/</a></p>
<hr />
<h2
id="repurposing-of-waste-marble-slabs-for-architectural-façades">Repurposing
of Waste Marble Slabs for Architectural Façades</h2>
<p>date: 2024-11-06, from: ETH Zurich, recently added</p>
<p>Reisach, Dominik</p>
<p><br></p>
<p><a href="http://hdl.handle.net/20.500.11850/703142"
class="uri">http://hdl.handle.net/20.500.11850/703142</a></p>
<hr />
<h2 id="how-good-is-your-matching-1">How good is your matching?</h2>
<p>date: 2024-11-06, from: Crossref Blog</p>
<p>
In our
<a href="https://www.crossref.org/blog/the-myth-of-perfect-metadata-matching">previous
blog post</a> in this series, we explained why no metadata matching
strategy can return perfect results. Thankfully, however, this does not
mean that it’s impossible to know anything about the quality of
matching. Indeed, we can (and should!) measure how close (or far) we are
from achieving perfection with our matching. Read on to learn how this
can be done!
</p>
<p>
How about we start with a quiz? Imagine a database of scholarly metadata
that needs to be enriched with identifiers, such as ORCIDs or ROR IDs.
Hopefully, by this point in our series this is recognizable as a classic
matching problem. In searching for a solution, you identify an
externally-developed matching tool that makes one of the below claims.
Which of the following would demonstrate satisfactory performance?
</p>
<ol>
<li>
It is a cutting-edge, state-of-the-art, intelligent-as-they-come,
bullet-proof technology! All the big players are using it. You won’t
find anything better!
</li>
<li>
The tool was tested on the metadata of 10 articles we authored, and many
identifiers were matched.
</li>
<li>
The quality of our matching is 98%.
</li>
</ol>
<p>
Okay, okay, trick question. The correct answer here is to opt for secret
answer #4: “I wouldn’t be satisfied by any of these claims!” Let’s dig
in a bit more to why this is the correct response.
</p>
<h2 id="the-importance-of-the-evaluation">
The importance of the evaluation
</h2>
<p>
Before we decide to integrate a matching strategy, it is important to
understand as much as possible about how it will perform. Whether it is
used in a semi or fully automated fashion, metadata matching will result
in the creation of new relationships between things like works, authors,
funding sources, and institutions. Those relationships will then, in
turn, be used by the consumers of this metadata to guide their
understanding and perhaps even to make important decisions about those
same entities. As organisations providing scholarly infrastructure, we
must therefore take it as our paramount responsibility to understand any
caveats or shortcomings of the scholarly metadata we make available,
including that resulting from matching.
</p>
<p>
Proper evaluation is what allows us to do this, as it is impossible to
know how well a given matching strategy will perform in its absence.
This is true no matter how simple or complex a matching strategy may
seem. Complex methods can be tailored to data with specific
characteristics and might fail when faced with something different from
this. Simple methods might be only appropriate for clean metadata or a
narrow set of use cases.
</p>
<p>
Beyond complexity, matching strategies themselves vary widely in
character, inheriting biases from their design, training data, or how a
problem has been formulated. Some prioritise avoiding false negatives,
while others focus on minimising false positives. Even a generally
high-performing strategy might not be perfectly aligned with your
specific needs or data. In some cases, the task also itself might be too
challenging, or the available metadata too noisy, for any matching
strategy to perform adequately.
</p>
<p>
Evaluation is, again, how we understand these nuances and make informed
decisions about whether to implement matching or avoid it altogether. By
now, it should also be clear that the notion “we don’t need to evaluate”
is far from ideal! Given its importance, let’s explore how evaluation is
actually done.
</p>
<h2 id="evaluation-process">
Evaluation process
</h2>
<p>
In general, a proper evaluation procedure should follow the following
steps:
</p>
<ol>
<li>
Preparation of an evaluation dataset containing many examples of
matching inputs and the corresponding expected outputs.
</li>
<li>
Applying the strategy to all inputs from the dataset and recording the
responses.
</li>
<li>
Comparing the expected outputs with the outputs from the strategy.
</li>
<li>
Converting the results of the above comparison into evaluation metrics.
</li>
</ol>
<p>
From this accounting, we can see that there are two primary components
for the evaluation process: an evaluation dataset and metrics.
</p>
<h3 id="evaluation-dataset">
Evaluation dataset
</h3>
<p>
It’s useful to conceive an evaluation dataset as the specification for
an ideal matching strategy, describing what would be returned from our
forever-elusive perfect matching. When creating such a dataset, what
this means in practice is that it should contain a number of real-world,
example inputs, along with the corresponding ideal or expected outputs,
and that all data should be in the same format as the strategy is
expected to process. The outputs should themselves also confirm the
strategy’s overall requirements, for example, by being consistent with
its cardinality, meaning whether zero, one, or multiple matches should
be returned and under what circumstances. In terms of size, it’s
generally useful to calculate the ideal number of evaluation examples
using a sample size calculator or using
<a href="https://doi.org/10.1520/E0122-17R22" target="_blank">standardised
measures</a>, but as a quick rule of thumb: less than 100 examples is
probably insufficient, more than 1,000 or 2,000 is generally acceptable.
</p>
<p>
It is also important that the evaluation dataset be representative of
the data to be matched in order to ensure reliable results. Using
unrepresentative data, even if convenient, can lead to biassed or
misleading evaluations. For example, if matching affiliations from
various journals, building an evaluation dataset solely from one journal
that already assigns ROR IDs to authors’ affiliations might be tempting.
The data, having been already annotated, allow us to avoid the tedious
work of labelling, and we might even know that it is produced by a
high-quality source. This is still, unfortunately, a flawed approach. In
practice, such datasets are unlikely to represent the entire range of
affiliations to be matched, potentially leading to a significant
discrepancy between the evaluated quality and the actual performance of
the matching strategy, when applied to the full dataset. To assess a
matching strategy’s effectiveness, we have to resist shortcuts and
instead do our best to create truly representative evaluation datasets
to be confident that we’ve accurately measured their performance.
</p>
<h3 id="evaluation-metrics">
Evaluation metrics
</h3>
<p>
Evaluation metrics are what allow us to summarise the results of the
evaluation into a single number. Metrics give us a quick way to get an
estimation of how close the strategy was to achieving perfect results.
They are also useful if we want to compare different strategies with
each other or decide whether the strategy is sufficient for our use
case, removing the need to compare countless evaluation examples from
different strategies against one another.
</p>
<p>
The simplest metric is
<a href="https://en.wikipedia.org/wiki/Accuracy_and_precision" target="_blank">accuracy</a>,
which can be calculated as the fraction of the dataset examples that
were matched correctly. While a commonsense benchmark, accuracy can be
misleading, and we generally do not recommend using it. To understand
why, let’s consider the following small dataset and the responses from
two strategies:
</p>
<table>
<thead>
<tr>
<th>
Input
</th>
<th>
Expected output
</th>
<th>
Strategy 1
</th>
<th>
Strategy 2
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
string 1
</td>
<td>
ID 1
</td>
<td>
ID 1
</td>
<td>
ID 1
</td>
</tr>
<tr>
<td>
string 2
</td>
<td>
ID 2
</td>
<td>
ID 3
</td>
<td>
Empty output
</td>
</tr>
<tr>
<td>
string 3
</td>
<td>
Empty output
</td>
<td>
Empty output
</td>
<td>
Empty output
</td>
</tr>
</tbody>
</table>
<p>
Both strategies achieved the same accuracy, 0.67, making one mistake
each on the second affiliation string. However, a closer examination
reveals that these error types are distinct. The first strategy matched
to an incorrect identifier, while the second refused to return any value
illustrating the limitation of accuracy as a measure: it generally fails
to capture important nuances in strategy behaviour. In our example, the
first strategy appears more permissive, returning matches even in
unclear circumstances, while the second is more conservative,
withholding them when uncertain. Although using such a small dataset
would preclude drawing any definitive conclusions, it highlights how
relying on accuracy alone can obscure differences in performance.
</p>
<p>
For evaluating matching strategies, we instead recommend using two
metrics:
<a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">precision
and recall</a>. To recap from our previous blog post:
</p>
<ul>
<li>
Precision is calculated as the number of correctly matched relationships
resulting from a strategy, divided by the total number of matched
relationships. It can also be interpreted as the probability that a
match is correct. Low precision indicates a high rate of false
positives, which are incorrect relationships created by the strategy.
</li>
<li>
Recall is calculated as the number of correctly matched relationships
resulting from a strategy, divided by the number of true (expected)
relationships. It can also be interpreted as the probability that a true
(correct) relationship will be created by the strategy. Low recall means
a high rate of false negatives, which are relationships that should have
been created by the strategy but were not made.
</li>
</ul>
<p>
Applying these measures to our prior example, the strategies achieved
the following results:
</p>
<ul>
<li>
Strategy 1: accuracy 0.67, precision 0.5, recall 0.5
</li>
<li>
Strategy 2: accuracy 0.67, precision 1.0, recall 0.5
</li>
</ul>
<p>
As we can see, while both strategies have the same accuracy, using
precision and recall better describes the difference between the two
sets of results. Strategy 1’s lower precision indicates it made false
positive matches, while Strategy 2’s perfect precision shows that it
made none. The identical recall scores show both identified half of the
possible matches.
</p>
<p>
Of course, results calculated using such a small dataset are not very
meaningful. If we obtained these scores from a large, representative
evaluation dataset, it would indicate to us that Strategy 1 risks
introducing many incorrect relationships, while Strategy 2 would be
unlikely to do so. In both cases, we would still expect approximately
half of the possible relationships to be missing from the strategies’
outputs.
</p>
<p>
Which one is more important to prioritise, precision or recall? It
depends on the use case. As a general rule, if you want to use the
strategy in a fully automated way, without any form of manual review or
correction of the results, we recommend paying more attention to
precision. Privileging precision will allow you to better control the
number of incorrect relationships added to your data. If you want to use
the strategy in a semi-automated fashion, where there is a manual
examination of and a chance to correct the results, pay more attention
to recall. Doing so will guarantee that enough options are presented
during the manual review stage and fewer relationships will be missed as
a result.
</p>
<p>
To get a more balanced estimation of performance, we can also consider
both precision and recall at the same time using a measure called
<a href="https://en.wikipedia.org/wiki/F-score" target="_blank">F-score</a>.
F-score combines precision and recall into a single number, with
variable weight given to either aspect. There are three commonly used
types, each calculated as the weighted
<a href="https://en.wikipedia.org/wiki/Harmonic_mean" target="_blank">harmonic
mean</a> of precision and recall:
</p>
<ul>
<li>
F0.5: Precision is weighted more heavily. It can be understood as a
score that is 50% more sensitive to precision than recall. A high F0.5
score indicates a measure of performance that minimises false positives.
</li>
<li>
F1: Equal weight is given to both precision and recall. It can be
interpreted as the most balanced score in this set. High F1 indicates
good overall performance, with both false positives and false negatives
being minimised equally.
</li>
<li>
F2: Recall is weighted more heavily. It can be understood as a score
that is 50% more sensitive to recall than precision. A high F2 score
indicates a measure of performance where false negatives are minimised.
</li>
</ul>
<p>
Each of these variants allows for fine-tuning the evaluation metric to
align with your expectations for a specific matching task. Choose
whichever reflects the relative importance of precision versus recall
for your use case.
</p>
<p>
To summarise, to avoid falling prey to misleading sales pitches or silly
quizzes, it is important to have a good understanding of the performance
of any strategies you are building or integrating. With thorough
evaluation, including a representative dataset and carefully considered
metrics, we can estimate the quality of matching and, by extension, its
resulting relationships.
</p>
<p>
Now that we’ve covered how to evaluate effectively, we can move on to
some other aspects of metadata matching. Our next blog post will take a
final, more holistic view of matching, exploring some complementary
considerations to all of the preceding. Stay tuned for more!
</p>
<p><br></p>
<p><a href="https://www.crossref.org/blog/how-good-is-your-matching/"
class="uri">https://www.crossref.org/blog/how-good-is-your-matching/</a></p>
<hr />
<h2
id="columbus-neighborhood-newspapers-showcase-the-citys-diverse-communities">Columbus
Neighborhood Newspapers Showcase the City’s Diverse Communities</h2>
<p>date: 2024-11-05, from: Internet Archive Blog</p>
<p>The following guest post from Aaron O’Donovan
(aodonovan@columbuslibrary.org), Columbus Metropolitan Library Special
Collections Manager, is part of a series written by members of the
Internet Archive’s Community Webs program. Community […]</p>
<p><br></p>
<p><a
href="https://blog.archive.org/2024/11/05/columbus-neighborhood-newspapers-showcase-the-citys-diverse-communities/"
class="uri">https://blog.archive.org/2024/11/05/columbus-neighborhood-newspapers-showcase-the-citys-diverse-communities/</a></p>
<hr />
<h2
id="the-top-ten-challenges-needs-and-goals-of-publishers-and-how-ai-can-help-in-digital-transformation-and-the-open-science-movement">The
Top Ten Challenges, Needs, and Goals of Publishers – and How AI Can Help
in Digital Transformation and the Open Science Movement</h2>
<p>date: 2024-11-05, from: Scholarly Kitchen</p>
<p>
As artificial intelligence begins to play an ever-bigger role in the
scholarly publishing landscape, how might it help solve some of the
biggest challenges facing publishers?
</p>
<p>
The post
<a href="https://scholarlykitchen.sspnet.org/2024/11/05/the-top-ten-challenges-needs-and-goals-of-publishers-and-how-ai-can-help-in-digital-transformation-and-the-open-science-movement/">The
Top Ten Challenges, Needs, and Goals of Publishers – and How AI Can Help
in Digital Transformation and the Open Science Movement</a> appeared
first on <a href="https://scholarlykitchen.sspnet.org">The Scholarly
Kitchen</a>.
</p>
<p><br></p>
<p><a
href="https://scholarlykitchen.sspnet.org/2024/11/05/the-top-ten-challenges-needs-and-goals-of-publishers-and-how-ai-can-help-in-digital-transformation-and-the-open-science-movement/"
class="uri">https://scholarlykitchen.sspnet.org/2024/11/05/the-top-ten-challenges-needs-and-goals-of-publishers-and-how-ai-can-help-in-digital-transformation-and-the-open-science-movement/</a></p>
<hr />
<h2
id="vanishing-culture-qa-with-philip-bump-the-washington-post">Vanishing
Culture: Q&amp;A with Philip Bump, The Washington Post</h2>
<p>date: 2024-11-04, from: Internet Archive Blog</p>
<p>The following Q&amp;A between writer Caralee Adams and
journalist Philip Bump of The Washington Post is part of our Vanishing
Culture series, highlighting the power and importance of preservation in
our digital age. Read more […]</p>
<p><br></p>
<p><a
href="https://blog.archive.org/2024/11/04/vanishing-culture-qa-with-philip-bump-the-washington-post/"
class="uri">https://blog.archive.org/2024/11/04/vanishing-culture-qa-with-philip-bump-the-washington-post/</a></p>
<hr />
<h2 id="là-bas-by-j.-k.-huysmans">Là-Bas, by J.-K. Huysmans</h2>
<p>date: 2024-11-04, from: Standard Ebooks, new releaases</p>
<p>An author struggles to complete his book without having any practical
experience of Satanism.</p>
<p><br></p>
<p><a
href="https://standardebooks.org/ebooks/j-k-huysmans/la-bas/keene-wallace"
class="uri">https://standardebooks.org/ebooks/j-k-huysmans/la-bas/keene-wallace</a></p>
<hr />
<h2
id="mental-health-awareness-mondays-ssp-launches-the-mental-health-awareness-and-action-community-of-interest-group">Mental
Health Awareness Mondays – SSP Launches the Mental Health Awareness and
Action Community of Interest Group</h2>
<p>date: 2024-11-04, from: Scholarly Kitchen</p>
<p>
The Society for Scholarly Publishing is launching the Mental Health
Awareness and Action Community of Interest (CoIN) Group.
</p>
<p>
The post
<a href="https://scholarlykitchen.sspnet.org/2024/11/04/mental-health-awareness-mondays-ssp-launches-the-mental-health-awareness-and-action-community-of-interest-group/">Mental
Health Awareness Mondays – SSP Launches the Mental Health Awareness and
Action Community of Interest Group</a> appeared first on
<a href="https://scholarlykitchen.sspnet.org">The Scholarly Kitchen</a>.
</p>
<p><br></p>
<p><a
href="https://scholarlykitchen.sspnet.org/2024/11/04/mental-health-awareness-mondays-ssp-launches-the-mental-health-awareness-and-action-community-of-interest-group/"
class="uri">https://scholarlykitchen.sspnet.org/2024/11/04/mental-health-awareness-mondays-ssp-launches-the-mental-health-awareness-and-action-community-of-interest-group/</a></p>
</section>
<footer>
Antenna is a personal aggregation of items found around the web.
Curated with <a href="https://rsdoiel.github.io/skimmer">skimmer</a> and <a href="https://sqlite.org">sqlite</a> then rendered with <a href="https://pandoc.org">Pandoc</a>.
</footer>
</body>
</html>
