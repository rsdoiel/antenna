<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8" >
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" >
  <title>snapshots</title>
<!--  <link rel="stylesheet" type="text/css"  href="webfonts/fonts.css" media="screen" > -->
  <link rel="stylesheet" type="text/css"  href="css/site.css" media="screen" >
</head>
<body>
<header>
	<img class="logo" 
		src="https://upload.wikimedia.org/wikipedia/commons/9/9c/Antenna_1_-_The_Noun_Project.svg"
		alt="line art showing an antenna"
		height="80" width="60" >
	<h1>The Antenna</h1> 
	<h2>finding signal in the noise</h2>
</header>
<nav>
<ul>
	<li><a href="./">The Antenna</a></li>
	<li><a href="archives/">Archives</a></li>
	<li><a href="about.html">About</a></li>
</ul>
</nav>
<section>
<div class="description-for-items">
Items collected from feeds in <a href="snapshots.txt">snapshots.txt</a>
</div>
<h1 id="snapshots">snapshots</h1>
<p>(date: 2025-08-06 14:11:00)</p>
<hr />
<h2
id="ice-is-buying-mobile-iris-scanning-tech-for-its-deportation-arm">ICE
Is Buying Mobile Iris Scanning Tech for Its Deportation Arm</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>MORIS and I.R.I.S. was designed for Sheriff‚Äôs Offices to identify
known persons with their iris. Now ICE says it plans to buy the
tech.</p>
<p><br></p>
<p><a
href="https://www.404media.co/ice-is-buying-mobile-iris-scanning-tech-for-its-deportation-arm/"
class="uri">https://www.404media.co/ice-is-buying-mobile-iris-scanning-tech-for-its-deportation-arm/</a></p>
<hr />
<h2
id="trump-is-launching-an-ai-search-engine-powered-by-perplexity">Trump
Is Launching an AI Search Engine Powered by Perplexity</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>America‚Äôs scandalous president is teaming up with its most
disreputable AI company to make a search engine.</p>
<p><br></p>
<p><a
href="https://www.404media.co/trump-is-launching-an-ai-search-engine-powered-by-perplexity/"
class="uri">https://www.404media.co/trump-is-launching-an-ai-search-engine-powered-by-perplexity/</a></p>
<hr />
<h2
id="jules-our-asynchronous-coding-agent-is-now-available-for-everyone">Jules,
our asynchronous coding agent, is now available for everyone</h2>
<p>date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://blog.google/technology/google-labs/jules-now-available/">Jules,
our asynchronous coding agent, is now available for
everyone</a></strong>
</p>
I wrote about the Jules beta
<a href="https://simonwillison.net/2025/May/19/jules/">back in May</a>.
Google‚Äôs version of the OpenAI Codex PR-submitting hosted coding tool
graduated from beta today.
</p>
<p>
I‚Äôm mainly linking to this now because I like the new term they are
using in this blog entry: <strong>Asynchronous coding agent</strong>. I
like it so much I
<a href="https://simonwillison.net/tags/asynchronous-coding-agents/">gave
it a tag</a>.
</p>
<p>
I continue to avoid the term ‚Äúagent‚Äù as infuriatingly vague, but I can
grudgingly accept it when accompanied by a prefix that clarifies the
type of agent we are talking about. ‚ÄúAsynchronous coding agent‚Äù feels
just about obvious enough to me to be useful.
</p>
<p>
‚Ä¶ I just ran a Google search for <code>‚Äúasynchronous coding agent‚Äù
-jules</code> and came up with a few more notable examples of this name
being used elsewhere:
</p>
<ul>
<li>
<a href="https://blog.langchain.com/introducing-open-swe-an-open-source-asynchronous-coding-agent/">Introducing
Open SWE: An Open-Source Asynchronous Coding Agent</a> is an
announcement from LangChain just this morning of their take on this
pattern. They provide a hosted version (bring your own API keys) or you
can run it yourself with
<a href="https://github.com/langchain-ai/open-swe">their MIT licensed
code</a>.
</li>
<li>
The press release for GitHub‚Äôs own version of this
<a href="https://github.com/newsroom/press-releases/coding-agent-for-github-copilot">GitHub
Introduces Coding Agent For GitHub Copilot</a> states that ‚ÄúGitHub
Copilot now includes an asynchronous coding agent‚Äù.
</li>
</ul>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://news.ycombinator.com/item?id=44813854&quot;&gt;Hacker News&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/github&quot;&gt;github&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/google&quot;&gt;google&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-assisted-programming&quot;&gt;ai-assisted-programming&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gemini&quot;&gt;gemini&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/agent-definitions&quot;&gt;agent-definitions&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/asynchronous-coding-agents&quot;&gt;asynchronous-coding-agents&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/6/asynchronous-coding-agents/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/6/asynchronous-coding-agents/#atom-everything</a></p>
<hr />
<h2 id="xcode-26-beta-5">Xcode 26 Beta 5</h2>
<p>date: 2025-08-06, from: Michael Tsai</p>
<p>Apple: Xcode 26 beta 5 requires a Mac running macOS Sequoia 15.5 or
later. The download is back to being a .xip file, and there‚Äôs a slightly
smaller Apple Silicon‚Äìonly version. Again, the release notes don‚Äôt
actually show what‚Äôs new in this build. The span property of UTF8View
does not support the small string representation [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/08/06/xcode-26-beta-5/"
class="uri">https://mjtsai.com/blog/2025/08/06/xcode-26-beta-5/</a></p>
<hr />
<h2 id="ios-26-developer-beta-5">iOS 26 Developer Beta 5</h2>
<p>date: 2025-08-06, from: Michael Tsai</p>
<p>Juli Clover: Apple today provided developers with the fifth betas of
iOS 26 and iPadOS 26 for testing purposes, with the updates coming a
week after Apple seeded the fourth betas. Juli Clover: Apple is
continuing to refine button placement, animations, and design in
preparation for launching iOS 26 in September.[‚Ä¶]Apple added a toggle in
[‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/08/06/ios-26-developer-beta-5/"
class="uri">https://mjtsai.com/blog/2025/08/06/ios-26-developer-beta-5/</a></p>
<hr />
<h2 id="ublock-origin-lite-for-safari">uBlock Origin Lite for
Safari</h2>
<p>date: 2025-08-06, from: Michael Tsai</p>
<p>PseudorandomNoise (Hacker News): TLDR uBO Lite is available in Test
Flight today for all the Cupertino OS‚Äôs Jen Simmons (Hacker News,
Reddit): Over the years, I‚Äôve heard a lot of developers &amp; other
people wish that uBlock Origin was available for Safari. Now it is!
Download for Safari 18.6 and Safari 26 beta. VastTension6022: I [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/06/ublock-origin-lite-for-safari/"
class="uri">https://mjtsai.com/blog/2025/08/06/ublock-origin-lite-for-safari/</a></p>
<hr />
<h2 id="disk-utility-erase-process-has-failed">Disk Utility: Erase
Process Has Failed</h2>
<p>date: 2025-08-06, from: Michael Tsai</p>
<p>Especially with macOS Sequoia, I get this error almost every time I
connect a hard drive or SSD and try to erase it. Disk Utility reports:
The calling process or user lacks the proper privileges to perform this
operation The workaround is to quit and relaunch Disk Utility.</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/06/disk-utility-erase-process-has-failed/"
class="uri">https://mjtsai.com/blog/2025/08/06/disk-utility-erase-process-has-failed/</a></p>
<hr />
<h2
id="my-video-chat-with-gary-ginsberg-friend-and-confidante-of-jfk-jr-and-a-producer-of-the-new-cnn-documentary-american-prince-jfk-jr.">My
video chat with Gary Ginsberg, friend and confidante of JFK Jr, and a
producer of the new CNN documentary American Prince, JFK Jr.¬†</h2>
<p>date: 2025-08-06, from: Tina Brown</p>
<p>Plus, his fix for the divided Democratic Party‚Äì and the light-saber
battle between Trump and Rupert Murdoch</p>
<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://api.substack.com/feed/podcast/170281160/c8eefaa03fe7193bb429226c31f117af.mp3">
</source>
</audio>
<p><a href="https://api.substack.com/feed/podcast/170281160/c8eefaa03fe7193bb429226c31f117af.mp3" target="_blank">download
audio/mpeg</a><br></p>
<p><a
href="https://tinabrown.substack.com/p/my-video-chat-with-gary-ginsberg"
class="uri">https://tinabrown.substack.com/p/my-video-chat-with-gary-ginsberg</a></p>
<hr />
<p><strong><span class="citation" data-cites="Robert">@Robert</span>‚Äôs
feed at BlueSky</strong> (date: 2025-08-06, from: Robert‚Äôs feed at
BlueSky)</p>
<p>üìå</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/rsdoiel.bsky.social/post/3lvqrdh2mhc27"
class="uri">https://bsky.app/profile/rsdoiel.bsky.social/post/3lvqrdh2mhc27</a></p>
<hr />
<h2
id="cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m.2-ethernet-and-hdmi">CM5
MINIMA is a tiny $65 Raspberry Pi CM5 carrier board with M.2, Ethernet,
and HDMI</h2>
<p>date: 2025-08-06, from: Liliputing</p>
<p>
The credit card-sized Raspberry Pi Model B line of computers may be
small, at just 85 x 56mm. But the Raspberry Compute Module 5 is even
smaller, measuring just 55 x 40mm. Unfortunately this tiny
computer-on-a-module isn‚Äôt much use on its own, because it lacks the
full-sized ports you‚Äôd need to connect a power source, [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m-2-ethernet-and-hdmi/">CM5
MINIMA is a tiny $65 Raspberry Pi CM5 carrier board with M.2, Ethernet,
and HDMI</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m-2-ethernet-and-hdmi/"
class="uri">https://liliputing.com/cm5-minima-is-a-tiny-65-raspberry-pi-cm5-carrier-board-with-m-2-ethernet-and-hdmi/</a></p>
<hr />
<h2 id="tom-macwright-observable-notebooks-2.0">Tom MacWright:
Observable Notebooks 2.0</h2>
<p>date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://macwright.com/2025/07/31/observable-notebooks-2">Tom
MacWright: Observable Notebooks 2.0</a></strong>
</p>
Observable announced
<a href="https://observablehq.com/notebook-kit/">Observable Notebooks
2.0</a> last week - the latest take on their JavaScript notebook
technology, this time with an
<a href="https://observablehq.com/notebook-kit/kit">open file format</a>
and a brand new
<a href="https://observablehq.com/notebook-kit/desktop">macOS desktop
app</a>.
</p>
<p>
Tom MacWright worked at Observable during their first iteration and here
provides thoughtful commentary from an insider-to-outsider perspective
on how their platform has evolved over time.
</p>
<p>
I particularly appreciated this aside on the downsides of evolving your
own not-quite-standard language syntax:
</p>
<blockquote>
<p>
Notebook Kit and Desktop
<a href="https://observablehq.com/notebook-kit/#vanilla-java-script">support
vanilla JavaScript</a>, which is excellent and cool. The Observable
changes to JavaScript were always tricky and meant that we struggled to
use off-the-shelf parsers, and users couldn‚Äôt use standard JavaScript
tooling like eslint. This is stuff like the <code>viewof</code> operator
which meant that
<a href="https://observablehq.com/@observablehq/observable-javascript">Observable
was not JavaScript</a>. [‚Ä¶] <em>Sidenote</em>: I now work on
<a href="https://www.val.town/">Val Town</a>, which is also a platform
based on writing JavaScript, and when I joined it <em>also</em> had a
tweaked version of JavaScript. We used the <code>@</code> character to
let you ‚Äòmention‚Äô other vals and implicitly import them. This was, like
it was in Observable, not worth it and we switched to standard syntax:
don‚Äôt mess with language standards folks!
</p>
</blockquote>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/javascript&quot;&gt;javascript&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/observable&quot;&gt;observable&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/tom-macwright&quot;&gt;tom-macwright&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/val-town&quot;&gt;val-town&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/6/observable-notebooks-20/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/6/observable-notebooks-20/#atom-everything</a></p>
<hr />
<h2
id="fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40">Fruit
Jam credit card-sized PC with an RP2350B chip launches for $40</h2>
<p>date: 2025-08-06, from: Liliputing</p>
<p>
Earlier this year Adafruit introduced a credit card-sized computer
called the Fruit Jam. It‚Äôs the size of a typical Raspberry Pi Model B,
but it‚Äôs powered by a low-power Raspberry Pi RP2350B microcontroller.
This weekend the company announced that the Fruit Jam was available for
purchase for $40. Only a few units were available at [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40/">Fruit
Jam credit card-sized PC with an RP2350B chip launches for $40</a>
appeared first on <a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40/"
class="uri">https://liliputing.com/fruit-jam-credit-card-sized-pc-with-an-rp2350b-chip-launches-for-40/</a></p>
<hr />
<h2
id="home-depot-and-lowes-share-data-from-hundreds-of-ai-cameras-with-cops">Home
Depot and Lowe‚Äôs Share Data From Hundreds of AI Cameras With Cops</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>Home improvement stores are finding ways to share data from their
Flock license plate reader cameras with law enforcement, according to
public records.</p>
<p><br></p>
<p><a
href="https://www.404media.co/home-depot-and-lowes-share-data-from-hundreds-of-ai-cameras-with-cops/"
class="uri">https://www.404media.co/home-depot-and-lowes-share-data-from-hundreds-of-ai-cameras-with-cops/</a></p>
<hr />
<h2
id="constitution-sections-on-due-process-and-foreign-gifts-just-vanished-from-congress-website">Constitution
Sections on Due Process and Foreign Gifts Just Vanished from Congress‚Äô
Website</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>Part of Article I Section 8, and all of Sections 9 and 10, which
address things like habeas corpus, nobility, and militias, are gone from
Congress‚Äôs website for the Constitution.</p>
<p><br></p>
<p><a
href="https://www.404media.co/constitution-sections-on-due-process-and-foreign-gifts-just-vanished-from-congress-website/"
class="uri">https://www.404media.co/constitution-sections-on-due-process-and-foreign-gifts-just-vanished-from-congress-website/</a></p>
<hr />
<h2
id="million-year-old-evidence-of-epic-journey-near-hobbit-island-discovered-by-scientists">Million-Year-Old
Evidence of Epic Journey Near ‚ÄòHobbit‚Äô Island Discovered by
Scientists</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>Stone tools found on the Indonesian island of Sulawesi reveal a
long-lost population of human relatives; their identity, and how they
crossed the sea, is a mystery.</p>
<p><br></p>
<p><a
href="https://www.404media.co/indonesian-island-sulawesi-early-humans-hobbits-calio/"
class="uri">https://www.404media.co/indonesian-island-sulawesi-early-humans-hobbits-calio/</a></p>
<hr />
<h2 id="ai-and-data-privacy-under-scrutiny">AI and Data Privacy Under
Scrutiny</h2>
<p>date: 2025-08-06, from: Purism News and Events</p>
<p>
The promise of AI is seductive: instant answers, personalized insights,
and a frictionless interface with the digital world. But beneath the
surface of convenience lies a growing privacy crisis‚Äîone that‚Äôs now
impossible to ignore.
</p>
<p>
The post
<a rel="nofollow" href="https://puri.sm/posts/ai-and-data-privacy-under-scrutiny/">AI
and Data Privacy Under Scrutiny</a> appeared first on
<a rel="nofollow" href="https://puri.sm/">Purism</a>.
</p>
<p><br></p>
<p><a href="https://puri.sm/posts/ai-and-data-privacy-under-scrutiny/"
class="uri">https://puri.sm/posts/ai-and-data-privacy-under-scrutiny/</a></p>
<hr />
<h2 id="the-case-for-optimism">The Case for Optimism</h2>
<p>date: 2025-08-06, from: Guy Kawasaki blog</p>
<p>It‚Äôs a strategic advantage.</p>
<p><br></p>
<p><a href="https://guykawasaki.substack.com/p/the-case-for-optimism"
class="uri">https://guykawasaki.substack.com/p/the-case-for-optimism</a></p>
<hr />
<h2
id="watch-this-guys-interactive-wooden-pixel-machine-make-art-in-real-time">Watch
This Guy‚Äôs Interactive Wooden Pixel Machine Make Art in Real Time</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>Kilopixel by Ben Holmen turns a CNC machine and a thousand wooden
blocks into pixel art.</p>
<p><br></p>
<p><a
href="https://www.404media.co/kilopixel-live-stream-interactive-pixel-art/"
class="uri">https://www.404media.co/kilopixel-live-stream-interactive-pixel-art/</a></p>
<hr />
<h2
id="join-me-today-at-12pet-for-a-video-chat-with-the-great-politicalmedia-insider-gary-ginsberg-on-the-new-doc-about-his-friend-jfk-jr-and-the-ongoing-smackdown-between-trump-and-the-media">Join
me today at 12pET for a video chat with the great political/media
insider Gary Ginsberg on the new doc about his friend JFK Jr, and the
ongoing smackdown between Trump and the media</h2>
<p>date: 2025-08-06, from: Tina Brown</p>
<p>There is much to discuss with Ginsberg, who is consulting producer on
American Prince: JFK Jr., a new documentary on CNN.</p>
<p><br></p>
<p><a
href="https://tinabrown.substack.com/p/join-me-today-at-12pet-for-a-video"
class="uri">https://tinabrown.substack.com/p/join-me-today-at-12pet-for-a-video</a></p>
<hr />
<h2 id="podcast-google-is-exposing-peoples-chatgpt-secrets">Podcast:
Google Is Exposing Peoples‚Äô ChatGPT Secrets</h2>
<p>date: 2025-08-06, from: 404 Media Group</p>
<p>Shared ChatGPT indexed by Google; how Wikipedia is fighting AI slop;
and the history of how we got to Steam censorship.</p>
<p><br></p>
<p><a
href="https://www.404media.co/podcast-google-is-exposing-peoples-chatgpt-secrets/"
class="uri">https://www.404media.co/podcast-google-is-exposing-peoples-chatgpt-secrets/</a></p>
<hr />
<h2 id="quoting-artificial-analysis">Quoting Artificial Analysis</h2>
<p>date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://x.com/artificialanlys/status/1952887733803991070">
<p>
<strong>gpt-oss-120b is the most intelligent American open weights
model, comes behind DeepSeek R1 and Qwen3 235B in intelligence but
offers efficiency benefits</strong> [‚Ä¶]
</p>
<p>
We‚Äôre seeing the 120B beat o3-mini but come in behind o4-mini and o3.
The 120B is the most intelligent model that can be run on a single H100
and the 20B is the most intelligent model that can be run on a consumer
GPU. [‚Ä¶]
</p>
<p>
While the larger gpt-oss-120b does not come in above DeepSeek R1 0528‚Äôs
score of 59 or Qwen3 235B 2507s score of 64, it is notable that it is
significantly smaller in both total and active parameters than both of
those models.
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://x.com/artificialanlys/status/1952887733803991070">Artificial
Analysis</a>, see also their
<a href="https://artificialanalysis.ai/models/open-source">updated
leaderboard</a>
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/evals&quot;&gt;evals&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/deepseek&quot;&gt;deepseek&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/qwen&quot;&gt;qwen&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gpt-oss&quot;&gt;gpt-oss&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/6/artificial-analysis/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/6/artificial-analysis/#atom-everything</a></p>
<hr />
<h2 id="about-that-stock-market">About That Stock Market</h2>
<p>date: 2025-08-06, from: Paul Krugman</p>
<p>Policy has gone mad; why aren‚Äôt stocks down?</p>
<p><br></p>
<p><a href="https://paulkrugman.substack.com/p/about-that-stock-market"
class="uri">https://paulkrugman.substack.com/p/about-that-stock-market</a></p>
<hr />
<h2
id="ai-interdimensional-phone-entertains-party-guests-like-its-1999">AI
interdimensional phone entertains party guests like it‚Äôs 1999</h2>
<p>date: 2025-08-06, from: Raspberry Pi News (.com)</p>
<p>
This AI-powered landline phone talks to guests, gives them clues, and
sends them on a mystery tour around the house.
</p>
<p>
The post
<a href="https://www.raspberrypi.com/news/ai-interdimensional-phone-entertains-party-guests-like-its-1999/">AI
interdimensional phone entertains party guests like it‚Äôs 1999</a>
appeared first on <a href="https://www.raspberrypi.com">Raspberry
Pi</a>.
</p>
<p><br></p>
<p><a
href="https://www.raspberrypi.com/news/ai-interdimensional-phone-entertains-party-guests-like-its-1999/"
class="uri">https://www.raspberrypi.com/news/ai-interdimensional-phone-entertains-party-guests-like-its-1999/</a></p>
<hr />
<h2 id="no-ai-is-not-making-engineers-10x-as-productive">No, AI is not
Making Engineers 10x as Productive</h2>
<p>date: 2025-08-06, updated: 2025-08-06, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">No,
AI is not Making Engineers 10x as Productive</a></strong>
</p>
Colton Voege on ‚Äúcuring your AI 10x engineer imposter syndrome‚Äù.
</p>
<p>
There‚Äôs a lot of rhetoric out there suggesting that if you can‚Äôt 10x
your productivity through tricks like running a dozen Claude Code
instances at once you‚Äôre falling behind. Colton‚Äôs piece here is a pretty
thoughtful exploration of why that likely isn‚Äôt true. I found myself
agreeing with quite a lot of this article.
</p>
<p>
I‚Äôm a pretty huge proponent for AI-assisted development, but I‚Äôve never
found those 10x claims convincing. I‚Äôve estimated that LLMs make me 2-5x
more productive on the parts of my job which involve typing code into a
computer, which is itself a small portion of that I do as a software
engineer.
</p>
<p>
That‚Äôs not too far from this article‚Äôs assumptions. From the article:
</p>
<blockquote>
<p>
I wouldn‚Äôt be surprised to learn AI helps many engineers do certain
tasks 20-50% faster, but the nature of software bottlenecks mean this
doesn‚Äôt translate to a 20% productivity increase and certainly not a 10x
increase.
</p>
</blockquote>
<p>
<p>I think that‚Äôs an under-estimation - I suspect engineers that really
know how to use this stuff effectively will get more than a 0.2x
increase - but I do think all of the <em>other stuff</em> involved in
building software makes the 10x thing unrealistic in most cases.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://news.ycombinator.com/item?id=44798189&quot;&gt;Hacker News&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/careers&quot;&gt;careers&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-assisted-programming&quot;&gt;ai-assisted-programming&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/6/not-10x/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/6/not-10x/#atom-everything</a></p>
<hr />
<h2 id="ghc-9.10.3-rc2-is-now-available">GHC 9.10.3-rc2 is now
available</h2>
<p>date: 2025-08-06, from: Glasgow Haskell Compiler</p>
<h1>
GHC 9.10.3-rc2 is now available
</h1>
<h4 class="text-muted">
wz1000 - 2025-08-06
</h4>
<p>
The GHC developers are very pleased to announce the availability of the
second release candidate for GHC 9.10.3. Binary distributions, source
distributions, and documentation are available at
<a href="https://downloads.haskell.org/ghc/9.10.3-rc2">downloads.haskell.org</a>
and via <a href="https://www.haskell.org/ghcup/">GHCup</a>.
</p>
<p>
GHC 9.10.3 is a bug-fix release fixing over 50 issues of a variety of
severities and scopes. A full accounting of these fixes can be found in
the
<a href="https://gitlab.haskell.org/ghc/ghc/-/blob/ghc-9.10/docs/users_guide/9.10.3-notes.rst?ref_type=heads&amp;plain=1">release
notes</a>. As always, GHC‚Äôs release status, including planned future
releases, can be found on the GHC Wiki
<a href="https://gitlab.haskell.org/ghc/ghc/-/wikis/GHC-status">status</a>.
</p>
<p>
The changes from the first release candidate are:
</p>
<ul>
<li>
Bumping the text submodule to 2.1.3
</li>
<li>
Reverting a bug fix
(<a href="https://gitlab.haskell.org/ghc/ghc/merge_requests/14291">!14291</a>)
that restricted previously allowed namespace specifiers
(<a href="https://gitlab.haskell.org/ghc/ghc/issues/26250">#26250</a>)
</li>
<li>
Reverting the bump of the deepseq submodule to 1.5.2.0
(<a href="https://gitlab.haskell.org/ghc/ghc/issues/26251">#26251</a>)
</li>
</ul>
<p>
This release candidate will have a two-week testing period. If all goes
well the final release will be available the week of 19 August 2025.
</p>
<p>
We would like to thank Well-Typed, Tweag I/O, Juspay, QBayLogic,
Channable, Serokell, SimSpace, the Haskell Foundation, and other
anonymous contributors whose on-going financial and in-kind support has
facilitated GHC maintenance and release management over the years.
Finally, this release would not have been possible without the hundreds
of open-source contributors whose work comprise this release.
</p>
<p>
As always, do give this release a try and open a
<a href="https://gitlab.haskell.org/ghc/ghc/-/issues/new">ticket</a> if
you see anything amiss.
</p>
<p><br></p>
<p><a
href="http://haskell.org/ghc/blog/20250806-ghc-9.10.3-rc2-released.html"
class="uri">http://haskell.org/ghc/blog/20250806-ghc-9.10.3-rc2-released.html</a></p>
<hr />
<h2 id="out-fibbing-cpython-with-the-plush-interpreter">Out-Fibbing
CPython with the Plush Interpreter</h2>
<p>date: 2025-08-06, from: Pointers gone wild blog</p>
<p><br></p>
<p><a
href="https://pointersgonewild.com/2025-08-06-out-fibbing-cpython-with-the-plush-interpreter/"
class="uri">https://pointersgonewild.com/2025-08-06-out-fibbing-cpython-with-the-plush-interpreter/</a></p>
<hr />
<h2
id="available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge">Available
today: gpt-oss-20B Model on Windows with GPU Acceleration ‚Äì further
pushing the boundaries on the edge</h2>
<p>date: 2025-08-05, from: Windows Developer Blog</p>
<p>
With OpenAI‚Äôs release of gpt-oss models today, we are thrilled to bring
GPU optimized gpt-oss-20B model variants to Windows devices.
</p>
<p>
This milestone brings powerful, open-source reasoning models to Windows
developers, with support for local inferen
</p>
<p>
The post
<a href="https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/">Available
today: gpt-oss-20B Model on Windows with GPU Acceleration ‚Äì further
pushing the boundaries on the edge</a> appeared first on
<a href="https://blogs.windows.com/windowsdeveloper">Windows Developer
Blog</a>.
</p>
<p><br></p>
<p><a
href="https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/"
class="uri">https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/</a></p>
<hr />
<h2
id="openais-new-open-weight-apache-2-models-are-really-good">OpenAI‚Äôs
new open weight (Apache 2) models are really good</h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs
Weblog</p>
<p>
The long promised
<a href="https://openai.com/index/introducing-gpt-oss/">OpenAI open
weight models are here</a>, and they are <em>very</em> impressive.
They‚Äôre available under proper open source licenses - Apache 2.0 - and
come in two sizes, 120B and 20B.
</p>
<p>
OpenAI‚Äôs own benchmarks are eyebrow-raising - emphasis mine:
</p>
<blockquote>
<p>
The <strong>gpt-oss-120b</strong> model achieves <strong>near-parity
with OpenAI o4-mini</strong> on core reasoning benchmarks, while running
efficiently on a single 80 GB GPU. The <strong>gpt-oss-20b</strong>
model delivers <strong>similar results to OpenAI o3‚Äëmini</strong> on
common benchmarks and can run on edge devices with just 16 GB of memory,
making it ideal for on-device use cases, local inference, or rapid
iteration without costly infrastructure.
</p>
</blockquote>
<p>
o4-mini and o3-mini are <em>really good</em> proprietary models - I was
not expecting the open weights releases to be anywhere near that class,
especially given their small sizes. That gpt-oss-20b model should run
quite comfortably on a Mac laptop with 32GB of RAM.
</p>
<p>
Both models are mixture-of-experts:
</p>
<blockquote>
<p>
gpt-oss-120b activates 5.1B parameters per token, while gpt-oss-20b
activates 3.6B. The models have 117b and 21b total parameters
respectively.
</p>
</blockquote>
<p>
Something that surprised me even more about the benchmarks was the
scores for general knowledge based challenges. I can just about believe
they managed to train a strong reasoning model that fits in 20B
parameters, but these models score highly on benchmarks like ‚ÄúGPQA
Diamond (without tools) PhD-level science questions‚Äù too:
</p>
<ul>
<li>
o3 ‚Äî 83.3%
</li>
<li>
o4-mini ‚Äî 81.4%
</li>
<li>
gpt-oss-120b ‚Äî 80.1%
</li>
<li>
o3-mini ‚Äî 77%
</li>
<li>
gpt-oss-20b ‚Äî 71.5%
</li>
</ul>
<p>
A lot of these benchmarks are edging towards saturated.
</p>
<ul>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#running-gpt-oss-20b-on-my-mac-with-lm-studio">Running
gpt-oss-20b on my Mac with LM Studio</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-low">Pelican
on reasoning=low</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-medium">Pelican
on reasoning=medium</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#pelican-on-reasoning-high">Pelican
on reasoning=high</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#space-invaders-with-gpt-oss-20b">Space
invaders with gpt-oss-20b</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#trying-gpt-oss-120b-via-api-providers">Trying
gpt-oss-120b via API providers</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#llama-cpp-is-coming-very-shortly">llama.cpp
is coming very shortly</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#gpt-oss-20b-in-ollama">gpt-oss:20b
in Ollama</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#the-model-card">Training
details from the model card</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#openai-harmony-a-new-format-for-prompt-templates">OpenAI
Harmony, a new format for prompt templates</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#the-open-question-for-me-how-good-is-tool-calling-">The
open question for me: how good is tool calling?</a>
</li>
<li>
<a href="https://simonwillison.net/2025/Aug/5/gpt-oss/#china">Competing
with the Chinese open models</a>
</li>
</ul>
<h4 id="running-gpt-oss-20b-on-my-mac-with-lm-studio">
Running gpt-oss-20b on my Mac with LM Studio
</h4>
<p>
There are already a bunch of different ways to run these models - OpenAI
partnered with numerous organizations in advance of the release.
</p>
<p>
I decided to start with <a href="https://lmstudio.ai/">LM Studio</a>.
</p>
<p>
I had to update to the most recent version of the app, then install the
new model from
<a href="https://lmstudio.ai/models/openai/gpt-oss-20b">their
openai/gpt-oss-20b</a> page.
</p>
<p>
First impressions: this is a <em>really good</em> model, and it somehow
runs using just 11.72GB of my system RAM.
</p>
<p>
The model supports three reasoning efforts: low, medium and high. LM
Studio makes those available via a dropdown.
</p>
<p>
Let‚Äôs try ‚ÄúGenerate an SVG of a pelican riding a bicycle‚Äù:
</p>
<h4 id="pelican-on-reasoning-low">
Pelican on reasoning=low
</h4>
<p>
I started
<a href="https://gist.github.com/simonw/b71394cc85fe0f048e376392e41586da">with
low</a>. It thought for 0.07 seconds and then output this (at 39 tokens
a second):
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/gpt-20-low.png" alt="" style="max-width: 100%;" />
</p>
<p>
Except‚Ä¶ it output invalid SVG. One of the path elements looked like
this:
</p>
<pre><code>&lt;!-- Frame --&gt;
&lt;path d="
    M150,250          &lt;!-- rear wheel center --&gt;
    L300,120          &lt;!-- top tube to front --&gt;
    L450,250          &lt;!-- chain stays back to front --&gt;
    L300,350          &lt;!-- seat stays down --&gt;
    Z"
    fill="#e0e0e0" stroke="#555" stroke-width="4"/&gt;
</code></pre>
<p>
But you can‚Äôt put comments inside attributes like that. I fixed this to
get the above image.
</p>
<h4 id="pelican-on-reasoning-medium">
Pelican on reasoning=medium
</h4>
<p>
I tried again
<a href="https://gist.github.com/simonw/642e9e371387fc59a5aad25dcd41b0cc">on
medium</a>. This time it thought for 4.44 seconds and gave me this, at
55 tokens/second:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/gpt-20-medium.png" alt="" style="max-width: 100%;" />
</p>
<h4 id="pelican-on-reasoning-high">
Pelican on reasoning=high
</h4>
<p>
And then
<a href="https://gist.github.com/simonw/164a3f1b4b1a971637af279a0ac49be3">on
high</a>. My first attempt here failed because it hit the default LM
Studio 4096 context limit - so I reloaded the model and set the new
limit to the maximum possible.
</p>
<p>
It jumped up to using 17.35GB RAM (I imagine it might use a lot more
with a longer prompt) and after thinking for 5 minutes and 50 seconds
(!) gave me this:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/gpt-20-high.png" alt="" style="max-width: 100%;" />
</p>
<p>
It thought <em>really hard</em> to get there. Here‚Äôs a snippet of
<a href="https://gist.github.com/simonw/164a3f1b4b1a971637af279a0ac49be3 ">that
transcript</a>:
</p>
<blockquote>
<p>
But maybe we want seat below body bottom? Actually seat should be
between body and pedals, not above body bottom. Wait, typical bicycle:
seat post goes from bottom bracket upward to the seat. The seat sits on
top of the seat tube (vertical). So the seat is above bottom bracket. In
our representation, the seat is at y=250-255. Body center at 245, body
bottom at 265. So body bottom is below seat. That means the pelican‚Äôs
body is below the seat? Actually no: body bottom extends to 265; seat
top is at 250; so seat sits above body bottom by about 15 units. That
seems plausible because seat should be slightly above body, but body
bottom is at 265 &gt; seat top 255. [‚Ä¶]
</p>
</blockquote>
<p>
I‚Äôve seen this from other local reasoning models too: they can really
overthink this kind of problem! I don‚Äôt know if there‚Äôs any pelican SVG
that‚Äôs worth waiting nearly six minutes for.
</p>
<h4 id="space-invaders-with-gpt-oss-20b">
Space invaders with gpt-oss-20b
</h4>
<p>
Given how long high took I switched back to medium for my next
experiment:
</p>
<blockquote>
<p>
<code>Write an HTML and JavaScript page implementing space
invaders</code>
</p>
</blockquote>
<p>
It
<a href="https://gist.github.com/simonw/63d7d8c43ae2ac93c214325bd6d607e4">thought
for 10.78 seconds</a> and produced this:
</p>
<div style="max-width: 100%; margin-bottom: 0.4em">
<pre><code>&lt;video controls=&quot;controls&quot; preload=&quot;none&quot; aria-label=&quot;Space Invaders&quot; poster=&quot;https://static.simonwillison.net/static/2025/space-invaders-gpt-20.jpg&quot; loop=&quot;loop&quot; style=&quot;width: 100%; height: auto;&quot; muted=&quot;muted&quot;&gt;
    &lt;source src=&quot;https://static.simonwillison.net/static/2025/space-invaders-gpt-20.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;</code></pre>
</div>
<p>
You can
<a href="https://tools.simonwillison.net/space-invaders-gpt-oss-20b-mxfp4-medium">play
that here</a>.
</p>
<p>
It‚Äôs not the best I‚Äôve seen - I was more impressed
<a href="https://simonwillison.net/2025/Jul/29/space-invaders/">by GLM
4.5 Air</a> - but it‚Äôs very competent for a model that only uses 12GB of
my RAM (GLM 4.5 Air used 47GB).
</p>
<h4 id="trying-gpt-oss-120b-via-api-providers">
Trying gpt-oss-120b via API providers
</h4>
<p>
I don‚Äôt quite have the resources on my laptop to run the larger model.
Thankfully it‚Äôs already being hosted by a number of different API
providers.
</p>
<p>
OpenRouter already
<a href="https://openrouter.ai/openai/gpt-oss-120b/providers">lists
three</a> - Fireworks, Groq and Cerebras. (Update: now also Parasail and
Baseten.)
</p>
<p>
Cerebras is <em>fast</em>, so I decided to try them first.
</p>
<p>
I installed the
<a href="https://github.com/irthomasthomas/llm-cerebras">llm-cerebras</a>
plugin and ran the <code>refresh</code> command to ensure it had their
latest models:
</p>
<div class="highlight highlight-source-shell">
<pre>llm install -U llm-cerebras jsonschema
llm cerebras refresh</pre>
</div>
<p>
(Installing jsonschema worked around a warning message.)
</p>
<p>
Output:
</p>
<pre><code>Refreshed 10 Cerebras models:
  - cerebras-deepseek-r1-distill-llama-70b
  - cerebras-gpt-oss-120b
  - cerebras-llama-3.3-70b
  - cerebras-llama-4-maverick-17b-128e-instruct
  - cerebras-llama-4-scout-17b-16e-instruct
  - cerebras-llama3.1-8b
  - cerebras-qwen-3-235b-a22b-instruct-2507
  - cerebras-qwen-3-235b-a22b-thinking-2507
  - cerebras-qwen-3-32b
  - cerebras-qwen-3-coder-480b
</code></pre>
<p>
Now:
</p>
<div class="highlight highlight-source-shell">
<pre>llm -m cerebras-gpt-oss-120b \
  <span class="pl-s"><span class="pl-pds">'</span>Generate an SVG of a pelican riding a bicycle<span class="pl-pds">'</span></span></pre>
</div>
<p>
Cerebras runs the new model at between 2 and 4 thousands tokens per
second!
</p>
<p>
To my surprise this one
<a href="https://gist.github.com/simonw/4c685f19f1a93b68eacb627125e36be4">had
the same comments-in-attributes bug</a> that we saw with oss-20b
earlier. I fixed those and got this pelican:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/gpt-120-cerebras.jpg" alt="Yellow and not great pelican, quite a good bicycle if a bit sketchy." style="max-width: 100%;" />
</p>
<p>
That bug appears intermittently - I‚Äôve not seen it on some of my other
runs of the same prompt.
</p>
<p>
The
<a href="https://github.com/simonw/llm-openrouter">llm-openrouter</a>
plugin also provides access to the models, balanced across the
underlying providers. You can use that like so:
</p>
<div class="highlight highlight-source-shell">
<pre>llm install llm-openrouter
llm keys <span class="pl-c1">set</span> openrouter
<span class="pl-c"><span class="pl-c">#</span> Paste API key here</span>
llm -m openrouter/openai/gpt-oss-120b <span class="pl-s"><span class="pl-pds">"</span>Say hi<span class="pl-pds">"</span></span></pre>
</div>
<h4 id="llama-cpp-is-coming-very-shortly">
llama.cpp is coming very shortly
</h4>
<p>
The <code>llama.cpp</code>
<a href="https://github.com/ggml-org/llama.cpp/pull/15091">pull request
for gpt-oss</a> was landed less than an hour ago. It‚Äôs worth browsing
through the coded - a <em>lot</em> of work went into supporting this new
model, spanning 48 commits to 83 different files. Hopefully this will
land in the
<a href="https://formulae.brew.sh/formula/llama.cpp">llama.cpp Homebrew
package</a> within the next day or so, which should provide a convenient
way to run the model via <code>llama-server</code> and friends.
</p>
<h4 id="gpt-oss-20b-in-ollama">
gpt-oss:20b in Ollama
</h4>
<p>
Ollama <a href="https://ollama.com/library/gpt-oss">also have
gpt-oss</a>, requiring an update to their app.
</p>
<p>
I fetched that 14GB model like this:
</p>
<div class="highlight highlight-source-shell">
<pre>ollama pull gpt-oss:20b</pre>
</div>
<p>
Now I can use it with the new Ollama native app, or access it from
<a href="https://llm.datasette.io/">LLM</a> like this:
</p>
<div class="highlight highlight-source-shell">
<pre>llm install llm-ollama
llm -m gpt-oss:20b <span class="pl-s"><span class="pl-pds">'</span>Hi<span class="pl-pds">'</span></span></pre>
</div>
<p>
This also appears to use around 13.26GB of system memory while running a
prompt.
</p>
<p>
Ollama also launched <a href="https://ollama.com/turbo">Ollama Turbo</a>
today, offering the two OpenAI models as a paid hosted service:
</p>
<blockquote>
<p>
Turbo is a new way to run open models using datacenter-grade hardware.
Many new models are too large to fit on widely available GPUs, or run
very slowly. Ollama Turbo provides a way to run these models fast while
using Ollama‚Äôs App, CLI, and API.
</p>
</blockquote>
<h4 id="the-model-card">
Training details from the model card
</h4>
<p>
Here are some interesting notes about how the models were trained from
<a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">the
model card</a> (PDF):
</p>
<blockquote>
<p>
<strong>Data</strong>: We train the models on a text-only dataset with
trillions of tokens, with a focus on STEM, coding, and general
knowledge. To improve the safety of the model, we filtered the data for
harmful content in pre-training, especially around hazardous biosecurity
knowledge, by reusing the CBRN pre-training filters from GPT-4o. Our
model has a knowledge cutoff of June 2024.
</p>
<p>
<strong>Training</strong>: The gpt-oss models trained on NVIDIA H100
GPUs using the PyTorch framework with expert-optimized Triton kernels.
The training run for gpt-oss-120b required 2.1 million H100-hours to
complete, with gpt-oss-20b needing almost 10x fewer. [‚Ä¶]
</p>
</blockquote>
<p>
Thunder Compute‚Äôs article
<a href="https://www.thundercompute.com/blog/nvidia-h100-pricing">NVIDIA
H100 Pricing (August 2025): Cheapest On-Demand Cloud GPU Rates</a> lists
prices from around $2/hour to $11/hour, which would indicate a training
cost of the 120b model between $4.2m and $23.1m and the 20b between
$420,000 and $2.3m.
</p>
<blockquote>
<p>
After pre-training, we post-train the models using similar CoT RL
techniques as OpenAI o3. This procedure teaches the models how to reason
and solve problems using CoT and teaches the model how to use tools.
Because of the similar RL techniques, these models have a personality
similar to models served in our first-party products like ChatGPT. Our
training dataset consists of a wide range of problems from coding, math,
science, and more.
</p>
</blockquote>
<p>
The models have additional special training to help them use web browser
and Python (Jupyter notebook) tools more effectively:
</p>
<blockquote>
<p>
During post-training, we also teach the models to use different agentic
tools:
</p>
<ul>
<li>
A browsing tool, that allows the model to call search and open functions
to interact with the web. This aids factuality and allows the models to
fetch info beyond their knowledge cutoff.
</li>
<li>
A python tool, which allows the model to run code in a stateful Jupyter
notebook environment.
</li>
<li>
Arbitrary developer functions, where one can specify function schemas in
a <code>Developer</code> message similar to the OpenAI API. The
definition of function is done within our harmony format.
</li>
</ul>
</blockquote>
<p>
There‚Äôs a corresponding
<a href="https://github.com/openai/gpt-oss?tab=readme-ov-file#python">section
about Python tool usage</a> in the <code>openai/gpt-oss</code>
repository README.
</p>
<h4 id="openai-harmony-a-new-format-for-prompt-templates">
OpenAI Harmony, a new format for prompt templates
</h4>
<p>
One of the gnarliest parts of implementing harnesses for LLMs is
handling the prompt template format.
</p>
<p>
Modern prompts are complicated beasts. They need to model user v.s.
assistant conversation turns, and tool calls, and reasoning traces and
an increasing number of other complex patterns.
</p>
<p>
<a href="https://github.com/openai/harmony">openai/harmony</a> is a
brand new open source project from OpenAI (again, Apache 2) which
implements a new response format that was created for the
<code>gpt-oss</code> models. It‚Äôs clearly inspired by their new-ish
<a href="https://openai.com/index/new-tools-for-building-agents/">Responses
API</a>.
</p>
<p>
The format is described in the new
<a href="https://cookbook.openai.com/articles/openai-harmony">OpenAI
Harmony Response Format</a> cookbook document. It introduces some
concepts that I‚Äôve not seen in open weight models before:
</p>
<ul>
<li>
<code>system</code>, <code>developer</code>, <code>user</code>,
<code>assistant</code> and <code>tool</code> roles - many other models
only use user and assistant, and sometimes system and tool.
</li>
<li>
Three different channels for output: <code>final</code>,
<code>analysis</code> and <code>commentary</code>. Only the
<code>final</code> channel is default intended to be visible to users.
<code>analysis</code> is for chain of thought and
<code>commentary</code> is sometimes used for tools.
</li>
</ul>
<p>
That channels concept has been present in ChatGPT for a few months,
starting with the release of o3.
</p>
<p>
The details of the new tokens used by Harmony caught my eye:
</p>
<center>
<table>
<tbody>
<tr>
<th>
Token
</th>
<th>
Purpose
</th>
<th>
ID
</th>
</tr>
<tr>
<td>
&lt;|start|&gt;
</td>
<td>
Start of message header
</td>
<td>
200006
</td>
</tr>
<tr>
<td>
&lt;|end|&gt;
</td>
<td>
End of message
</td>
<td>
200007
</td>
</tr>
<tr>
<td>
&lt;|message|&gt;
</td>
<td>
Start of message content
</td>
<td>
200008
</td>
</tr>
<tr>
<td>
&lt;|channel|&gt;
</td>
<td>
Start of channel info
</td>
<td>
200005
</td>
</tr>
<tr>
<td>
&lt;|constrain|&gt;
</td>
<td>
Data type for tool call
</td>
<td>
200003
</td>
</tr>
<tr>
<td>
&lt;|return|&gt;
</td>
<td>
Stop after response
</td>
<td>
200002
</td>
</tr>
<tr>
<td>
&lt;|call|&gt;
</td>
<td>
Call a tool
</td>
<td>
200012
</td>
</tr>
</tbody>
</table>
</center>
<p>
Those token IDs are particularly important. They are part of a new token
vocabulary called <code>o200k_harmony</code>, which landed in OpenAI‚Äôs
tiktoken tokenizer library
<a href="https://github.com/openai/tiktoken/commit/3591ff175d6a80efbe4fcc7f0e219ddd4b8c52f1">this
morning</a>.
</p>
<p>
In the past I‚Äôve seen models get confused by special tokens - try
pasting <code>&lt;|end|&gt;</code> into a model and see what happens.
</p>
<p>
Having these special instruction tokens formally map to dedicated token
IDs should hopefully be a whole lot more robust!
</p>
<p>
The Harmony repo itself includes a Rust library and a Python library
(wrapping that Rust library) for working with the new format in a much
more ergonomic way.
</p>
<p>
I tried one of their demos using <code>uv run</code> to turn it into a
shell one-liner:
</p>
<div class="highlight highlight-source-shell">
<pre>uv run --python 3.12 --with openai-harmony python -c <span class="pl-s"><span class="pl-pds">'</span></span>
<span class="pl-s">from openai_harmony import *</span>
<span class="pl-s">from openai_harmony import DeveloperContent</span>
<span class="pl-s">enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)</span>
<span class="pl-s">convo = Conversation.from_messages([</span>
<span class="pl-s">    Message.from_role_and_content(</span>
<span class="pl-s">        Role.SYSTEM,</span>
<span class="pl-s">        SystemContent.new(),</span>
<span class="pl-s">    ),</span>
<span class="pl-s">    Message.from_role_and_content(</span>
<span class="pl-s">        Role.DEVELOPER,</span>
<span class="pl-s">        DeveloperContent.new().with_instructions("Talk like a pirate!")</span>
<span class="pl-s">    ),</span>
<span class="pl-s">    Message.from_role_and_content(Role.USER, "Arrr, how be you?"),</span>
<span class="pl-s">])</span>
<span class="pl-s">tokens = enc.render_conversation_for_completion(convo, Role.ASSISTANT)</span>
<span class="pl-s">print(tokens)<span class="pl-pds">'</span></span></pre>
</div>
<p>
Which outputs:
</p>
<blockquote>
<p>
<code>[200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410,
6439, 2359, 22203, 656, 7788, 17527, 558, 87447, 100594, 25, 220, 1323,
19, 12, 3218, 279, 30377, 289, 25, 14093, 279, 2, 13888, 18403, 25,
8450, 11, 49159, 11, 1721, 13, 21030, 2804, 413, 7360, 395, 1753, 3176,
13, 200007, 200006, 77944, 200008, 2, 68406, 279, 37992, 1299, 261,
96063, 0, 200007, 200006, 1428, 200008, 8977, 81, 11, 1495, 413, 481,
30, 200007, 200006, 173781]</code>
</p>
</blockquote>
<p>
Note those token IDs like <code>200006</code> corresponding to the
special tokens listed above.
</p>
<h4 id="the-open-question-for-me-how-good-is-tool-calling-">
The open question for me: how good is tool calling?
</h4>
<p>
There‚Äôs one aspect of these models that I haven‚Äôt explored in detail
yet: <strong>tool calling</strong>. How these work is clearly a big part
of the new Harmony format, but the packages I‚Äôm using myself (around my
own <a href="https://simonwillison.net/2025/May/27/llm-tools/">LLM tool
calling</a> support) need various tweaks and fixes to start working with
that new mechanism.
</p>
<p>
Tool calling currently represents my biggest disappointment with local
models that I‚Äôve run on my own machine. I‚Äôve been able to get them to
perform simple single calls, but the state of the art these days is
wildly more ambitious than that.
</p>
<p>
Systems like Claude Code can make dozens if not hundreds of tool calls
over the course of a single session, each one adding more context and
information to a single conversation with an underlying model.
</p>
<p>
My experience to date has been that local models are unable to handle
these lengthy conversations. I‚Äôm not sure if that‚Äôs inherent to the
limitations of my own machine, or if it‚Äôs something that the right model
architecture and training could overcome.
</p>
<p>
OpenAI make big claims about the tool calling capabilities of these new
models. I‚Äôm looking forward to seeing how well they perform in practice.
</p>
<h4 id="china">
Competing with the Chinese open models
</h4>
<p>
I‚Äôve been writing a <em>lot</em> about the
<a href="https://simonwillison.net/tags/ai-in-china/">flurry of
excellent open weight models</a> released by Chinese AI labs over the
past few months - all of them very capable and most of them under Apache
2 or MIT licenses.
</p>
<p>
Just last week
<a href="https://simonwillison.net/2025/Jul/30/chinese-models/">I
said</a>:
</p>
<blockquote>
<p>
Something that has become undeniable this month is that the best
available open weight models now come from the Chinese AI labs.
</p>
<p>
I continue to have a lot of love for Mistral, Gemma and Llama but my
feeling is that Qwen, Moonshot and Z.ai have positively smoked them over
the course of July. [‚Ä¶]
</p>
<p>
I can‚Äôt help but wonder if part of the reason for the delay in release
of OpenAI‚Äôs open weights model comes from a desire to be notably better
than this truly impressive lineup of Chinese models.
</p>
</blockquote>
<p>
With the release of the gpt-oss models that statement no longer holds
true. I‚Äôm waiting for the dust to settle and the independent benchmarks
(that are more credible than my ridiculous pelicans) to roll out, but I
think it‚Äôs likely that OpenAI now offer the best available open weights
models.
</p>
<p>
<strong>Update</strong>: Independent evaluations are beginning to roll
in. Here‚Äôs
<a href="https://x.com/artificialanlys/status/1952887733803991070">Artificial
Analysis</a>:
</p>
<blockquote>
<p>
gpt-oss-120b is the most intelligent American open weights model, comes
behind DeepSeek R1 and Qwen3 235B in intelligence but offers efficiency
benefits [‚Ä¶]
</p>
<p>
While the larger gpt-oss-120b does not come in above DeepSeek R1 0528‚Äôs
score of 59 or Qwen3 235B 2507s score of 64, it is notable that it is
significantly smaller in both total and active parameters than both of
those models.
</p>
</blockquote>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/open-source&quot;&gt;open-source&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/local-llms&quot;&gt;local-llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm&quot;&gt;llm&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-tool-use&quot;&gt;llm-tool-use&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/cerebras&quot;&gt;cerebras&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ollama&quot;&gt;ollama&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/pelican-riding-a-bicycle&quot;&gt;pelican-riding-a-bicycle&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-reasoning&quot;&gt;llm-reasoning&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-release&quot;&gt;llm-release&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/lm-studio&quot;&gt;lm-studio&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/space-invaders&quot;&gt;space-invaders&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gpt-oss&quot;&gt;gpt-oss&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/5/gpt-oss/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/5/gpt-oss/#atom-everything</a></p>
<hr />
<h2 id="the-icebergs-visit-greenland">The Icebergs Visit Greenland</h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: One Foot Tsunami</p>
<p><br></p>
<p><a
href="https://onefoottsunami.com/2025/08/05/the-icebergs-visit-greenland/"
class="uri">https://onefoottsunami.com/2025/08/05/the-icebergs-visit-greenland/</a></p>
<hr />
<h2 id="the-ai-economy-is-full-of-financial-gimmicks.">The AI economy is
full of financial gimmicks.</h2>
<p>date: 2025-08-05, from: Dave Karpf‚Äôs blog</p>
<p>Tech journalists need to start moonlighting as finance
journalists.</p>
<p><br></p>
<p><a
href="https://davekarpf.substack.com/p/the-ai-economy-is-full-of-financial"
class="uri">https://davekarpf.substack.com/p/the-ai-economy-is-full-of-financial</a></p>
<hr />
<h2 id="academic-independence-eroding">Academic Independence
Eroding</h2>
<p>date: 2025-08-05, from: Guy Kawasaki blog</p>
<p>Brendan Cantwell, Associate Professor of Higher, Adult, and Lifelong
Education, Michigan State University.</p>
<p><br></p>
<p><a
href="https://guykawasaki.substack.com/p/academic-independence-eroding"
class="uri">https://guykawasaki.substack.com/p/academic-independence-eroding</a></p>
<hr />
<h2 id="pcie-8.0-to-be-up-to-16-times-faster-than-pcie-4.0">PCIe 8.0 to
be up to 16 times faster than PCIe 4.0</h2>
<p>date: 2025-08-05, from: Liliputing</p>
<p>
PCI-SIG has announced plans to release the PCIe 8.0 specification to
members by 2028, offering support for data transfer speeds up to 256
GT/s in terms of raw bit rates, and up to 1TB/s of bi-directional speed
when used in a x16 configuration. Given how long it takes for new PCIe
standards to roll out, [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/pcie-8-0-to-be-up-to-16-times-faster-than-pcie-4-0/">PCIe
8.0 to be up to 16 times faster than PCIe 4.0</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/pcie-8-0-to-be-up-to-16-times-faster-than-pcie-4-0/"
class="uri">https://liliputing.com/pcie-8-0-to-be-up-to-16-times-faster-than-pcie-4-0/</a></p>
<hr />
<h2 id="macos-tahoe-26-developer-beta-5">macOS Tahoe 26 Developer Beta
5</h2>
<p>date: 2025-08-05, from: Michael Tsai</p>
<p>Juli Clover (Mr.¬†Macintosh): Apple today provided developers with the
fifth beta of macOS Tahoe 26 for testing purposes, with the update
coming two weeks after the fourth beta. There are no updates to the
release notes, which still say Beta 4. Mario Guzm√°n: THIS IS THE NEW
MACINTOSH HD ICON?! WTF Previously: macOS Tahoe [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/05/macos-tahoe-26-developer-beta-5/"
class="uri">https://mjtsai.com/blog/2025/08/05/macos-tahoe-26-developer-beta-5/</a></p>
<hr />
<h2 id="apple-the-first-50-years-forthcoming">Apple: The First 50 Years
(Forthcoming)</h2>
<p>date: 2025-08-05, from: Michael Tsai</p>
<p>David Pogue (tweet): In time for Apple‚Äôs 50th anniversary, ‚ÄúCBS
Sunday Morning‚Äù correspondent David Pogue tells the iconic company‚Äôs
entire life story: how it was born, nearly died, was born again under
Steve Jobs, and became, under CEO Tim Cook, one of the most valuable
companies in the world. The 600-page book features 360 full-color
[‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/05/apple-the-first-50-years-forthcoming/"
class="uri">https://mjtsai.com/blog/2025/08/05/apple-the-first-50-years-forthcoming/</a></p>
<hr />
<h2 id="swiftui-documentgroups-are-terribly-limited">SwiftUI
DocumentGroups Are Terribly Limited</h2>
<p>date: 2025-08-05, from: Michael Tsai</p>
<p>Christian Tietze: This is how little you need to get
started[‚Ä¶][‚Ä¶]What the system does is provide a launch scene for you when
you only declare a DocumentGroup in your SwiftUI.App.body. You can
customize this by making the launch scene yourself. WWDC24 ‚ÄúEvolve Your
Document Launch Experience‚Äù contains examples that at least offer to
style what‚Äôs [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/05/swiftui-documentgroups-are-terribly-limited/"
class="uri">https://mjtsai.com/blog/2025/08/05/swiftui-documentgroups-are-terribly-limited/</a></p>
<hr />
<h2 id="google-loses-appeal-against-epic">Google Loses Appeal Against
Epic</h2>
<p>date: 2025-08-05, from: Michael Tsai</p>
<p>Mike Scarcella (MacRumors, Slashdot): The San Francisco-based 9th
U.S. Circuit Court of Appeals, in a unanimous ruling, rejected, claims
from Google that the trial judge made legal errors in the antitrust case
that unfairly benefited ‚ÄúFortnite‚Äù maker Epic Games, which filed the
lawsuit in 2020.[‚Ä¶]U.S. District Judge James Donato in San Francisco
ordered Google in [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/05/google-loses-appeal-against-epic/"
class="uri">https://mjtsai.com/blog/2025/08/05/google-loses-appeal-against-epic/</a></p>
<hr />
<h2
id="amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs">AMD
Ryzen 7 H 255 is a mid-range Hawk Point chip for laptops &amp; mini
PCs</h2>
<p>date: 2025-08-05, from: Liliputing</p>
<p>
Over the past few days I‚Äôve spotted a bunch of mini PCs from Chinese
brands that are using a new processor based on slightly older
technology. The¬†AMD Ryzen 7 H 255¬†processor is an 8-core, 16-thread chip
that‚Äôs made for the Chinese market, but which is showing up in mini PCs
shipped to customers globally. The [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs/">AMD
Ryzen 7 H 255 is a mid-range Hawk Point chip for laptops &amp; mini
PCs</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs/"
class="uri">https://liliputing.com/amd-ryzen-7-h-255-is-a-mid-range-hawk-point-chip-for-laptops-mini-pcs/</a></p>
<hr />
<h2 id="claude-opus-4.1">Claude Opus 4.1</h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://www.anthropic.com/news/claude-opus-4-1">Claude
Opus 4.1</a></strong>
</p>
Surprise new model from Anthropic today - Claude Opus 4.1, which they
describe as ‚Äúa drop-in replacement for Opus 4‚Äù.
</p>
<p>
My favorite thing about this model is the version number - treating this
as a .1 version increment looks like it‚Äôs an accurate depiction of the
model‚Äôs capabilities.
</p>
<p>
Anthropic‚Äôs own benchmarks show very small incremental gains.
</p>
<p>
Comparing Opus 4 and Opus 4.1 (I
<a href="https://claude.ai/share/c7366629-784a-4088-9fc4-15613aa41a7f">got
4.1 to extract this information from a screenshot</a> of Anthropic‚Äôs own
benchmark scores, then asked it to look up the links, then verified the
links myself and fixed a few):
</p>
<ul>
<li>
<strong>Agentic coding</strong>
(<a href="https://github.com/SWE-bench/SWE-bench">SWE-bench
Verified</a>): From 72.5% to 74.5%
</li>
<li>
<strong>Agentic terminal coding</strong>
(<a href="https://github.com/laude-institute/terminal-bench">Terminal-Bench</a>):
From 39.2% to 43.3%
</li>
<li>
<strong>Graduate-level reasoning</strong>
(<a href="https://github.com/idavidrein/gpqa">GPQA Diamond</a>): From
79.6% to 80.9%
</li>
<li>
<strong>Agentic tool use</strong>
(<a href="https://github.com/sierra-research/tau-bench">TAU-bench</a>):
</li>
<li>
Retail: From 81.4% to 82.4%
</li>
<li>
<strong>Airline: From 59.6% to 56.0%</strong> <em>(decreased)</em>
</li>
<li>
<strong>Multilingual Q&amp;A</strong>
(<a href="https://huggingface.co/datasets/openai/MMMLU">MMMLU</a>): From
88.8% to 89.5%
</li>
<li>
<strong>Visual reasoning</strong>
(<a href="https://mmmu-benchmark.github.io/">MMMU validation</a>): From
76.5% to 77.1%
</li>
<li>
<strong>High school math competition</strong>
(<a href="https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions">AIME
2025</a>): From 75.5% to 78.0%
</li>
</ul>
<p>
Likewise, the
<a href="https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf">model
card</a> shows only tiny changes to the various safety metrics that
Anthropic track.
</p>
<p>
It‚Äôs priced the same as Opus 4 - $15/million for input and $75/million
for output, making it one of
<a href="https://www.llm-prices.com/#sb=input&amp;sd=descending">the
most expensive models</a> on the market today.
</p>
<p>
I had it
<a href="https://gist.github.com/simonw/7fead138d31d751d65c7253a1c18751b">draw
me this pelican</a> riding a bicycle:
</p>
<p>
<img alt="Pelican is line art, does have a good beak and feet on the pedals, bicycle is very poorly designed and not the right shape." src="https://static.simonwillison.net/static/2025/opus-4.1-pelican.png" />
</p>
<p>
For comparison I got a fresh new pelican
<a href="https://gist.github.com/simonw/96a958e39aaed10e1e47c1aab2d05e20">out
of Opus 4</a> which I actually like a little more:
</p>
<p>
<img alt="This one has shaded colors for the different parts of the pelican. Still a bad bicycle." src="https://static.simonwillison.net/static/2025/opus-4-pelican.png" />
</p>
<p>
<p>I shipped
<a href="https://github.com/simonw/llm-anthropic/releases/tag/0.18">llm-anthropic
0.18</a> with support for the new model.</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm&quot;&gt;llm&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/anthropic&quot;&gt;anthropic&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/claude&quot;&gt;claude&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/evals&quot;&gt;evals&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-pricing&quot;&gt;llm-pricing&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/pelican-riding-a-bicycle&quot;&gt;pelican-riding-a-bicycle&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-release&quot;&gt;llm-release&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/5/claude-opus-41/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/5/claude-opus-41/#atom-everything</a></p>
<hr />
<h2
id="florida-sues-huge-porn-sites-including-xvideos-and-bang-bros-over-age-verification-law">Florida
Sues Huge Porn Sites Including XVideos and Bang Bros Over Age
Verification Law</h2>
<p>date: 2025-08-05, from: 404 Media Group</p>
<p>The lawsuit alleges XVideos, Bang Bros, XNXX, Girls Gone Wild and
TrafficFactory are in violation of Florida‚Äôs law that requires adult
platforms to verify visitors are over 18.</p>
<p><br></p>
<p><a
href="https://www.404media.co/florida-sues-huge-porn-sites-including-xvideos-and-bang-bros-over-age-verification-law/"
class="uri">https://www.404media.co/florida-sues-huge-porn-sites-including-xvideos-and-bang-bros-over-age-verification-law/</a></p>
<hr />
<h2 id="lets-do-lunch">Let‚Äôs Do Lunch!</h2>
<p>date: 2025-08-05, from: Paul Krugman</p>
<p>A recording from Paul Krugman and Jared Bernstein‚Äôs live video</p>
<audio crossorigin="anonymous" controls="controls">
<source type="audio/mpeg" src="https://api.substack.com/feed/podcast/170192625/564a4eb93450dfb4145438f847184acb.mp3">
</source>
</audio>
<p><a href="https://api.substack.com/feed/podcast/170192625/564a4eb93450dfb4145438f847184acb.mp3" target="_blank">download
audio/mpeg</a><br></p>
<p><a href="https://paulkrugman.substack.com/p/lets-do-lunch"
class="uri">https://paulkrugman.substack.com/p/lets-do-lunch</a></p>
<hr />
<h2
id="argon-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop">Argon
ONE UP hits Kickstarter for $330 and up (Raspberry Pi CM5-powered
laptop)</h2>
<p>date: 2025-08-05, from: Liliputing</p>
<p>
The¬†Argon ONE UP is a laptop with a 14 inch, 1920 x 1200 pixel IPS LCD
display, an aluminum body, backlit keyboard, and one thing that sets it
apart from most other laptops: the Argon ONE UP is powered by a
removable Raspberry Pi CM5 computer module. Argon40 has been making
Raspberry Pi accessories like [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/argone-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop/">Argon
ONE UP hits Kickstarter for $330 and up (Raspberry Pi CM5-powered
laptop)</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/argone-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop/"
class="uri">https://liliputing.com/argone-one-up-hits-kickstarter-for-330-and-up-raspberry-pi-cm5-powered-laptop/</a></p>
<hr />
<h2
id="tell-us-what-you-really-think-mr.-secretary-poison-gas-warfare-1942">Tell
Us What You Really Think Mr.¬†Secretary [Poison Gas Warfare], 1942</h2>
<p>date: 2025-08-05, from: National Archives, Text Message blog</p>
<p>In January 1942, shortly after the United States was thrust into
World War II by the December 7, 1941, Japanese attack on Pearl Harbor
and the subsequent December 11 declaration of war by Germany, officials
in the Department of State considered the issue of the U.S. attitude
toward the Geneva Protocol for the Prohibition of ‚Ä¶
<a href="https://text-message.blogs.archives.gov/2025/08/05/tell-us-what-you-really-think-mr-secretary-poison-gas-warfare-1942/" class="more-link">Continue
reading <span class="screen-reader-text">Tell Us What You Really Think
Mr.¬†Secretary [Poison Gas Warfare], 1942</span></a></p>
<p><br></p>
<p><a
href="https://text-message.blogs.archives.gov/2025/08/05/tell-us-what-you-really-think-mr-secretary-poison-gas-warfare-1942/"
class="uri">https://text-message.blogs.archives.gov/2025/08/05/tell-us-what-you-really-think-mr-secretary-poison-gas-warfare-1942/</a></p>
<hr />
<h2 id="amores-materialistas-el-deseo-en-tiempos-de-capital">Amores
materialistas: el deseo en tiempos de capital</h2>
<p>date: 2025-08-05, from: Iv√°n Paredes Res√©ndiz blog, Mexico‚Äôs
cinema</p>
<p>
Direcci√≥n:¬†Celine Song.¬† Guion:¬†Celine Song. Elenco:¬†Dakota Johnson,
Chris Evans, Pedro Pascal.¬† Pa√≠s:¬†Estados Unidos. ¬†¬† M√°s informaci√≥n de
la pel√≠cula:¬†https://www.imdb.com/title/tt30253473/¬†¬† Desde su √≥pera
prima¬†Vidas pasadas¬†(2023), Celine Song dej√≥ entrever una sensibilidad
particular: la de una cineasta¬†interesada en explorar c√≥mo fuerzas
invisibles ‚Äîel tiempo, la¬†distancia, la cultura‚Äî moldean las relaciones
humanas. Su segundo largometraje,¬†Amores¬†materialistas, no solo conserva
ese [‚Ä¶]
</p>
<p>
La entrada
<a href="https://www.palomitademaiz.net/resenas-amores-materialistas/">Amores
materialistas: el deseo en tiempos de capital</a> se public√≥ primero en
<a href="https://www.palomitademaiz.net">Palomita de ma√≠z</a>.
</p>
<p><br></p>
<p><a
href="https://www.palomitademaiz.net/resenas-amores-materialistas/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=resenas-amores-materialistas"
class="uri">https://www.palomitademaiz.net/resenas-amores-materialistas/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=resenas-amores-materialistas</a></p>
<hr />
<h2
id="ice-is-about-to-go-on-a-social-media-and-tv-ad-recruiting-blitz">ICE
Is About To Go on a Social Media and TV Ad Recruiting Blitz</h2>
<p>date: 2025-08-05, from: 404 Media Group</p>
<p>Contracting records reviewed by 404 Media show that ICE wants to
target Gen Z, including with ads on Hulu and HBO Max.</p>
<p><br></p>
<p><a
href="https://www.404media.co/ice-is-about-to-go-on-a-social-media-and-tv-ad-recruiting-blitz/"
class="uri">https://www.404media.co/ice-is-about-to-go-on-a-social-media-and-tv-ad-recruiting-blitz/</a></p>
<hr />
<h2
id="wikipedia-editors-adopt-speedy-deletion-policy-for-ai-slop-articles">Wikipedia
Editors Adopt ‚ÄòSpeedy Deletion‚Äô Policy for AI Slop Articles</h2>
<p>date: 2025-08-05, from: 404 Media Group</p>
<p>‚ÄúThe ability to quickly generate a lot of bogus content is
problematic if we don‚Äôt have a way to delete it just as quickly.‚Äù</p>
<p><br></p>
<p><a
href="https://www.404media.co/wikipedia-editors-adopt-speedy-deletion-policy-for-ai-slop-articles/"
class="uri">https://www.404media.co/wikipedia-editors-adopt-speedy-deletion-policy-for-ai-slop-articles/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Robert">@Robert</span>‚Äôs
feed at BlueSky</strong> (date: 2025-08-05, from: Robert‚Äôs feed at
BlueSky)</p>
<p>Thank you Dave. I enjoyed your work. Keep on trucking.</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/rsdoiel.bsky.social/post/3lvnwjox4w226"
class="uri">https://bsky.app/profile/rsdoiel.bsky.social/post/3lvnwjox4w226</a></p>
<hr />
<h2
id="nearly-100000-chatgpt-conversations-were-searchable-on-google">Nearly
100,000 ChatGPT Conversations Were Searchable on Google</h2>
<p>date: 2025-08-05, from: 404 Media Group</p>
<p>A researcher has scraped a much larger dataset of indexed ChatGPT
conversations, exposing contracts and intimate conversations.</p>
<p><br></p>
<p><a
href="https://www.404media.co/nearly-100-000-chatgpt-conversations-were-searchable-on-google/"
class="uri">https://www.404media.co/nearly-100-000-chatgpt-conversations-were-searchable-on-google/</a></p>
<hr />
<h2 id="what-epstein-was-afraid-of">What Epstein Was Afraid Of</h2>
<p>date: 2025-08-05, from: Tina Brown</p>
<p>I guess Ghislaine Maxwell must have given up something juicy enough
in her session with Deputy AG Todd Blanche to earn her those new relaxed
digs in the minimum-security, open-campus Bryan prison camp in
Texas.</p>
<p><br></p>
<p><a href="https://tinabrown.substack.com/p/what-epstein-was-afraid-of"
class="uri">https://tinabrown.substack.com/p/what-epstein-was-afraid-of</a></p>
<hr />
<h2 id="quoting-greyduet-on-rteachers">Quoting greyduet on
r/teachers</h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/">
<p>
I teach HS Science in the south. I can only speak for my district, but a
few teacher work days in the wave of enthusiasm I‚Äôm seeing for AI tools
is overwhelming. We‚Äôre getting district approved ads for AI tools by
email, Admin and ICs are pushing it on us, and at least half of the
teaching staff seems all in at this point.
</p>
<p>
I was just in a meeting with my team and one of the older teachers
brought out a powerpoint for our first lesson and almost everyone agreed
to use it after a quick scan - but it was missing important tested
material, repetitive, and just totally airy and meaningless. Just slide
after slide of the same handful of sentences rephrased with random
loosely related stock photos. When I asked him if it was AI generated,
he said ‚Äòof course‚Äô, like it was a strange question. [‚Ä¶]
</p>
<p>
We don‚Äôt have a leg to stand on to teach them anything about
originality, academic integrity/intellectual honesty, or the importance
of doing things for themselves when they catch us indulging in it just
to save time at work.
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/">greyduet
on r/teachers</a>, Unpopular Opinion: Teacher AI use is already out of
control and it‚Äôs not ok
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai-ethics&quot;&gt;ai-ethics&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/slop&quot;&gt;slop&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/education&quot;&gt;education&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/#atom-everything</a></p>
<hr />
<h2 id="the-paranoid-style-in-american-economics">The Paranoid Style in
American Economics</h2>
<p>date: 2025-08-05, from: Paul Krugman</p>
<p>Remember, every accusation is a confession</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/the-paranoid-style-in-american-economics"
class="uri">https://paulkrugman.substack.com/p/the-paranoid-style-in-american-economics</a></p>
<hr />
<h2
id="section"><default:div xmlns="http://www.w3.org/1999/xhtml" class="if-your-feed-reader-displays-this-then-it-is-violating-the-Atom-spec-RFC-4287-section-4.2.14"/></h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: Tantek √áelik‚Äôs blog</p>
<p><br></p>
<p><a href="https://tantek.com/2025/216/t1/finished-skyline50k-ultra"
class="uri">https://tantek.com/2025/216/t1/finished-skyline50k-ultra</a></p>
<hr />
<h2 id="a-friendly-introduction-to-svg">A Friendly Introduction to
SVG</h2>
<p>date: 2025-08-05, updated: 2025-08-05, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://www.joshwcomeau.com/svg/friendly-introduction-to-svg/">A
Friendly Introduction to SVG</a></strong>
</p>
This SVG tutorial by Josh Comeau is fantastic. It‚Äôs filled with neat
interactive illustrations - with a pleasing subtly ‚Äúclick‚Äù audio effect
as you adjust their sliders - and provides a useful introduction to a
bunch of well chosen SVG fundamentals.
</p>
<p>
<p>I finally understand what all four numbers in the
<code>viewport=‚Äú‚Ä¶‚Äù</code> attribute are for!</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://lobste.rs/s/ome2lo/friendly_introduction_svg&quot;&gt;Lobste.rs&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/svg&quot;&gt;svg&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/explorables&quot;&gt;explorables&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/josh-comeau&quot;&gt;josh-comeau&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/5/a-friendly-introduction-to-svg/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/5/a-friendly-introduction-to-svg/#atom-everything</a></p>
<hr />
<h2 id="a-treatise-on-ai-chatbots-undermining-the-enlightenment">A
Treatise on AI Chatbots Undermining the Enlightenment</h2>
<p>date: 2025-08-05, from: Maggie Appleton blog</p>
<p>On chatbot sycophancy, passivity, and the case for more
intellectually challenging companions</p>
<p><br></p>
<p><a href="https://maggieappleton.com/ai-enlightenment/"
class="uri">https://maggieappleton.com/ai-enlightenment/</a></p>
<hr />
<h2 id="chatgpt-agents-user-agent">ChatGPT agent‚Äôs user-agent</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<p>
I was exploring how ChatGPT agent works today. I learned some
interesting things about how it exposes its identity through HTTP
headers, then made a huge blunder in thinking it was leaking its URLs to
Bingbot and Yandex‚Ä¶ but it turned out
<a href="https://simonwillison.net/2025/Aug/4/chatgpt-agents-agent/#cloudflare-crawler-hints">that
was a Cloudflare feature</a> that had nothing to do with ChatGPT.
</p>
<p>
ChatGPT agent is the
<a href="https://openai.com/index/introducing-chatgpt-agent/">recently
released</a> (and confusingly named) ChatGPT feature that provides
browser automation combined with terminal access as a feature of ChatGPT
- replacing their previous
<a href="https://help.openai.com/en/articles/10421097-operator">Operator
research preview</a> which is scheduled for deprecation on August 31st.
</p>
<h4 id="investigating-chatgpt-agent-s-user-agent">
Investigating ChatGPT agent‚Äôs user-agent
</h4>
<p>
I decided to dig into how it works by creating a logged web URL endpoint
using
<a href="https://simonwillison.net/2024/Aug/8/django-http-debug/">django-http-debug</a>.
Then I told ChatGPT agent mode to explore that new page:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/chatgpt-agent-url.jpg" alt="ChatGPT screenshot. My prompt was &quot;Visit https://simonwillison.net/test-url-context and tell me what you see there&quot; - it said &quot;Worked for 15 seconds&quot; with an arrow, then a screnshot of the webpage content showing &quot;simonwillison.net&quot; with a favicon, heading &quot;This is a heading&quot;, text &quot;Text and text and more text.&quot; and &quot;this came from javascript&quot;. The bot then responds with: The webpage displays a simple layout with a large heading at the top that reads ‚ÄúThis is a heading.‚Äù Below it, there's a short paragraph that says ‚ÄúText and text and more text.‚Äù A final line appears underneath saying ‚Äúthis came from javascript,‚Äù indicating that this last line was inserted via a script. The page contains no interactive elements or instructions‚Äîjust these lines of plain text displayed on a white background." style="max-width: 100%;" />
</p>
<p>
My logging captured these request headers:
</p>
<pre><code>Via: 1.1 heroku-router
Host: simonwillison.net
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Cf-Ray: 96a0f289adcb8e8e-SEA
Cookie: cf_clearance=zzV8W...
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Priority: u=0, i
Sec-Ch-Ua: "Not)A;Brand";v="8", "Chromium";v="138"
Signature: sig1=:1AxfqHocTf693inKKMQ7NRoHoWAZ9d/vY4D/FO0+MqdFBy0HEH3ZIRv1c3hyiTrzCvquqDC8eYl1ojcPYOSpCQ==:
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36
Cf-Ipcountry: US
X-Request-Id: 45ef5be4-ead3-99d5-f018-13c4a55864d3
Sec-Fetch-Dest: document
Sec-Fetch-Mode: navigate
Sec-Fetch-Site: none
Sec-Fetch-User: ?1
Accept-Encoding: gzip, br
Accept-Language: en-US,en;q=0.9
Signature-Agent: "https://chatgpt.com"
Signature-Input: sig1=("@authority" "@method" "@path" "signature-agent");created=1754340838;keyid="otMqcjr17mGyruktGvJU8oojQTSMHlVm7uO-lrcqbdg";expires=1754344438;nonce="_8jbGwfLcgt_vUeiZQdWvfyIeh9FmlthEXElL-O2Rq5zydBYWivw4R3sV9PV-zGwZ2OEGr3T2Pmeo2NzmboMeQ";tag="web-bot-auth";alg="ed25519"
X-Forwarded-For: 2a09:bac5:665f:1541::21e:154, 172.71.147.183
X-Request-Start: 1754340840059
Cf-Connecting-Ip: 2a09:bac5:665f:1541::21e:154
Sec-Ch-Ua-Mobile: ?0
X-Forwarded-Port: 80
X-Forwarded-Proto: http
Sec-Ch-Ua-Platform: "Linux"
Upgrade-Insecure-Requests: 1
</code></pre>
<p>
That <strong>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0
Safari/537.36</strong> user-agent header is the one used by the most
recent Chrome on macOS - which is a little odd here as the
<strong>Sec-Ch-Ua-Platform : ‚ÄúLinux‚Äù</strong> indicates that the agent
browser runs on Linux.
</p>
<p>
At first glance it looks like ChatGPT is being dishonest here by not
including its bot identity in the user-agent header. I thought for a
moment it might be reflecting my own user-agent, but I‚Äôm using Firefox
on macOS and it identified itself as Chrome.
</p>
<p>
Then I spotted this header:
</p>
<pre><code>Signature-Agent: "https://chatgpt.com"
</code></pre>
<p>
Which is accompanied by a much more complex header called
<strong>Signature-Input</strong>:
</p>
<pre><code>Signature-Input: sig1=("@authority" "@method" "@path" "signature-agent");created=1754340838;keyid="otMqcjr17mGyruktGvJU8oojQTSMHlVm7uO-lrcqbdg";expires=1754344438;nonce="_8jbGwfLcgt_vUeiZQdWvfyIeh9FmlthEXElL-O2Rq5zydBYWivw4R3sV9PV-zGwZ2OEGr3T2Pmeo2NzmboMeQ";tag="web-bot-auth";alg="ed25519"
</code></pre>
<p>
And a <code>Signature</code> header too.
</p>
<p>
These turn out to come from a relatively new web standard:
<a href="https://www.rfc-editor.org/rfc/rfc9421.html">RFC 9421 HTTP
Message Signatures</a>‚Äô published February 2024.
</p>
<p>
The purpose of HTTP Message Signatures is to allow clients to include
signed data about their request in a way that cannot be tampered with by
intermediaries. The signature uses a public key that‚Äôs provided by the
following well-known endpoint:
</p>
<pre><code>https://chatgpt.com/.well-known/http-message-signatures-directory
</code></pre>
<p>
Add it all together and we now have a rock-solid way to identify traffic
from ChatGPT agent: look for the <code>Signature-Agent:
‚Äúhttps://chatgpt.com‚Äù</code> header and confirm its value by checking
the signature in the <code>Signature-Input</code> and
<code>Signature</code> headers.
</p>
<h4 id="and-then-came-the-crawlers">
And then came Bingbot and Yandex
</h4>
<p>
Just over a minute after it captured that request, my logging endpoint
got another request:
</p>
<pre><code>Via: 1.1 heroku-router
From: bingbot(at)microsoft.com
Host: simonwillison.net
Accept: */*
Cf-Ray: 96a0f4671d1fc3c6-SEA
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36
Cf-Ipcountry: US
X-Request-Id: 6214f5dc-a4ea-5390-1beb-f2d26eac5d01
Accept-Encoding: gzip, br
X-Forwarded-For: 207.46.13.9, 172.71.150.252
X-Request-Start: 1754340916429
Cf-Connecting-Ip: 207.46.13.9
X-Forwarded-Port: 80
X-Forwarded-Proto: http
</code></pre>
<p>
I pasted <code>207.46.13.9</code> into Microsoft‚Äôs
<a href="https://www.bing.com/toolbox/verify-bingbot-verdict">Verify
Bingbot</a> tool (after solving a particularly taxing CAPTCHA) and it
confirmed that this was indeed a request from Bingbot.
</p>
<p>
I set up a second URL to confirm‚Ä¶ and this time got a visit from Yandex!
</p>
<pre><code>Via: 1.1 heroku-router
From: support@search.yandex.ru
Host: simonwillison.net
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Cf-Ray: 96a16390d8f6f3a7-DME
Server: Heroku
Cdn-Loop: cloudflare; loops=1
Cf-Visitor: {"scheme":"https"}
User-Agent: Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)
Cf-Ipcountry: RU
X-Request-Id: 3cdcbdba-f629-0d29-b453-61644da43c6c
Accept-Encoding: gzip, br
X-Forwarded-For: 213.180.203.138, 172.71.184.65
X-Request-Start: 1754345469921
Cf-Connecting-Ip: 213.180.203.138
X-Forwarded-Port: 80
X-Forwarded-Proto: http
</code></pre>
<p>
Yandex
<a href="https://yandex.com/support/webmaster/en/robot-workings/check-yandex-robots.html?lang=en">suggest
a reverse DNS lookup</a> to verify, so I ran this command:
</p>
<pre><code>dig -x 213.180.203.138 +short
</code></pre>
<p>
And got back:
</p>
<pre><code>213-180-203-138.spider.yandex.com.
</code></pre>
<p>
Which confirms that this is indeed a Yandex crawler.
</p>
<p>
I tried a third experiment to be sure‚Ä¶ and got hits from both Bingbot
and YandexBot.
</p>
<h4 id="cloudflare-crawler-hints">
It was Cloudflare Crawler Hints, not ChatGPT
</h4>
<p>
So I wrote up and posted about my discovery‚Ä¶ and
<a href="https://x.com/jatan_loya/status/1952506398270767499">Jatan Loya
asked:</a>
</p>
<blockquote>
<p>
do you have crawler hints enabled in cf?
</p>
</blockquote>
<p>
And yeah, it turned out I did. I spotted this in my caching
configuration page (and it looks like I must have turned it on myself at
some point in the past):
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/cloudflare-crawler-hints.jpg" alt="Screenshot of Cloudflare settings panel showing &quot;Crawler Hints Beta&quot; with description text explaining that Crawler Hints provide high quality data to search engines and other crawlers when sites using Cloudflare change their content. This allows crawlers to precisely time crawling, avoid wasteful crawls, and generally reduce resource consumption on origins and other Internet infrastructure. Below states &quot;By enabling this service, you agree to share website information required for feature functionality and agree to the Supplemental Terms for Crawler Hints.&quot; There is a toggle switch in the on position on the right side and a &quot;Help&quot; link in the bottom right corner." style="max-width: 100%" />
</p>
<p>
Here‚Äôs
<a href="https://developers.cloudflare.com/cache/advanced-configuration/crawler-hints/">the
Cloudflare documentation for that feature</a>.
</p>
<p>
I deleted my posts on Twitter and Bluesky (since you can‚Äôt edit those
and I didn‚Äôt want the misinformation to continue to spread) and edited
<a href="https://fedi.simonwillison.net/@simon/114972968822349077">my
post on Mastodon</a>, then updated this entry with the real reason this
had happened.
</p>
<p>
I also changed the URL of this entry as it turned out Twitter and
Bluesky were caching my social media preview for the previous one, which
included the incorrect information in the title.
</p>
<details>
<summary>
Original ‚ÄúSo what‚Äôs going on here?‚Äù section from my post
</summary>
<p>
<em>Here‚Äôs a section of my original post with my theories about what was
going on before learning about Cloudflare Crawler Hints.</em>
</p>
<h4 id="so-what-s-going-on-here-">
So what‚Äôs going on here?
</h4>
<p>
There are quite a few different moving parts here.
</p>
<ol>
<li>
I‚Äôm using Firefox on macOS with the 1Password and Readwise Highlighter
extensions installed and active. Since I didn‚Äôt visit the debug pages at
all with my own browser I don‚Äôt think any of these are relevant to these
results.
</li>
<li>
ChatGPT agent makes just a single request to my debug URL ‚Ä¶
</li>
<li>
‚Ä¶ which is proxied through both Cloudflare and Heroku.
</li>
<li>
Within about a minute, I get hits from one or both of Bingbot and
Yandex.
</li>
</ol>
<p>
Presumably ChatGPT agent itself is running behind at least one proxy - I
would expect OpenAI to keep a close eye on that traffic to ensure it
doesn‚Äôt get abused.
</p>
<p>
I‚Äôm guessing that infrastructure is hosted by Microsoft Azure. The
<a href="https://openai.com/policies/sub-processor-list/">OpenAI
Sub-processor List</a> - though that lists Microsoft Corporation,
CoreWeave Inc, Oracle Cloud Platform and Google Cloud Platform under the
‚ÄúCloud infrastructure‚Äù section so it could be any of those.
</p>
<p>
Since the page is served over HTTPS my guess is that any intermediary
proxies should be unable to see the path component of the URL, making
the mystery of how Bingbot and Yandex saw the URL even more intriguing.
</p>
</details>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/bing&quot;&gt;bing&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/privacy&quot;&gt;privacy&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/search-engines&quot;&gt;search-engines&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/user-agents&quot;&gt;user-agents&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/cloudflare&quot;&gt;cloudflare&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/chatgpt&quot;&gt;chatgpt&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/chatgpt-agents-user-agent/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/chatgpt-agents-user-agent/#atom-everything</a></p>
<hr />
<h2
id="particle-tachyon-5g-single-board-pc-now-available-for-299">Particle
Tachyon 5G single-board PC now available for $299</h2>
<p>date: 2025-08-04, from: Liliputing</p>
<p>
The¬†Particle Tachyon¬†is a single-board computer that‚Äôs about the same
size as a Raspberry Pi 5 and it even has a Raspberry Pi-compatible
40-pin GPIO header. But Particle positions the Tachyon as a versatile
little PC with the guts of ‚Äúa modern smartphone.‚Äù That‚Äôs because it‚Äôs
powered by a Qualcomm QCM6490 Dragonwing processor with 8 Kryo [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/particle-tachyon-5g-single-board-pc-now-available-for-299/">Particle
Tachyon 5G single-board PC now available for $299</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/particle-tachyon-5g-single-board-pc-now-available-for-299/"
class="uri">https://liliputing.com/particle-tachyon-5g-single-board-pc-now-available-for-299/</a></p>
<hr />
<h2 id="usage-charts-for-my-llm-tool-against-openrouter">Usage charts
for my LLM tool against OpenRouter</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://openrouter.ai/apps?url=https%3A%2F%2Fllm.datasette.io%2F">Usage
charts for my LLM tool against OpenRouter</a></strong>
</p>
OpenRouter proxies requests to a large number of different LLMs and
provides high level statistics of which models are the most popular
among their users.
</p>
<p>
Tools that call OpenRouter can include <code>HTTP-Referer</code> and
<code>X-Title</code> headers to credit that tool with the token usage.
My
<a href="https://github.com/simonw/llm-openrouter/">llm-openrouter</a>
plugin
<a href="https://github.com/simonw/llm-openrouter/blob/8e4be78e60337154b063faaa7161dddd91462730/llm_openrouter.py#L99C13-L99C20">does
that here</a>.
</p>
<p>
‚Ä¶ which means
<a href="https://openrouter.ai/apps?url=https%3A%2F%2Fllm.datasette.io%2F">this
page</a> displays aggregate stats across users of that plugin! Looks
like someone has been running a lot of traffic through
<a href="https://openrouter.ai/qwen/qwen3-14b">Qwen 3 14B</a> recently.
</p>
<p>
<p><img alt="Screenshot of LLM usage statistics dashboard showing a stacked bar chart from July 5 to August 4, 2025, with a legend on the right displaying &quot;Top models&quot; including Qwen: Qwen3 14B (480M), Google: Gemini 2.5 Flash Lite Preview 06-17 (31.7M), Horizon Beta (3.77M), Google: Gemini 2.5 Flash Lite (1.67M), google/gemini-2.0-flash-exp (1.14M), DeepSeek: DeepSeek V3 0324 (1.11M), Meta: Llama 3.3 70B Instruct (228K), Others (220K), Qwen: Qwen3 Coder (218K), MoonshotAI: Kimi K2 (132K), and Horizon Alpha (75K), with a total of 520M usage shown for August 3, 2025." src="https://static.simonwillison.net/static/2025/llm-usage-openrouter.jpg" /></p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm&quot;&gt;llm&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openrouter&quot;&gt;openrouter&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/llm-openrouter-usage/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/llm-openrouter-usage/#atom-everything</a></p>
<hr />
<h2 id="superduper-4.0-beta">SuperDuper 4.0 Beta</h2>
<p>date: 2025-08-04, from: Michael Tsai</p>
<p>Dave Nanian: Our new trace capability showed quite clearly that the
folder we were working on was~/Pictures/Photos
Library.photoslibrary/database/search/Spotlight/SpotlightKnowledgeEvents/index.V2/journals/12/cs_defaultAnd
that‚Äôs a folder I don‚Äôt have. When the user navigated to it at first, he
said it was ‚Äúempty‚Äù‚Ä¶which was weird. But later, he noticed that there
was a spinner at the bottom of the Finder window. [‚Ä¶]</p>
<p><br></p>
<p><a href="https://mjtsai.com/blog/2025/08/04/superduper-4-0-beta/"
class="uri">https://mjtsai.com/blog/2025/08/04/superduper-4-0-beta/</a></p>
<hr />
<h2 id="logging-privacy-shenanigans">Logging Privacy Shenanigans</h2>
<p>date: 2025-08-04, from: Michael Tsai</p>
<p>Peter Steinberger: If you‚Äôve ever tried debugging a macOS app using
the unified logging system, you‚Äôve probably encountered the dreaded
&lt;private&gt; redaction. Your carefully crafted log messages turn into
cryptic puzzles where the most important debugging information is
hidden. [‚Ä¶] You don‚Äôt need to use .mobileconfig files ‚Äì you can simply
drop plist files directly [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/04/logging-privacy-shenanigans/"
class="uri">https://mjtsai.com/blog/2025/08/04/logging-privacy-shenanigans/</a></p>
<hr />
<h2 id="device-added-to-your-account">Device Added to Your Account</h2>
<p>date: 2025-08-04, from: Michael Tsai</p>
<p>Riccardo Mori: Whenever I revive one of these devices, if it‚Äôs still
able to access iCloud and other Apple ID-related services, I get a
notification on all my other Apple devices that a certain device has now
access to FaceTime and iMessage. The wording in this notification has
changed for the worse in more recent [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/04/device-added-to-your-account/"
class="uri">https://mjtsai.com/blog/2025/08/04/device-added-to-your-account/</a></p>
<hr />
<h2 id="accuweather-to-discontinue-free-api">AccuWeather to Discontinue
Free API</h2>
<p>date: 2025-08-04, from: Michael Tsai</p>
<p>AccuWeather (via Hacker News): AccuWeather‚Äôs‚ÄØcurrent Free Limited
Trials for Core Weather and MinuteCast¬Æ will be retired with the new
portal launch. [‚Ä¶] Once your trial ends, you can keep building with our
affordable Starter package, which offers essential API access at a
competitive monthly rate. It doesn‚Äôt say what the new plans are.
Previously: Weather [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://mjtsai.com/blog/2025/08/04/accuweather-to-discontinue-free-api/"
class="uri">https://mjtsai.com/blog/2025/08/04/accuweather-to-discontinue-free-api/</a></p>
<hr />
<h2 id="qwen-image-crafting-with-native-text-rendering">Qwen-Image:
Crafting with Native Text Rendering</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://qwenlm.github.io/blog/qwen-image/">Qwen-Image:
Crafting with Native Text Rendering</a></strong>
</p>
Not content with releasing
<a href="https://simonwillison.net/2025/Jul/30/chinese-models/">six
excellent open weights LLMs in July</a>, Qwen are kicking off August
with their first ever image generation model.
</p>
<p>
Qwen-Image is a 20 billion parameter MMDiT (Multimodal Diffusion
Transformer, originally proposed for Stable Diffusion 3) model under an
Apache 2.0 license. The
<a href="https://huggingface.co/Qwen/Qwen-Image">Hugging Face repo</a>
is 53.97GB.
</p>
<p>
Qwen released a
<a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf">detailed
technical report</a> (PDF) to accompany the model. The model builds on
their Qwen-2.5-VL vision LLM, and they also made extensive use of that
model to help create some of their their training data:
</p>
<blockquote>
<p>
In our data annotation pipeline, we utilize a capable image captioner
(e.g., Qwen2.5-VL) to generate not only comprehensive image
descriptions, but also structured metadata that captures essential image
properties and quality attributes.
</p>
<p>
Instead of treating captioning and metadata extraction as independent
tasks, we designed an annotation framework in which the captioner
concurrently describes visual content and generates detailed information
in a structured format, such as JSON. Critical details such as object
attributes, spatial relationships, environmental context, and verbatim
transcriptions of visible text are captured in the caption, while key
image properties like type, style, presence of watermarks, and abnormal
elements (e.g., QR codes or facial mosaics) are reported in a structured
format.
</p>
</blockquote>
<p>
They put a <em>lot</em> of effort into the model‚Äôs ability to render
text in a useful way. 5% of the training data (described as ‚Äúbillions of
image-text pairs‚Äù) was data ‚Äúsynthesized through controlled text
rendering techniques‚Äù, ranging from simple text through text on an image
background up to much more complex layout examples:
</p>
<blockquote>
<p>
To improve the model‚Äôs capacity to follow complex, structured prompts
involving layout-sensitive content, we propose a synthesis strategy
based on programmatic editing of pre-defined templates, such as
PowerPoint slides or User Interface Mockups. A comprehensive rule-based
system is designed to automate the substitution of placeholder text
while maintaining the integrity of layout structure, alignment, and
formatting.
</p>
</blockquote>
<p>
I tried the model out using the
<a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced">ModelScope
demo</a> - I signed in with GitHub and verified my account via a text
message to a phone number. Here‚Äôs what I got for ‚ÄúA raccoon holding a
sign that says‚ÄùI love trash‚Äù that was written by that raccoon‚Äù:
</p>
<p>
<img alt="A great photo of a raccoon holding a cardboard sign, the text I love trash is written on it in marker, the raccoon has chosen to draw the o in love as a heart filled with red marker pen." src="https://static.simonwillison.net/static/2025/qwen-trash.jpg" />
</p>
<p>
The raccoon has very neat handwriting!
</p>
<p>
<strong>Update</strong>: A version of the model exists that can edit
existing images but it‚Äôs
<a href="https://github.com/QwenLM/Qwen-Image/issues/3#issuecomment-3151573614">not
yet been released</a>:
</p>
<blockquote>
<p>
Currently, we have only open-sourced the text-to-image foundation model,
but the editing model is also on our roadmap and planned for future
release.
</p>
</blockquote>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://x.com/Alibaba_Qwen/status/1952398250121756992&quot;&gt;@Alibaba_Qwen&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/stable-diffusion&quot;&gt;stable-diffusion&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/vision-llms&quot;&gt;vision-llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/training-data&quot;&gt;training-data&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/qwen&quot;&gt;qwen&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/text-to-image&quot;&gt;text-to-image&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-in-china&quot;&gt;ai-in-china&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/qwen-image/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/qwen-image/#atom-everything</a></p>
<hr />
<h2
id="modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding">Modos
Paper Dev Kit cranks E Ink monitor refresh rates up to 75 Hz
(crowdfunding)</h2>
<p>date: 2025-08-04, from: Liliputing</p>
<p>
E Ink displays are often used in eBook readers or digital signage thanks
to their low power consumption and paper-like qualities. But most
devices with E Ink displays have low screen refresh rates that make them
awkward fits for video playback or other high-motion graphics. While
we‚Äôve seen a few smartphones, tablets, and monitors with [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding/">Modos
Paper Dev Kit cranks E Ink monitor refresh rates up to 75 Hz
(crowdfunding)</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding/"
class="uri">https://liliputing.com/modos-paper-dev-kit-cranks-e-ink-monitor-refresh-rates-up-to-75-hz-crowdfunding/</a></p>
<hr />
<h2 id="quoting-himbodhisattva">Quoting <span class="citation"
data-cites="himbodhisattva">@himbodhisattva</span></h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://x.com/himbodhisattva/status/1525182881726730240">
<p>
for services that wrap GPT-3, is it possible to do the equivalent of sql
injection? like, a prompt-injection attack? make it think it‚Äôs completed
the task and then get access to the generation, and ask it to repeat the
original instruction?
</p>
</blockquote>
<p class="cite">
‚Äî
<a href="https://x.com/himbodhisattva/status/1525182881726730240"><span
class="citation" data-cites="himbodhisattva">@himbodhisattva</span></a>,
coining the term prompt injection on 13th May 2022, four months before
<a href="https://simonwillison.net/2022/Sep/12/prompt-injection/">I
did</a>
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/prompt-injection&quot;&gt;prompt-injection&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/security&quot;&gt;security&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/himbodhisattva/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/himbodhisattva/#atom-everything</a></p>
<hr />
<h2 id="i-saved-a-png-image-to-a-bird">I Saved a PNG Image To A
Bird</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://www.youtube.com/watch?v=hCQCP-5g5bo">I Saved a
PNG Image To A Bird</a></strong>
</p>
Benn Jordan provides one of the all time great YouTube video titles, and
it‚Äôs justified. He drew an image in an audio spectrogram, played that
sound to a talented starling (internet celebrity
<a href="https://www.tiktok.com/@farijuana_bird/video/7452882774991572254">‚ÄúThe
Mouth‚Äù</a>) and recorded the result that the starling almost perfectly
imitated back to him.
</p>
<blockquote>
<p>
Hypothetically, if this were an audible file transfer protocol that used
a 10:1 data compression ratio, that‚Äôs nearly 2 megabytes of information
per second. While there are a lot of caveats and limitations there, the
fact that you could set up a speaker in your yard and conceivably store
any amount of data in songbirds is crazy.
</p>
</blockquote>
<p>
<p>This video is full of so much more than just that. Fast forward to
<a href="https://www.youtube.com/watch?v=hCQCP-5g5bo&amp;t=358s">5m58s</a>
for footage of a nest full of brown pelicans showing the sounds made by
their chicks!</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/audio&quot;&gt;audio&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/youtube&quot;&gt;youtube&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/i-saved-a-png-image-to-a-bird/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/i-saved-a-png-image-to-a-bird/#atom-everything</a></p>
<hr />
<h2 id="your-own-newspaper-or-not">Your Own Newspaper, Or Not</h2>
<p>date: 2025-08-04, from: Chris Coyier blog</p>
<p>You‚Äôve likely heard me go on about how much I like an encourage using
an RSS reader. Molly White frames it nicely: What if you could take all
your favorite newsletters, ditch the data collection, and curate your
own newspaper? It could include independent journalists, bloggers,
mainstream media, worker-owned media collectives, and just about anyone
[‚Ä¶]</p>
<p><br></p>
<p><a
href="https://chriscoyier.net/2025/08/04/your-own-newspaper-or-not/"
class="uri">https://chriscoyier.net/2025/08/04/your-own-newspaper-or-not/</a></p>
<hr />
<h2 id="superman-el-nuevo-punk-rock">Superman: el nuevo punk rock</h2>
<p>date: 2025-08-04, from: Iv√°n Paredes Res√©ndiz blog, Mexico‚Äôs
cinema</p>
<p>
Direcci√≥n: James Gunn. Guion: James Gunn. Elenco: David Corenswet,
Rachel Brosnahan, Nicholas Hoult, Edi Gathegi, Nathan Fillion, Isabela
Merced, Mar√≠a Gabriela de Far√≠a, Anthony Carrigan, Skyler Gisondo, Sara
Sampaio. Pa√≠s: Estados Unidos. ¬†¬† M√°s informaci√≥n de la pel√≠cula:
https://www.imdb.com/title/tt5950044 Superman, como h√©roe de c√≥mics, es
una figura que necesita de una constante actualizaci√≥n para mantenerse
[‚Ä¶]
</p>
<p>
La entrada
<a href="https://www.palomitademaiz.net/resenas-superman/">Superman: el
nuevo punk rock</a> se public√≥ primero en
<a href="https://www.palomitademaiz.net">Palomita de ma√≠z</a>.
</p>
<p><br></p>
<p><a
href="https://www.palomitademaiz.net/resenas-superman/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=resenas-superman"
class="uri">https://www.palomitademaiz.net/resenas-superman/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=resenas-superman</a></p>
<hr />
<h2 id="public-broadcastings-democratic-value">Public Broadcasting‚Äôs
Democratic Value</h2>
<p>date: 2025-08-04, from: Guy Kawasaki blog</p>
<p>Stephanie A. (Sam) Martin, Frank and Bethine Church Endowed Chair of
Public Affairs, Boise State University.</p>
<p><br></p>
<p><a
href="https://guykawasaki.substack.com/p/public-broadcastings-democratic-value"
class="uri">https://guykawasaki.substack.com/p/public-broadcastings-democratic-value</a></p>
<hr />
<h2 id="slopocalypse-now">Slopocalypse Now</h2>
<p>date: 2025-08-04, from: Gary Marcus blog</p>
<p>The Horror</p>
<p><br></p>
<p><a href="https://garymarcus.substack.com/p/slopocalypse-now"
class="uri">https://garymarcus.substack.com/p/slopocalypse-now</a></p>
<hr />
<h2 id="quoting-nick-turley">Quoting Nick Turley</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: Simon Willison‚Äôs
Weblog</p>
<blockquote cite="https://x.com/nickaturley/status/1952385556664520875">
<p>
This week, ChatGPT is on track to reach 700M weekly active users ‚Äî up
from 500M at the end of March and 4√ó since last year.
</p>
</blockquote>
<p class="cite">
‚Äî <a href="https://x.com/nickaturley/status/1952385556664520875">Nick
Turley</a>, Head of ChatGPT, OpenAI
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/chatgpt&quot;&gt;chatgpt&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/4/nick-turley/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/4/nick-turley/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="IIIF">@IIIF</span>
Mastodon feed</strong> (date: 2025-08-04, from: IIIF Mastodon feed)</p>
<p>
The Glycerine Framework has been extended: Glycerine server provides the
back-end data model and APIs for integration into existing DAMS and CMS
platforms, Glycerine editor allows for annotations, and the Glycerine
viewer is embeddable into an existing CMS.
</p>
<p>
If you're interested in learning more, the
<a href="https://glammr.us/tags/IIIF" class="mention hashtag" rel="tag">#<span>IIIF</span></a>
Consortium and the Glycerine team are hosting (45 min.) showcases &amp;
demos in different time zones on August 12 and 13. Free registration on
Eventbrite:
<a href="https://www.eventbrite.com/o/iiif-consortium-19836883937" target="_blank" rel="nofollow noopener" translate="no"><span
class="invisible">https://www.</span><span
class="ellipsis">eventbrite.com/o/iiif-consorti</span><span
class="invisible">um-19836883937</span></a>
</p>
<p><br></p>
<p><a href="https://glammr.us/@IIIF/114971144220041722"
class="uri">https://glammr.us/@IIIF/114971144220041722</a></p>
<hr />
<h2 id="r2-d2-vex-robot">R2-D2 VEX robot</h2>
<p>date: 2025-08-04, from: Raspberry Pi News (.com)</p>
<p>
Raspberry¬†Pi 3 and a VEX Robotics kit transform a toy version of an
iconic film character into a working robot.
</p>
<p>
The post
<a href="https://www.raspberrypi.com/news/r2-d2-vex-robot/">R2-D2 VEX
robot¬†</a> appeared first on
<a href="https://www.raspberrypi.com">Raspberry Pi</a>.
</p>
<p><br></p>
<p><a href="https://www.raspberrypi.com/news/r2-d2-vex-robot/"
class="uri">https://www.raspberrypi.com/news/r2-d2-vex-robot/</a></p>
<hr />
<h2
id="the-anti-porn-crusade-that-censored-steam-and-itch.io-started-30-years-ago">The
Anti-Porn Crusade That Censored Steam and Itch.io Started 30 Years
Ago</h2>
<p>date: 2025-08-04, from: 404 Media Group</p>
<p>Keywords and tags have never been a useful metric for distilling
nuance. Pushing for regulations based on them is repeating a 30-year
history of porn panic online.</p>
<p><br></p>
<p><a
href="https://www.404media.co/steam-itchio-collective-shout-nsfw-games-campaign/"
class="uri">https://www.404media.co/steam-itchio-collective-shout-nsfw-games-campaign/</a></p>
<hr />
<h2 id="the-microsoft-smurface">The Microsoft Smurface</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: One Foot Tsunami</p>
<p><br></p>
<p><a
href="https://onefoottsunami.com/2025/08/04/the-microsoft-smurface/"
class="uri">https://onefoottsunami.com/2025/08/04/the-microsoft-smurface/</a></p>
<hr />
<h2
id="should-lyft-and-uber-charge-more-if-your-battery-is-low-california-may-soon-ban-that">Should
Lyft and Uber charge more if your battery is low? California may soon
ban that</h2>
<p>date: 2025-08-04, from: The Markup blog</p>
<p>California lawmakers want to ban companies from using data about
consumers‚Äô devices like battery life, model and geolocation to set
fluctuating prices. Proponents say such ‚Äúsurveillance pricing‚Äù is
discriminatory.</p>
<p><br></p>
<p><a
href="https://themarkup.org/artificial-intelligence/2025/08/04/california-surveillance-pricing-ban"
class="uri">https://themarkup.org/artificial-intelligence/2025/08/04/california-surveillance-pricing-ban</a></p>
<hr />
<h2
id="ai-open-science-and-the-future-of-research-integrity-an-interview-with-alison-mudditt-of-plos">AI,
Open Science, and the Future of Research Integrity: An Interview with
Alison Mudditt of PLOS</h2>
<p>date: 2025-08-04, from: Authors Union blogs</p>
<p>Below is an interview with¬†Alison Mudditt, CEO of PLOS (Public
Library of Science) discussing the impact of AI on publishing [‚Ä¶]</p>
<p><br></p>
<p><a
href="https://www.authorsalliance.org/2025/08/04/ai-open-science-and-the-future-of-research-integrity-an-interview-with-alison-mudditt-of-plos/"
class="uri">https://www.authorsalliance.org/2025/08/04/ai-open-science-and-the-future-of-research-integrity-an-interview-with-alison-mudditt-of-plos/</a></p>
<hr />
<h2 id="trump-is-getting-desperate">Trump is Getting Desperate</h2>
<p>date: 2025-08-04, from: Paul Krugman</p>
<p>We‚Äôre in an extremely dangerous moment</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/trump-is-getting-desperate-and-dangerous"
class="uri">https://paulkrugman.substack.com/p/trump-is-getting-desperate-and-dangerous</a></p>
<hr />
<h2 id="new-wave-of-projects-to-create-digital-commons">New wave of
projects to create digital commons</h2>
<p>date: 2025-08-04, updated: 2025-08-04, from: nlnet feed</p>
<p><br></p>
<p><a
href="https://nlnet.nl/news/2025/20250804-announcement-grants-CommonsFund.html"
class="uri">https://nlnet.nl/news/2025/20250804-announcement-grants-CommonsFund.html</a></p>
<hr />
<h2 id="making-my-github-heatmap-widget">Making my GitHub heatmap
widget</h2>
<p>date: 2025-08-04, from: Lean Rada‚Äôs blog</p>
<p>
<em>For RSS readers: This article contains interactive content available
on the
<a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">original
post on leanrada.com</a>.</em>
</p>
<p>
This post is about how I made the GitHub heatmap widget on my site.
</p>
<p>
Here‚Äôs the raw, live WebComponent <code>&lt;gh-contribs&gt;</code> by
the way:
</p>
<card-box>
<pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</card-box>
<h2>
Scraping the data
</h2>
<p>
First, I had to scrape the heatmap data. I don‚Äôt know if there‚Äôs a
proper API, but I found an endpoint that renders an HTML partial of what
I wanted.
</p>
<p>
As far as I know, the GitHub website today works using partial HTMLs to
update its UI without reloading the whole page. I think this endpoint
populates the contribution graph section of the profile page.
</p>
<p>
The endpoint that returns a user‚Äôs contribution graph is
<a href="https://github.com/users/Kalabasa/contributions" target="_blank"><code>https://github.com/users/{username}/contributions</code></a>.
I presume the user has to have contribution stats public. This
undocumented API could also break at any time. üò¨
</p>
<figure>
<img src="https://leanrada.com/notes/github-heatmap-widget/contributions-html.png?ref=rss" loading="lazy" width="932" height="741">
<figcaption>
The response HTML for my github.com/users/Kalabasa/contributions
</figcaption>
</figure>
<p>
Loading this endpoint gives you an unstyled piece of HTML containing an
HTML table of contribution data and other UI. The table cells are
invisible because of the lack of styles! When embedded in the profile
page, it inherits the appropriate styling in context.
</p>
<p><img alt="styled contribution table" src="https://leanrada.com/notes/github-heatmap-widget/contributions-styled.png?ref=rss" loading="lazy" width="741" height="204"></p>
<p>
The first column is the weekday label, and the rest of the cells seem to
represent a single day each. The data is encoded in the HTML that
presents the data! This reminds me of
<a href="https://htmx.org/essays/hateoas/" target="_blank">Hypermedia as
the Engine of Application State</a>. html = data.
</p>
<pre><code>&lt;tbody&gt;
&lt;tr style="height: 10px"&gt;
  &lt;td class="ContributionCalendar-label" style="position: relative"&gt;
    &lt;span class="sr-only"&gt;Monday&lt;/span&gt;
    &lt;span aria-hidden="true" style="clip-path: None; position: absolute; bottom: -3px"&gt;
      Mon
    &lt;/span&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="0"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-2"
    style="width: 10px"
    data-date="2024-08-05"
    id="contribution-day-component-1-0"
    data-level="2"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="1"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-1"
    style="width: 10px"
    data-date="2024-08-12"
    id="contribution-day-component-1-1"
    data-level="1"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  &lt;td
    tabindex="0"
    data-ix="2"
    aria-selected="false"
    aria-describedby="contribution-graph-legend-level-2"
    style="width: 10px"
    data-date="2024-08-19"
    id="contribution-day-component-1-2"
    data-level="2"
    role="gridcell"
    data-view-component="true"
    class="ContributionCalendar-day"&gt;
  &lt;/td&gt;
  <!--...--></code></pre>
<p>
What I was looking for here was the <code>data-level</code> attribute on
each cell. It contains a coarse integer value that indicates the
activity level for the day.
</p>
<p>
Coupled with the <code>data-date</code> attribute, it became rather easy
to scrape this data! Instead of keeping track of columns and rows, I
just go through each <code>data-date</code> and <code>data-level</code>
as a (date,level) data point.
</p>
<p>
Here‚Äôs my parse function using
<a href="https://cheerio.js.org/" target="_blank">cheerio</a>, a jQuery
clone for Node.js.
</p>
<pre><code>const res = await fetch(
  "https://github.com/users/Kalabasa/contributions");
let data = parseContribs(await res.text());

/**
 * Parses a GitHub contribution calendar HTML string and extracts contribution data.
 *
 * @param {string} html - The HTML string containing the GitHub contribution calendar.
 * @returns {{ date: Date, level: number }[]} Array of contribution objects with date and activity level.
 * @throws {Error} If the contribution calendar table cannot be found in the HTML.
 */
function parseContribs(html) {
  const ch = cheerio.load(html);
  const chTable = ch("table.ContributionCalendar-grid");
  if (!chTable.length) throw new Error("Can't find table.");
  const chDays = chTable.find("[data-date]");
  const data = chDays
    .map((_, el) =&gt; {
      const chDay = ch(el);
      const date = new Date(chDay.attr("data-date"));
      const level = parseInt(chDay.attr("data-level"), 10);
      return { date, level };
    })
    .get();
  return data;
}</code></pre>
<p>This data is scraped at regular intervals, reformatted into a grid,
and saved into a compact JSON format for later consumption and rendering
by the WebComponent on the client.</p>
<pre><code>// gh-contribs.json
[
  [1,2,1,2,2,1,1],
  [0,1,0,2,0,1,0],
  [1,1,1,1,1,1,0],
  [0,1,0,0,1,1,0],
  [0,0,1,0,0,0]
]</code></pre>
<h2>
Rendering the data
</h2>
<p>
The reason why the data is reformatted into a grid like that is to make
the rendering logic straightforward. The data is structured so that it
can be directly converted into HTML without thinking in dates and weeks
that are in the original data.
</p>
<p>
Here are the current JSON and WebComponent side by side. Each row in the
data gets directly rendered as a column in the component.
</p>
<auto-grid> <code-block language="js">
<pre><code>
      <pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Inline script</pre>
    </code></pre>
</code-block>
<pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</auto-grid>
<p>
As such, <code>&lt;gh-contribs&gt;</code>‚Äôs initialisation logic is
really simple:
</p>
<pre><code>const contribs = await fetch(
  "path/to/gh-contribs.json"
).then((res) =&gt; res.json());

let htmlString = "";
for (const col of contribs) {
  for (const level of col) {
    htmlString += html`&lt;div data-level="${level}"&gt;${level}&lt;/div&gt;`;
  }
}
this.innerHTML = htmlString;</code></pre>
<p>Add some CSS and it‚Äôs done:</p>
<pre><code>gh-contribs {
  display: grid;
  grid-auto-flow: column;
  grid-template-rows: repeat(7, auto);
  gap: 12px;
  div {
    position: relative;
    width: 18px;
    height: 18px;
    background: #222c2c;
    color: transparent;
    &amp;::after {
      content: "";
      position: absolute;
      inset: 0;
      background: #54f8c1;
    }
    &amp;[data-level="0"]::after {
      opacity: 0;
    }
    &amp;[data-level="1"]::after {
      opacity: 0.3;
    }
    &amp;[data-level="2"]::after {
      opacity: 0.6;
    }
    &amp;[data-level="3"]::after {
      opacity: 1;
    }
  }
}</code></pre>
<h2>
Why not use the original HTML?
</h2>
<p>
Why not just embed the contributions HTML from GitHub? Slice the
relevant <code>&lt;tr&gt;</code>s and <code>&lt;td&gt;</code>s‚Ä¶? Why
parse the original HTML table, convert it to JSON, then render it as
HTML again?
</p>
<p>
The main reason to do [HTML ‚Üí JSON ‚Üí HTML] is to remain flexible. As you
know, that endpoint is undocumented. Also, depending on the HTML
structure of the original is risky. Risk of breakage, risk of unwanted
content, etc.
</p>
<p>
This way, I can change how I get the data without refactoring the
WebComponent. I could go [<strong>GitHub API</strong> ‚Üí JSON ‚Üí HTML] or
[<strong>local git script</strong> ‚Üí JSON ‚Üí HTML] or whatever.
</p>
<p>
It also works the other end. I actually rewrote this widget recently
(from statically-generated HTML into a WebComponent) without having to
change the scraper script or the JSON data structure.
</p>
<h2>
Final touches
</h2>
<p>
The WebComponent renders just the grid itself for flexibility. This let
me use it in different ways, like with an icon and heading as in the
home page.
</p>
<card-box>
<h4>
<img src="https://leanrada.com/icons/github.png?ref=rss" alt="" loading="lazy" width="16" height="16">
my github<br>heatmap
</h4>
<pre>Interactive content: <a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss">Visit the post to interact with this content.</a>
Alternative name: Gh Contribs</pre>
</card-box>
<p>
Here‚Äôs the
<a href="https://leanrada.com/components/gh-contribs/gh-contribs.js?ref=rss" target="_blank">source
code for this WebComponent</a> if you‚Äôre interested.
</p>
<p><br></p>
<p><a href="https://leanrada.com/notes/github-heatmap-widget/?ref=rss"
class="uri">https://leanrada.com/notes/github-heatmap-widget/?ref=rss</a></p>
<hr />
<h2 id="modos-developer-kit-now-live-on-crowd-supply">Modos Developer
Kit Now Live on Crowd Supply!</h2>
<p>date: 2025-08-04, from: Modos Blog</p>
<p>The Modos Dev Kit is live! Build with fast-refresh, low-latency
e-paper. Now on Crowd Supply.</p>
<p><br></p>
<p><a href="https://www.modos.tech/blog/modos-developer-kit-live"
class="uri">https://www.modos.tech/blog/modos-developer-kit-live</a></p>
<hr />
<h2
id="llvmcgo25---carts-enabling-event-driven-task-and-data-block-compilation-for-distributed-hpc">LLVMCGO25
- CARTS: Enabling Event-Driven Task and Data Block Compilation for
Distributed HPC</h2>
<p>date: 2025-08-04, from: LLVM Blog</p>
<h1 id="llvmcgo25---carts-enabling-event-driven-task-and-data-block-compilation-for-distributed-hpc">
LLVMCGO25 - CARTS: Enabling Event-Driven Task and Data Block Compilation
for Distributed HPC
</h1>
<p>
Hello everyone! I‚Äôm Rafael, a PhD candidate at the University of
Delaware. I recently flew from Philadelphia to Las Vegas to attend the
CGO conference,where I had the chance to present my project and soak in
new ideas about HPC.
</p>
<p>
In this blog, I‚Äôll dive into the project I discussed at the conference
and share some personal insights and lessons I learned along the
way.Although comments aren‚Äôt enabled here, I‚Äôd love to hear from you,
feel free to reach out at (<em>rafaelhg at udel dot edu</em>) if you‚Äôre
interested in collaborating, have questions, or just want to chat.
</p>
<h2 id="motivation-why-carts">
Motivation: Why CARTS?
</h2>
<p>
Modern High-Performance Computing (HPC) and AI/ML workloads are pushing
our hardware and software to the limits. Some key challenges include:
</p>
<ul>
<li>
<strong>Evolving Architectures:</strong> Systems now have complex memory
hierarchies that need smart utilization.
</li>
<li>
<strong>Hardware Heterogeneity:</strong> With multi-core CPUs, GPUs, and
specialized accelerators in the mix, resource management gets tricky.
</li>
<li>
<strong>Performance Pressure:</strong> Large-scale systems demand
efficient handling of concurrency, synchronization, and communication.
</li>
</ul>
<p>
These challenges led to the creation of CARTS‚Äîa compiler framework that
combines the flexibility of MLIR with the reliability of LLVM to
optimize applications for distributed HPC environments.
</p>
<h2 id="a-closer-look-at-arts-and-its-inspirations">
A Closer Look at ARTS and Its Inspirations
</h2>
<p>
At the heart of CARTS is ARTS. Originally, ARTS stood for the
<strong>Abstract Runtime System</strong>.I often get mixed up and
mistakenly call it the <strong>Asynchronous Runtime System</strong>. To
keep things light,we sometimes joke about it being the <strong>Any
Runtime System</strong>.
</p>
<p>
ARTS is inspired by the Codelet model, a concept I could talk about all
day!The Codelet model breaks a computation into small, independent tasks
(or ‚Äúcodelets‚Äù) that can run as soon as their data dependencies are
met.If you‚Äôre curious to learn more about this model (or find it
delightfully abstract), I suggest you visit our research group websiteat
<a href="https://www.capsl.udel.edu/">CAPSL, University of Delaware</a>
and check out the
<a href="https://www.capsl.udel.edu/codelets.shtml#B4">Codelet Model
website</a>.
</p>
<h3 id="what-does-arts-do">
What Does ARTS Do?
</h3>
<p>
ARTS is designed to support fine-grained, event-driven task execution in
distributed systems. Here‚Äôs a simple breakdown of some key concepts:
</p>
<ul>
<li>
<strong>Event-Driven Tasks (EDTs):</strong> These are the basic units of
work that can be scheduled independently. Think of an EDT as a small,
self-contained task that runs once all its required data is ready.
</li>
<li>
<strong>DataBlocks:</strong> These represent memory regions holding the
data needed by tasks. ARTS tracks these DataBlocks across distributed
nodes so that tasks have quick and efficient access to the data they
need.
</li>
<li>
<strong>Events:</strong> These are signals that tell the system when a
DataBlock is ready or when a task has finished. They help synchronize
tasks without the need for heavy locks.
</li>
<li>
<strong>Epochs:</strong> These act as synchronization boundaries. An
epoch groups tasks together, ensuring that all tasks within the group
finish before moving on to the next phase.
</li>
</ul>
<p>
By modeling tasks, DataBlocks, events, and epochs explicitly, ARTS makes
it easier to analyze and optimize how tasks are executed across large,
distributed systems.
</p>
<h2 id="the-carts-compiler-pipeline">
The CARTS Compiler Pipeline
</h2>
<p>
Building on ARTS, CARTS creates a task-centric compiler workflow. Here‚Äôs
how it works:
</p>
<h3 id="clangpolygeist-from-copenmp-to-mlir">
Clang/Polygeist: From C/OpenMP to MLIR
</h3>
<ul>
<li>
<strong>Conversion Process:</strong> Using the Polygeist infrastructure,
we translate C/OpenMP code into MLIR. This process handles multiple
dialects (like OpenMP, SCF, Affine, and Arith).
</li>
<li>
<strong>Extended Support:</strong> We‚Äôve enhanced it to handle more
OpenMP constructs, including OpenMP Tasks
</li>
</ul>
<h3 id="arts-dialect-simplifying-concurrency">
ARTS Dialect: Simplifying Concurrency
</h3>
<ul>
<li>
<strong>Custom Language Constructs:</strong> The ARTS dialect converts
high-level OpenMP tasks into a form that directly represents EDTs,
DataBlocks, events, and epochs.
</li>
<li>
<strong>Easier Analysis:</strong> This clear representation makes it
simpler to analyze and optimize the code.
</li>
</ul>
<h3 id="optimization-and-transformation-passes">
Optimization and Transformation Passes
</h3>
<ul>
<li>
<strong>EDT Optimization:</strong> We remove redundant tasks and
optimize task structures‚Äîfor example, turning a ‚Äúparallel‚Äù task that
contains only one subtask into a ‚Äúsync‚Äù task.
</li>
<li>
<strong>DataBlock Management:</strong> We analyze memory access patterns
to decide which DataBlocks are needed and optimize their usage.
</li>
<li>
<strong>Event Handling and Classic Optimizations:</strong> We allocate
and manage events, applying techniques like dead code elimination and
common subexpression elimination to clean up the code.
</li>
</ul>
<h3 id="lowering-to-llvm-ir-and-runtime-integration">
Lowering to LLVM IR and Runtime Integration
</h3>
<ul>
<li>
<strong>Conversion to LLVM IR:</strong> The ARTS-enhanced MLIR is
converted into LLVM IR. This involves outlining EDT regions into
functions and inserting ARTS API calls for task, DataBlock, epoch, and
event management.
</li>
<li>
<strong>Seamless Integration:</strong> The final binary runs on the ARTS
runtime, which schedules tasks dynamically based on data readiness.
</li>
</ul>
<h2 id="looking-ahead-future-directions-for-carts">
Looking Ahead: Future Directions for CARTS
</h2>
<p>
The journey with CARTS is just beginning. Here‚Äôs a glimpse of what‚Äôs
next:
</p>
<ul>
<li>
<strong>Comprehensive Benchmarking:</strong> Testing the infrastructure
with a variety of benchmarks to validate performance under diverse
scenarios.
</li>
<li>
<strong>Expanded OpenMP Support:</strong> Enhancing support for
additional OpenMP constructs such as loops, barriers, and locks.
</li>
<li>
<strong>Advanced Transformation Passes:</strong> Developing techniques
like dependency pruning, task splitting/fusion, and affine
transformations to further optimize task management and data locality.
</li>
<li>
<strong>Memory-Centric Optimizations:</strong> Implementing strategies
like cache-aware tiling, data partitioning, and optimized memory layouts
to reduce cache misses and enhance data transfer efficiency.
</li>
<li>
<strong>Feedback-Directed Compilation:</strong> Incorporating runtime
profiling data to adapt optimizations dynamically based on actual
workload and hardware behavior.
</li>
<li>
<strong>Domain-Specific Extensions:</strong> Creating specialized
operations for domains such as stencil computations and tensor
operations to boost performance in targeted HPC applications.
</li>
</ul>
<h2 id="wrapping-up">
Wrapping Up
</h2>
<p>
Conferences like CGO are not just about technical presentations, they‚Äôre
also about meeting people and sharing ideas. I really enjoyed the mix of
technical sessions and informal conversations.One of my favorite moments
was meeting a professor at the conference and joking about how we only
seem to meet when we‚Äôre away from Newark.It‚Äôs these human connections,
along with the valuable feedback on my work, that make attending such
events worthwhile. Here are a few personal takeaways:
</p>
<ul>
<li>
<strong>Invaluable Feedback:</strong> Presenting work-in-progress at
LLVM CGO workshops has taught me that constructive criticism is the fuel
for innovation.
</li>
<li>
<strong>Community Spirit:</strong> Reconnecting with fellow researchers,
whether through formal sessions or casual hallway conversations,
enriches both our professional and personal lives.I encourage fellow PhD
candidates and early-career researchers to take every opportunity to
present your work,your ideas might not be 100% polished, but the
community is there to help you refine them.
</li>
</ul>
<p>
Presenting CARTS allowed me to share detailed technical insights,
discuss the practical challenges of HPC, and even have a few laughs
along the way. While the technical details might seem dense at times,
Ihope the mix of personal anecdotes and hands-on explanations makes the
topic accessible and engaging.If you‚Äôre interested in discussing more
about ARTS, the Codelet model, or anything else related to HPC, please
drop me an email at (<em>rafaelhg at udel dot edu</em>). I‚Äôd love to
chat, collaborate, or simply hang out.
</p>
<h2 id="acknowledgements">
Acknowledgements
</h2>
<ul>
<li>
This work is supported by the US DOE Office of Science project ‚ÄúAdvanced
Memory to Support Artificial Intelligence for Science‚Äù at PNNL. PNNL is
operated by Battelle Memorial Institute under Contract DEAC06-76RL01830.
</li>
<li>
Thanks to the LLVM Foundation for the travel award that made attending
the CGO conference possible.
</li>
</ul>
<p><br></p>
<p><a href="https://blog.llvm.org/posts/2025-03-26-llvmcgo-carts/"
class="uri">https://blog.llvm.org/posts/2025-03-26-llvmcgo-carts/</a></p>
<hr />
<h2
id="the-chatgpt-sharing-dialog-demonstrates-how-difficult-it-is-to-design-privacy-preferences">The
ChatGPT sharing dialog demonstrates how difficult it is to design
privacy preferences</h2>
<p>date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs
Weblog</p>
<p>
ChatGPT just removed their ‚Äúmake this chat discoverable‚Äù sharing
feature, after it turned out a material volume of users had
inadvertantly made their private chats available via Google search.
</p>
<p>
Dane Stuckey, CISO for OpenAI,
<a href="https://x.com/cryps1s/status/1951041845938499669">on
Twitter</a>:
</p>
<blockquote>
<p>
We just removed a feature from <span class="citation"
data-cites="ChatGPTapp">@ChatGPTapp</span> that allowed users to make
their conversations discoverable by search engines, such as Google. This
was a short-lived experiment to help people discover useful
conversations. [‚Ä¶]
</p>
<p>
Ultimately we think this feature introduced too many opportunities for
folks to accidentally share things they didn‚Äôt intend to, so we‚Äôre
removing the option.
</p>
</blockquote>
<p>
There‚Äôs been some media coverage of this issue - here are examples from
<a href="https://techcrunch.com/2025/07/31/your-public-chatgpt-queries-are-getting-indexed-by-google-and-other-search-engines/">TechCrunch</a>,
<a href="https://www.techradar.com/ai-platforms-assistants/chatgpt/openai-pulls-chat-sharing-tool-after-google-search-privacy-scare">TechRadar</a>,
and
<a href="https://www.pcmag.com/news/be-careful-what-you-tell-chatgpt-your-chats-could-show-up-on-google-search">PCMag</a>.
</p>
<p>
It turned out users had shared extremely private conversations and made
them discoverable by search engines, which meant that various
<code>site:chatgpt.com ‚Ä¶</code> searches were turning up all sorts of
potentially embarrassing details.
</p>
<p>
Here‚Äôs what that UI looked like before they removed the option:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/chatgpt-share.jpg" alt="Screenshot of a chat sharing dialog with title &quot;Public link created&quot; and X close button. Text reads &quot;A public link to your chat has been created. Manage previously shared chats at any time via Settings.&quot; Below is an unchecked checkbox labeled &quot;Make this chat discoverable&quot; with subtitle &quot;Allows it to be shown in web searches&quot;. The sharing URL shown is &quot;https://chatgpt.com/share/688b95ef-f986&quot; with a black &quot;Copy link&quot; button. At bottom are three social sharing icons for LinkedIn, Reddit, and X." style="max-width: 100%;" />
</p>
<p>
I‚Äôve seen a bunch of commentary, both on Twitter and
<a href="https://news.ycombinator.com/item?id=44778764">this Hacker News
thread</a>, from people who are baffled that anyone could be confused by
such a clear option in the UI.
</p>
<p>
I think that confusion is warranted. Let‚Äôs break it down.
</p>
<p>
Here‚Äôs the microcopy in question:
</p>
<blockquote>
<p>
<strong>Make this chat discoverable</strong><br /> Allows it to be shown
in web searches.
</p>
</blockquote>
<p>
The first problem here is the choice of terminology. ‚ÄúDiscoverable‚Äù is
not a widely understood term - it‚Äôs insider jargon. ‚ÄúAllows it to be
shown in web searches‚Äù is better, but still requires a surprisng depth
of understanding from users before they can make an informed decision.
</p>
<p>
Here‚Äôs everything a user would need to understand for this to make sense
to them:
</p>
<ul>
<li>
What a URL is, and how it‚Äôs posssible to create a URL that is
semi-public in that it‚Äôs unguessable by others but can still be read by
anyone you share it with. That concept is a pretty tall order just on
its own!
</li>
<li>
What a web search engine is - that in this case it‚Äôs intended as a
generic term for Google, Bing, DuckDuckGo etc.
</li>
<li>
That ‚Äúweb search‚Äù here means ‚Äúthose public search engines other people
can use‚Äù and not something like ‚Äúthe private search feature you use on
this website‚Äù.
</li>
<li>
A loose understanding of how search engines work: that they have
indexes, and those indexes can selectively include or exclude content.
</li>
<li>
That sites like ChatGPT get to control whether or not their content is
included in those indexes.
</li>
<li>
That the nature of a ‚Äúsecret URL‚Äù is that, once shared and made
discoverable, anyone with that link (or who finds it through search) can
now view the full content of that page.
</li>
</ul>
<p>
ChatGPT has over a billion users now. That means there is a giant range
of levels of technical expertise among those users. We can‚Äôt assume that
everyone understands the above concepts necessary to understand the
implications of checking that box.
</p>
<p>
And even if they have the pre-requisite knowledge required to understand
this, <strong>users don‚Äôt read</strong>.
</p>
<p>
When people are using an application they are always looking for the
absolute shortest path to achieving their goal. Any dialog box or
question that appears is something to be skipped over as quickly as
possible.
</p>
<p>
Sadly, a lot of users may have learned to just say ‚Äúyes‚Äù to any
question. This option about making something ‚Äúdiscoverable‚Äù? Sure,
whatever, click the box and keep on going.
</p>
<p>
I think there‚Äôs another factor at play here too: the option itself makes
almost no sense.
</p>
<p>
How many people looking for a way to share their chats are going to
think ‚Äúand you know what? Stick this in Google too‚Äù?
</p>
<p>
It‚Äôs such a tiny fraction of the audience that a logical conclusion,
when faced with the above option, could well be that obviously it
wouldn‚Äôt put my chats in Google because who on Earth would ever want
that to happen?
</p>
<p>
I think OpenAI made the right call disabling this feature. The value it
can provide for the tiny set of people who decide to use it is massively
outweighed by the potential for less discerning users to cause
themselves harm by inadvertently sharing their private conversations
with the world.
</p>
<h4 id="meta-ai-does-this-even-worse">
Meta AI does this even worse
</h4>
<p>
A much worse example of this anti-pattern is Meta AI‚Äôs decision to
provide a ‚ÄúPost to feed‚Äù button in their own Meta AI chat app:
</p>
<p>
<img src="https://static.simonwillison.net/static/2025/meta-ai-share.jpg" alt="Sharing dialog has two options: Post to feed - share this conversation to the public feed so anyone can see it and engage. and Share a link: Create a link to share this conversation with specific people." style="max-width: 100%;" />
</p>
<p>
I think their microcopy here is <em>top notch</em> - the text here uses
clear language and should be easy for anyone to understand.
</p>
<p>
(I took this screenshot today though, so it‚Äôs possible the text has been
recently updated.)
</p>
<p>
And yet‚Ä¶ Futurism, June 14th:
<a href="https://futurism.com/meta-ai-embarassing">People Don‚Äôt Realize
Meta‚Äôs AI App Is Publicly Blasting Their Humiliating Secrets to the
World</a>.
</p>
<p>
Once again, when your users number in the millions some of them are
going to randomly click things without understanding the consequences.
</p>
<p>
The Meta AI iPhone app (fun fact: it can talk to you in the voice of
Dame Judi Dench or John Cena) shows that public feed on the homepage
when you first open the app, presumably to try and help people get over
the blank slate ‚Äúwhat is this thing even for‚Äù problem. They do not
appear keen on losing this feature!
</p>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/design&quot;&gt;design&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/privacy&quot;&gt;privacy&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/usability&quot;&gt;usability&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/openai&quot;&gt;openai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/chatgpt&quot;&gt;chatgpt&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/meta&quot;&gt;meta&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/3/privacy-design/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/3/privacy-design/#atom-everything</a></p>
<hr />
<h2 id="high-quality-offline-music">High Quality Offline Music</h2>
<p>date: 2025-08-03, from: mrusme blog</p>
<p>A brief overview of how to enjoy high quality music without
subscribing to a privacy-invasive and usually lower-quality music
streaming service like Spotify, YouTube Music, Deezer, etc.</p>
<p><br></p>
<p><a href="https://xn--gckvb8fzb.com/high-quality-offline-music/"
class="uri">https://xn--gckvb8fzb.com/high-quality-offline-music/</a></p>
<hr />
<h2 id="xbai-o4">XBai o4</h2>
<p>date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://huggingface.co/MetaStoneTec/XBai-o4">XBai
o4</a></strong>
</p>
Yet <em>another</em> open source (Apache 2.0) LLM from a Chinese AI lab.
This model card claims:
</p>
<blockquote>
<p>
<strong>XBai o4</strong> excels in complex reasoning capabilities and
has now completely surpassed OpenAI-o3-mini in Medium mode.
</p>
</blockquote>
<p>
This a 32.8 billion parameter model released by MetaStone AI, a
new-to-me lab who released their first model in March -
<a href="https://huggingface.co/MetaStoneTec/MetaStone-L1-7B">MetaStone-L1-7B</a>,
then followed that with MetaStone-S1
<a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-1.5B">1.5B</a>,
<a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-7B">7B</a> and
<a href="https://huggingface.co/MetaStoneTec/MetaStone-S1-32B">32B</a>
in July and now XBai o4 in August.
</p>
<p>
The MetaStone-S1 models were accompanied with a with a paper,
<a href="https://arxiv.org/abs/2507.01951">Test-Time Scaling with
Reflective Generative Model</a>.
</p>
<p>
There is <em>very</em> little information available on the
English-language web about MetaStone AI. Their paper shows a
relationship with USTC,
<a href="https://en.wikipedia.org/wiki/University_of_Science_and_Technology_of_China">University
of Science and Technology of China</a> in Hefei. One of their
researchers
<a href="https://x.com/WangMagic_/status/1951690465222217872">confirmed
on Twitter</a> that their CEO is from
<a href="https://en.wikipedia.org/wiki/Kuaishou">KWAI</a> which lead me
to <a href="https://www.qbitai.com/2024/07/168071.html">this Chinese
language article</a> from July last year about Li Yan, formerly of KWAI
and now the founder of Wen Xiaobai and
<a href="https://x.com/simonw/status/1951694450369208361">evidently</a>
<a href="https://x.com/WangMagic_/status/1951694611191324929">now</a>
the CEO of MetaStone.
<a href="https://www.wenxiaobai.com">www.wenxiaobai.com</a> is listed as
the ‚Äúofficial website‚Äù linked to from
<a href="https://github.com/MetaStone-AI/XBai-o4">the XBai-o4 README</a>
on GitHub.
</p>
<p>
Ivan Fioravanti
<a href="https://huggingface.co/mlx-community/models?search=xbai-o4">got
it working under MLX</a> in 4bit, 5bit, 6bit, 8bit and 4bit-DWQ sizes. I
tried his
<a href="https://huggingface.co/mlx-community/XBai-o4-6bit">6bit one</a>
(a 24.81GB download) in LM Studio and had it ‚ÄúGenerate an SVG of a
pelican riding a bicycle‚Äù. It thought for 39.49 seconds
(<a href="https://gist.github.com/simonw/78182fc3409e36f8d22217992967b9d6#response">thinking
trace here</a>) and gave me this:
</p>
<p>
<img alt="The pelican is blue and looks a little like a sad elephant. The bicycle looks more like a flat motorbike. The pelican has a blue tail and orange legs." src="https://static.simonwillison.net/static/2025/xbai-o4-pelican.png" />
</p>
<p>
I also had it ‚ÄúWrite an HTML and JavaScript page implementing space
invaders‚Äù, which you can
<a href="https://tools.simonwillison.net/space-invaders-xbai-o4-6bit-mlx">try
out here</a>. It‚Äôs not great - the invaders don‚Äôt fire bullets (it
listed that as a ‚Äúpotential improvement‚Äù) and I don‚Äôt think it‚Äôs
winnable due to timing issues, but it‚Äôs an interesting addition to
<a href="https://simonwillison.net/tags/space-invaders/">my
collection</a>.
</p>
<p>
LM Studio reported a constant memory usage of 25.54GB and according to
Activity Monitor used 87.5% of my GPU and around 39% of a CPU core.
</p>
<p>
It did a <em>lot</em> of work in the thinking trace - which took 10m35s!
- including writing the first draft of much of the code that ended up in
the finished result. Here‚Äôs an extract:
</p>
<blockquote>
<p>
Let me think of variables:
</p>
<p>
<code>let enemyDirection = 1; // 1 for right, -1 for left</code>
</p>
<p>
In the game loop: <code>enemies.forEach(enemy =&gt; {&gt; enemy.x +=
enemyDirection * enemy.dx;&gt; });</code> Then check if any enemy is out
of bounds. If so, reverse direction and move all enemies down.
</p>
<p>
But how to check if any enemy is at the edge?
</p>
<p>
Maybe find the max x of all enemies and see if it exceeds
<code>canvas.width - enemy.width</code>.
</p>
<p>
Alternatively, track the leftmost and rightmost positions of enemies.
Maybe a bit complex.
</p>
<p>
For simplicity, perhaps check if any enemy‚Äôs <code>x + width &gt;
canvas.width or x &lt; 0</code>.
</p>
</blockquote>
<p>
Here‚Äôs
<a href="https://gist.github.com/simonw/457d55a5c426e0bbea9d47aee225c391">the
full transcript</a>.
</p>
<p>
My initial impressions are that I‚Äôm not as impressed with this model for
running on my own laptop as I was with
<a href="https://simonwillison.net/2025/Jul/31/qwen3-coder-flash/">Qwen3-Coder-30B-A3B-Instruct</a>
or
<a href="https://simonwillison.net/2025/Jul/29/space-invaders/">GLM-4.5
Air</a>.
</p>
<p>
<p>But‚Ä¶ how extraordinary is it that <em>another</em> Chinese AI lab has
been able to produce a competitive model, this time with far less
fanfare than we‚Äôve seen from Qwen and Moonshot AI and Z.ai.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://x.com/ivanfioravanti/status/1951643205985816807&quot;&gt;@ivanfioravanti&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/mlx&quot;&gt;mlx&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/pelican-riding-a-bicycle&quot;&gt;pelican-riding-a-bicycle&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-reasoning&quot;&gt;llm-reasoning&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llm-release&quot;&gt;llm-release&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/lm-studio&quot;&gt;lm-studio&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai-in-china&quot;&gt;ai-in-china&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/space-invaders&quot;&gt;space-invaders&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/3/xbai-o4/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/3/xbai-o4/#atom-everything</a></p>
<hr />
<h2 id="ai-agents-have-so-far-mostly-been-a-dud">AI Agents have, so far,
mostly been a dud</h2>
<p>date: 2025-08-03, from: Gary Marcus blog</p>
<p>Last year, big tech couldn‚Äôt stop talking about how AI ‚Äúagents‚Äù would
be the next big thing in 2025. It hasn‚Äôt quite turned out that way.</p>
<p><br></p>
<p><a
href="https://garymarcus.substack.com/p/ai-agents-have-so-far-mostly-been"
class="uri">https://garymarcus.substack.com/p/ai-agents-have-so-far-mostly-been</a></p>
<hr />
<h2 id="from-asyncawait-to-virtual-threads">From Async/Await to Virtual
Threads</h2>
<p>date: 2025-08-03, updated: 2025-08-03, from: Simon Willison‚Äôs
Weblog</p>
<p>
<strong><a href="https://lucumr.pocoo.org/2025/7/26/virtual-threads/">From
Async/Await to Virtual Threads</a></strong>
</p>
Armin Ronacher has long been critical of async/await in Python, both for
necessitating
<a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">colored
functions</a> and because of the more subtle challenges they introduce
like
<a href="https://lucumr.pocoo.org/2020/1/1/async-pressure/">managing
back pressure</a>.
</p>
<p>
Armin
<a href="https://lucumr.pocoo.org/2024/11/18/threads-beat-async-await/">argued
convincingly</a> for the threaded programming model back in December.
Now he‚Äôs expanded upon that with a description of how virtual threads
might make sense in Python.
</p>
<p>
Virtual threads behave like real system threads but can vastly outnumber
them, since they can be paused and scheduled to run on a real thread
when needed. Go uses this trick to implement goroutines which can then
support millions of virtual threads on a single system.
</p>
<p>
Python core developer Mark Shannon
<a href="https://discuss.python.org/t/add-virtual-threads-to-python/91403">started
a conversation</a> about the potential for seeing virtual threads to
Python back in May.
</p>
<p>
<p>Assuming this proposal turns into something concrete I don‚Äôt expect
we will see it in a production Python release for a few more years. In
the meantime there are some exciting improvements to the Python
concurrency story - most notably
<a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-pep734">around
sub-interpreters</a> - coming up this year in Python 3.14.</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/armin-ronacher&quot;&gt;armin-ronacher&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/concurrency&quot;&gt;concurrency&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/gil&quot;&gt;gil&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/go&quot;&gt;go&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/python&quot;&gt;python&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/threads&quot;&gt;threads&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2025/Aug/3/virtual-threads/#atom-everything"
class="uri">https://simonwillison.net/2025/Aug/3/virtual-threads/#atom-everything</a></p>
<hr />
<h2
id="gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel">GMK
NucBox K12 mini PC with AMD Ryzen 7 H 255 is like a cheaper EVO-T1 (with
AMD instead of Intel)</h2>
<p>date: 2025-08-03, from: Liliputing</p>
<p>
The¬†GMK NucBox K12¬†is a small desktop computer with an OCuLink port for
a high-speed connection to an external graphics dock or other
accessories, support for up to 128GB of DDR5-5600 memory, up to three
SSDs, and dual 2.5 Gb Ethernet ports. If all of that sounds familiar,
that‚Äôs because it could also describe the GMK [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel/">GMK
NucBox K12 mini PC with AMD Ryzen 7 H 255 is like a cheaper EVO-T1 (with
AMD instead of Intel)</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel/"
class="uri">https://liliputing.com/gmk-nucbox-k12-mini-pc-with-amd-ryzen-7-h-255-is-like-a-cheaper-evo-t1-with-amd-instead-of-intel/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Robert">@Robert</span>‚Äôs
feed at BlueSky</strong> (date: 2025-08-03, from: Robert‚Äôs feed at
BlueSky)</p>
<p>I am happy this is happening.</p>
<p>[contains quote post or other embedded content]</p>
<p><br></p>
<p><a
href="https://bsky.app/profile/rsdoiel.bsky.social/post/3lvj2lbommk2b"
class="uri">https://bsky.app/profile/rsdoiel.bsky.social/post/3lvj2lbommk2b</a></p>
<hr />
<h2
id="gpd-win-5-handheld-gaming-pc-specs-performance-preview-strix-halo-processor-7-inch-display-and-no-keyboard">GPD
Win 5 handheld gaming PC specs &amp; performance preview: Strix Halo
processor, 7 inch display, and no keyboard</h2>
<p>date: 2025-08-03, from: Liliputing</p>
<p>
This week GPD revealed that its next handheld gaming PC would be powered
by an AMD Strix Halo processor, bringing discrete-class graphics to a
handheld PC for the first time. But at the time the company didn‚Äôt share
many other details about the upcoming GPD Win 5, which left questions
about battery life and overall [‚Ä¶]
</p>
<p>
The post
<a href="https://liliputing.com/gpd-win-5-handheld-gaming-pc-specs-revealed-strix-halo-processor-7-inch-display-and-no-keyboard/">GPD
Win 5 handheld gaming PC specs &amp; performance preview: Strix Halo
processor, 7 inch display, and no keyboard</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/gpd-win-5-handheld-gaming-pc-specs-revealed-strix-halo-processor-7-inch-display-and-no-keyboard/"
class="uri">https://liliputing.com/gpd-win-5-handheld-gaming-pc-specs-revealed-strix-halo-processor-7-inch-display-and-no-keyboard/</a></p>
<hr />
<h2 id="the-economics-of-smoot-hawley-2.0-part-i">The Economics of
Smoot-Hawley 2.0, Part I</h2>
<p>date: 2025-08-03, from: Paul Krugman</p>
<p>Tariffs will be very high as far as the eye can see. What does that
mean?</p>
<p><br></p>
<p><a
href="https://paulkrugman.substack.com/p/the-economics-of-smoot-hawley-20"
class="uri">https://paulkrugman.substack.com/p/the-economics-of-smoot-hawley-20</a></p>
<hr />
<h2
id="creating-a-toy-programming-language-with-actor-based-parallelism">Creating
a Toy Programming Language with Actor-Based Parallelism</h2>
<p>date: 2025-08-03, from: Pointers gone wild blog</p>
<p><br></p>
<p><a
href="https://pointersgonewild.com/2025-08-03-creating-a-toy-language-with-actor-based-parallelism/"
class="uri">https://pointersgonewild.com/2025-08-03-creating-a-toy-language-with-actor-based-parallelism/</a></p>
</section>
<footer>
Antenna is a personal aggregation of items found around the web.
Curated with <a href="https://rsdoiel.github.io/skimmer">skimmer</a> and <a href="https://sqlite.org">sqlite</a> then rendered with <a href="https://pandoc.org">Pandoc</a>.
</footer>
</body>
</html>
