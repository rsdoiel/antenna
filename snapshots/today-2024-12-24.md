---
title: News gathered 2024-12-24
updated: 2024-12-24 07:09:28
---

# News gathered 2024-12-24

(date: 2024-12-24 07:09:28)

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

Whatever forces you into copying and pasting into tiny little text boxes, that&#39;s how you know you&#39;re in a silo. 

<br> 

<http://scripting.com/2024/12/24.html#a141952>

---

## Tipster Arrested After Feds Find AI Child Exploit Images and Plans to Make VR CSAM

date: 2024-12-24, from: 404 Media Group

A review of a tipster's phone revealed 'real' CSAM mixed in with AI CSAM, and a darker involvement in the case.  

<br> 

<https://www.404media.co/tipster-arrested-after-feds-find-ai-child-exploit-images-and-plans-to-make-vr-csam-2/>

---

## HO, HO, HO - WHAT'S SO FUNNY ABOUT AN OVERWEIGHT OLD MAN IN A WHITE BEARD? 

date: 2024-12-24, from: Howard Jacobson blog

Robert Cenedella 

<br> 

<https://jacobsonh.substack.com/p/ho-ho-ho-whats-so-funny-about-an>

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

My favorite news sources, pros and bloggers, podcasts, culture, business, tech, politics. 

<br> 

<https://news.scripting.com/>

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

Ars Technica picks for TV of 2024. 

<br> 

<https://arstechnica.com/culture/2024/12/tv-technica-2024-our-picks-for-the-best-of-tv/>

---

## Quoting Jeremy Edberg

date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s Weblog

<blockquote cite="https://news.ycombinator.com/item?id=42486610#42492484"><p>[On Reddit] we had to look up every single comment on the page to see if you had voted on it [...]</p>
<p>But with a bloom filter, we could very quickly look up all the comments and get back a list of all the ones you voted on (with a couple of false positives in there). Then we could go to the cache and see if your actual vote was there (and if it was an upvote or a downvote). It was only after a failed cache hit did we have to actually go to the database.</p>
<p>But that bloom filter saved us from doing sometimes 1000s of cache lookups.</p></blockquote>
<p class="cite">&mdash; <a href="https://news.ycombinator.com/item?id=42486610#42492484">Jeremy Edberg</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/reddit">reddit</a>, <a href="https://simonwillison.net/tags/bloom-filters">bloom-filters</a>, <a href="https://simonwillison.net/tags/scaling">scaling</a></p> 

<br> 

<https://simonwillison.net/2024/Dec/24/jeremy-edberg/#atom-everything>

---

## Finally, a replacement for BERT: Introducing ModernBERT

date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s Weblog

<p><strong><a href="https://www.answer.ai/posts/2024-12-19-modernbert.html">Finally, a replacement for BERT: Introducing ModernBERT</a></strong></p>
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> was an early language model released by Google in October 2018. Unlike modern LLMs it wasn't designed for generating text. BERT was trained for masked token prediction and was generally applied to problems like Named Entity Recognition or Sentiment Analysis. BERT also wasn't very useful on its own - most applications required you to fine-tune a model on top of it.</p>
<p>In exploring BERT I decided to try out <a href="https://huggingface.co/dslim/distilbert-NER">dslim/distilbert-NER</a>, a popular Named Entity Recognition model fine-tuned on top of DistilBERT (a smaller distilled version of the original BERT model). <a href="https://til.simonwillison.net/llms/bert-ner">Here are my notes</a> on running that using <code>uv run</code>.</p>
<p>Jeremy Howard's <a href="https://www.answer.ai/">Answer.AI</a> research group, <a href="https://www.lighton.ai/">LightOn</a> and friends supported the development of ModernBERT, a brand new BERT-style model that applies many enhancements from the past six years of advances in this space.</p>
<p>While BERT was trained on 3.3 billion tokens, producing 110 million and 340 million parameter models, ModernBERT trained on 2 trillion tokens, resulting in 140 million and 395 million parameter models. The parameter count hasn't increased much because it's designed to run on lower-end hardware. It has a 8192 token context length, a significant improvement on BERT's 512.</p>
<p>I was able to run one of the demos from the announcement post using <code>uv run</code> like this (I'm not sure why I had to use <code>numpy&lt;2.0</code> but without that I got an error about <code>cannot import name 'ComplexWarning' from 'numpy.core.numeric'</code>):</p>
<div class="highlight highlight-source-shell"><pre>uv run --with <span class="pl-s"><span class="pl-pds">'</span>numpy&lt;2.0<span class="pl-pds">'</span></span> --with torch --with <span class="pl-s"><span class="pl-pds">'</span>git+https://github.com/huggingface/transformers.git<span class="pl-pds">'</span></span> python</pre></div>
<p>Then this Python:</p>
<pre><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-s1">pipeline</span>
<span class="pl-k">from</span> <span class="pl-s1">pprint</span> <span class="pl-k">import</span> <span class="pl-s1">pprint</span>
<span class="pl-s1">pipe</span> <span class="pl-c1">=</span> <span class="pl-en">pipeline</span>(
    <span class="pl-s">"fill-mask"</span>,
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s">"answerdotai/ModernBERT-base"</span>,
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">bfloat16</span>,
)
<span class="pl-s1">input_text</span> <span class="pl-c1">=</span> <span class="pl-s">"He walked to the [MASK]."</span>
<span class="pl-s1">results</span> <span class="pl-c1">=</span> <span class="pl-en">pipe</span>(<span class="pl-s1">input_text</span>)
<span class="pl-en">pprint</span>(<span class="pl-s1">results</span>)</pre>
<p>Which downloaded 573MB to <code>~/.cache/huggingface/hub/models--answerdotai--ModernBERT-base</code> and output:</p>
<pre>[{<span class="pl-s">'score'</span>: <span class="pl-c1">0.11669921875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the door.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3369</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' door'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.037841796875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the office.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3906</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' office'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.0277099609375</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the library.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">6335</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' library'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.0216064453125</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the gate.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">7394</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' gate'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.020263671875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the window.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3497</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' window'</span>}]</pre>

<p>I'm looking forward to trying out models that use ModernBERT as their base. The model release is accompanied by a paper (<a href="https://arxiv.org/abs/2412.13663">Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</a>) and <a href="https://huggingface.co/docs/transformers/main/en/model_doc/modernbert">new documentation</a> for using it with the Transformers library.

    <p><small></small>Via <a href="https://bsky.app/profile/benjaminwarner.dev/post/3ldur45oz322b">@benjaminwarner.dev</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/jeremy-howard">jeremy-howard</a>, <a href="https://simonwillison.net/tags/transformers">transformers</a>, <a href="https://simonwillison.net/tags/hugging-face">hugging-face</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/nlp">nlp</a></p> 

<br> 

<https://simonwillison.net/2024/Dec/24/modernbert/#atom-everything>

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

Tesla&#39;s 2024 deliveries growth might hinge on Musk&#39;s unorthodox Cybertruck. 

<br> 

<https://www.reuters.com/business/autos-transportation/teslas-2024-deliveries-growth-might-hinge-musks-unorthodox-cybertruck-2024-12-20/>

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

Trump&#39;s Plan to Make America a Global Bully. 

<br> 

<https://www.theatlantic.com/magazine/archive/2025/01/trump-foreign-policy-isolation/680754/?gift=f35zZN0v_gDFE8xNwlQAHQ8w2Hd1-0-zPQQVqe3_z0Y>

---

**@Dave Winer's linkblog** (date: 2024-12-24, from: Dave Winer's linkblog)

Interesting way to sell an E-car. 🤪 

<br> 

<https://m.youtube.com/watch?v=LxtJ5yi_yps>

---

## Explainer: Latent Space Experts

date: 2024-12-24, updated: 2024-12-24, from: Tom Kellog blog

A new paper just dropped from Google DeepMind, Deliberation in Latent Space via Differentiable Cache Augmentation.
I don’t think this paper is very readable, but it also seems quite important so I wanted to take a moment
to break it down, as I understand it. 

<br> 

<http://timkellogg.me/blog/2024/12/24/latent-experts>

---

**@Tomosino's Mastodon feed** (date: 2024-12-23, from: Tomosino's Mastodon feed)

<p>Read a nasty critic review of a concert, got mad, and wrote this. Enjoy</p><p><a href="https://blog.tomasino.org/five-stars/" target="_blank" rel="nofollow noopener noreferrer" translate="no"><span class="invisible">https://</span><span class="">blog.tomasino.org/five-stars/</span><span class="invisible"></span></a></p> 

<br> 

<https://tilde.zone/@tomasino/113704671848614521>

---

**@Dave Winer's linkblog** (date: 2024-12-23, from: Dave Winer's linkblog)

Portion of Santa Cruz wharf collapses amid dangerous surf. 

<br> 

<https://lookout.co/portion-of-santa-cruz-wharf-collapses-amid-dangerous-surf-prompting-multiple-water-rescues/>

---

## Compare handheld gaming PC specs (Asus, AYA, GPD, Lenovo, MSI, ONEXPLAYER, and Valve)

date: 2024-12-23, from: Liliputing

<p>There are a growing number of handheld gaming PCs on the market, and the list keeps growing. While the Valve Steam Deck, Asus ROG Ally, and Lenovo Legion Go grab a lot of headlines these days, they&#8217;re not exactly the only games in town &#8211; not by a long shot. In fact, there are so [&#8230;]</p>
<p>The post <a href="https://liliputing.com/compare-handheld-gaming-pc-specs-anbernic-ayn-aya-gpd-onexplayer-and-valve/">Compare handheld gaming PC specs (Asus, AYA, GPD, Lenovo, MSI, ONEXPLAYER, and Valve)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/compare-handheld-gaming-pc-specs-anbernic-ayn-aya-gpd-onexplayer-and-valve/>

---

## Deleting Unused Photos From Apple Photos

date: 2024-12-23, from: Michael Tsai

I&#8217;ve been trying to reduce the storage that the Photos app uses, both on my Mac and in iCloud. I use Lightroom for my photo library, so I would like to delete all the photos that are not referenced by projects (calendars and photo books). Unfortunately, Photos is unable to display any of my projects [&#8230;] 

<br> 

<https://mjtsai.com/blog/2024/12/23/deleting-unused-photos-from-apple-photos/>

---

## Meta’s iOS Interoperability Requests

date: 2024-12-23, from: Michael Tsai

Juli Clover: Apple today said that Meta has made 15 interoperability requests under the Digital Markets Act (DMA) in the European Union, which is more than any other company.In a statement provided to Reuters, Apple said that Meta is asking for changes that could compromise user security and privacy.[&#8230;]In response to Apple&#8217;s comments on Meta&#8217;s [&#8230;] 

<br> 

<https://mjtsai.com/blog/2024/12/23/metas-ios-interoperability-requests/>

---

## WhatsApp v. NSO Group

date: 2024-12-23, from: Michael Tsai

Reuters (via Hacker News, Court Listener): U.S. judge ruled on Friday in favor of Meta Platforms&#8217;, WhatsApp in a lawsuit accusing Israel&#8217;s NSO Group of exploiting a bug in the messaging app to install spy software allowing unauthorized surveillance.[&#8230;]WhatsApp in 2019 sued NSO seeking an injunction and damages, accusing it of accessing WhatsApp servers without [&#8230;] 

<br> 

<https://mjtsai.com/blog/2024/12/23/whatsapp-v-nso-group/>

---

## Donald Bitzer, RIP

date: 2024-12-23, from: Michael Tsai

Dag Spicer (via Hacker News): Bitzer studied electrical engineering at the University of Illinois at Urbana-Champaign (UIUC), obtaining a PhD in 1960. Following graduation, he joined the UIUC faculty, where he learned of efforts to bring lessons to students over a closed-circuit television network. While a committee of engineers, psychologists, and educators were unable to [&#8230;] 

<br> 

<https://mjtsai.com/blog/2024/12/23/donald-bitzer-rip/>

---

## MINISFORUM V3 SE is a cheaper 3-in-1 tablet with an older AMD Ryzen processor

date: 2024-12-23, from: Liliputing

<p>The MINISFORUM V3 that launched earlier this year is a 14 inch tablet with a 2560 x 1600 pixel, 165 Hz, 500 nit display and an AMD Ryzen 7 8840U processor. It was one of the first tablets with an AMD &#8220;Hawk Point&#8221; processor. But it&#8217;s not exactly cheap: it still sells for $999 and up even [&#8230;]</p>
<p>The post <a href="https://liliputing.com/minisforum-v3-se-is-a-cheaper-3-in-1-tablet-with-an-older-amd-ryzen-processor/">MINISFORUM V3 SE is a cheaper 3-in-1 tablet with an older AMD Ryzen processor</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/minisforum-v3-se-is-a-cheaper-3-in-1-tablet-with-an-older-amd-ryzen-processor/>

---

## MeLE’s Overclock X5 is a compact computer with a 45 watt Intel Alder Lake processor

date: 2024-12-23, from: Liliputing

<p>The MeLE Overclock line of computers are small systems with enhanced cooling that allow laptop-class processors to run at higher-than-typical power limits. Up until recently MeLE had mostly focused on releasing models with low-power chips in the 6 to 15 watt range, like the Intel Celeron N5095, Intel Processor N100, or Intel Core i3-N300. But now the [&#8230;]</p>
<p>The post <a href="https://liliputing.com/meles-overclock-x5-is-a-compact-computer-with-a-45-watt-intel-alder-lake-processor/">MeLE&#8217;s Overclock X5 is a compact computer with a 45 watt Intel Alder Lake processor</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/meles-overclock-x5-is-a-compact-computer-with-a-45-watt-intel-alder-lake-processor/>

---

## For-Profit Healthcare Is Fundamentally Flawed

date: 2024-12-23, updated: 2024-12-23, from: One Foot Tsunami

 

<br> 

<https://onefoottsunami.com/2024/12/23/for-profit-healthcare-is-fundamentally-flawed/>

---

**@Dave Winer's linkblog** (date: 2024-12-23, from: Dave Winer's linkblog)

AOC Perfectly Sums Up the Big Problem in Shutdown Battle. 

<br> 

<https://www.yahoo.com/news/aoc-perfectly-sums-big-problem-215748185.html>

---

**@Dave Winer's linkblog** (date: 2024-12-23, from: Dave Winer's linkblog)

Biden announces 37 death row commutations. 

<br> 

<https://www.npr.org/2024/12/23/g-s1-38794/biden-death-row-commutations>

---

## AUTHORS ALLIANCE SUBMITS AMICUS BRIEF IN SEDLIK v. DRACHENBERG

date: 2024-12-23, from: Authors Union blogs

The case Sedlik v. Drachenberg, currently pending before the 9th Circuit, presents the 9th Circuit a first opportunity to interpret the fair use right in the wake of the Warhol decision. Anticipating the far-reaching consequences for artists and authors, Authors Alliance filed an amicus brief in support of KVD. In our brief, we explained that (1) a distinct purpose is required for the first factor to tilt in favor of fair use, (2) a successful social media presence does not automatically render all postings “commercial,” and (3) concrete evidence is needed to prove the existence of a licensing market or the likelihood of it developing.  

<br> 

<https://www.authorsalliance.org/2024/12/23/authors-alliance-submits-amicus-brief-in-sedlik-v-drachenberg/>

---

## Lenovo Yoga Slim 9 14 (2025) laptop could have a hidden camera

date: 2024-12-23, from: Liliputing

<p>Lenovo may be preparing to launch a thin and light laptop with a hidden camera. WalkingCat has shared a set of leaked pictures and a short promotional video for a new version of the Lenovo Yoga Slim 9 14 inch laptop featuring an Intel Core Ultra processor and what appears to be a 14 inch [&#8230;]</p>
<p>The post <a href="https://liliputing.com/lenovo-yoga-slim-9-14-2025-laptop-could-have-a-hidden-camera/">Lenovo Yoga Slim 9 14 (2025) laptop could have a hidden camera</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/lenovo-yoga-slim-9-14-2025-laptop-could-have-a-hidden-camera/>

---

## ScummVM brings classic PC games to SailfishOS (Linux-based smartphone OS)

date: 2024-12-23, from: Liliputing

<p>ScummVM is a free and open source tool that makes it possible to run classic PC games on a wide variety of modern platforms. The software was originally designed to recreate the LucasArts game engine used for adventure games like Maniac Mansion and The Secrete of Monkey Island, but over the past two decades it&#8217;s [&#8230;]</p>
<p>The post <a href="https://liliputing.com/scummvm-brings-classic-pc-games-to-sailfishos-linux-based-smartphone-os/">ScummVM brings classic PC games to SailfishOS (Linux-based smartphone OS)</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/scummvm-brings-classic-pc-games-to-sailfishos-linux-based-smartphone-os/>

---

## MediaTek Dimensity 8400 is an all-big-core chip for sub-flagship phones and tablets

date: 2024-12-23, from: Liliputing

<p>The MediaTek Dimensity 8400 is a mobile processor featuring eight ARM Cortex-A725 CPU cores, ARM Mali-G720 MC7 graphics, and a MediaTek NPU 880 for on-device AI features. Like the Dimensity 9400 processor that launched earlier this year, the new chip is said to bring big boosts in performance and efficiency. But while the Dimensity 9400 is aimed [&#8230;]</p>
<p>The post <a href="https://liliputing.com/mediatek-dimensity-8400-is-an-all-big-core-chip-for-sub-flagship-phones-and-tablets/">MediaTek Dimensity 8400 is an all-big-core chip for sub-flagship phones and tablets</a> appeared first on <a href="https://liliputing.com">Liliputing</a>.</p>
 

<br> 

<https://liliputing.com/mediatek-dimensity-8400-is-an-all-big-core-chip-for-sub-flagship-phones-and-tablets/>

---

## Quoting Jack Clark

date: 2024-12-23, updated: 2024-12-23, from: Simon Willison’s Weblog

<blockquote cite="https://jack-clark.net/2024/12/23/import-ai-395-ai-and-energy-demand-distributed-training-via-demo-and-phi-4/"><p>There’s been a lot of strange reporting recently about how ‘scaling is hitting a wall’ – in a very narrow sense this is true in that larger models were getting less score improvement on challenging benchmarks than their predecessors, but in a larger sense this is false – techniques like those which power O3 means scaling is continuing (and if anything the curve has steepened), you just now need to account for scaling both within the training of the model and in the compute you spend on it once trained.</p></blockquote>
<p class="cite">&mdash; <a href="https://jack-clark.net/2024/12/23/import-ai-395-ai-and-energy-demand-distributed-training-via-demo-and-phi-4/">Jack Clark</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/jack-clark">jack-clark</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/inference-scaling">inference-scaling</a>, <a href="https://simonwillison.net/tags/o3">o3</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2024/Dec/23/jack-clark/#atom-everything>

---

## NCR PC4i

date: 2024-12-23, from: Computer ads from the Past

Growing with your needs 

<br> 

<https://computeradsfromthepast.substack.com/p/ncr-pc4i>

---

## Government to Name ‘Key Witness’ Who Provided FBI With Backdoored Encrypted Chat App Anom

date: 2024-12-23, from: 404 Media Group

A lawyer has pushed to learn the identity of the person who first created Anom, which the FBI used to read tens of millions of messages sent by organized criminals. The confidential human source may testify in court, too. 

<br> 

<https://www.404media.co/government-to-name-key-witness-who-provided-fbi-with-backdoored-encrypted-chat-app-anom-2/>

---

## Happy Holidays from the National Archives!

date: 2024-12-23, from: National Archives, Pieces of History blog

If you visited the National Archives Building in late 1974, you could purchase a specially designed holiday card with this design:&#160; The card showed Santa Claus and one of his reindeers looking at the Declaration of Independence, Constitution, and Bill of Rights on display in the National Archives Exhibition Hall. The design reflects what the &#8230; <a href="https://prologue.blogs.archives.gov/2024/12/23/happy-holidays-from-the-national-archives/" class="more-link">Continue reading <span class="screen-reader-text">Happy Holidays from the National Archives!</span></a> 

<br> 

<https://prologue.blogs.archives.gov/2024/12/23/happy-holidays-from-the-national-archives/>

---

## Quoting Geoffrey Litt

date: 2024-12-23, updated: 2024-12-23, from: Simon Willison’s Weblog

<blockquote cite="https://www.geoffreylitt.com/2024/12/22/making-programming-more-fun-with-an-ai-generated-debugger.html"><p>Whether you’re an AI-programming skeptic or an enthusiast, the reality is that many programming tasks are beyond the reach of today’s models. But many decent <em>dev tools</em> are actually quite easy for AI to build, and can help the rest of the programming go smoother. In general, these days any time I’m spending more than a minute staring at a JSON blob, I consider whether it’s worth building a custom UI for it.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.geoffreylitt.com/2024/12/22/making-programming-more-fun-with-an-ai-generated-debugger.html">Geoffrey Litt</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/geoffrey-litt">geoffrey-litt</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p> 

<br> 

<https://simonwillison.net/2024/Dec/23/geoffrey-litt/#atom-everything>

---

## Christmas on Repeat

date: 2024-12-23, from: Tedium site

Despite often being terrible, we watch the same holiday movies every year. Why&#039;s that? Well, it&#039;s not often about the movies—it&#039;s about our memories. 

<br> 

<https://feed.tedium.co/link/15204/16926317/christmas-nostalgia-psychology>

---

## GSoC 2024: Improve Clang Doc

date: 2024-12-23, from: LLVM Blog

<p>Hi, my name is Peter, and this year I was involved in Google Summer of Code 2024. I worked on <a href="https://discourse.llvm.org/t/improve-Clang-Doc-usability/76996">improving the Clang-Doc documenation generator</a></p><p>Mentors: Petr Hosek and Paul Kirth</p><h2 id="project-background">Project Background</h2><p>Clang-Doc is a documentation generator developed on top of libtooling, as analternative to Doxygen. Development started in 2018 and continued through 2019,however, it has since stalled. Currently, the tool can generate HTML, YAML, and markdown but the generated output has usability issues. This GSOC project aimed to address the pain points regarding the output of the HTML, by adding support for various C++ constructs and reworking the CSS of the HTML output to be more user-friendly.</p><h2 id="work-done">Work Done</h2><p>The original scope of the project was to improve the output of Clang-Doc&rsquo;s generation. However during testing the tool was significantly slower than expected which made developing features for the tool impossible.Documentation generation for the LLVM codebase was taking upwards of 10 hours on my local machine. Additionally, the tool utilized a lot of memory and was prone to crashing with an out-of-memory error. Similar tools such as Doxygen and Hdoc ran in comparatively less time for the same codebase. This pointed to a significant bottleneck within Clang-Doc’s code path when generating large-scale software projects. Due to this the project scope quickly changed to improving the runtime of Clang-Doc so that it could run much faster. It was only during the latter half of the project did the scope changed back to improving Clang-Doc’s generation.</p><h3 id="added-more-test-cases-to-clang-doc-test-suite">Added More Test Cases to Clang-Doc test suite</h3><p>Clang-Doc previously had tests which did not test the full scope of the the HTML or Markdown output. I added more end-to-end tests to make sure that in the process of optimizing documentation generation we were not degrading the quality or functionality of the tool.</p><p>In summary, I added four comprehensive tests that covered all features that we were not testing such as testing the generation for Enums, Namespace, and Records for HTML and Markdown.</p><h3 id="improve-clang-docs-performance-by-158-times">Improve Clang-Doc’s performance by 1.58 times</h3><p>Internally, the way Clang-Doc works is by leveraging libtooling&rsquo;s ASTVisitor class to parse the source level declarations in each TU.</p><p>The tool is architected using a Map-Reduce pattern. Clang-Doc parses each fragment of a declaration into an in-memory data format which is serialized then into an internal format and stored as a key value paired, identified by their <a href="https://clang.llvm.org/doxygen/group__CINDEX__CURSOR__XREF.html#ga51679cb755bbd94cc5e9476c685f2df3">USR</a>. After, Clang-Doc deserializes and combines each of the fragment declarations back into the in-memory data format which is used by each of the backend to generate the results.</p><p>Many experiments were conducted to identified the source of the bottleneck. First I tried benchmarking the code with many different codebases such JSON, and fmtlib to identify certain code patterns that slowed the code path down. This didn&rsquo;t really work since the bottlenecking only showed up for large codebases like LLVM.Next I leverage Windows prolifer (since I was coding on windows) however the visualizations was not helpful and the my system was not capable of profiling the 10 hour runtime required to compile LLVM documenation.</p><p>Eventually, we were able to identify a major bottleneck in Clang-Doc&rsquo;s by leveraging the TimeProfiler (similar to -ftime-trace in clang) code to identify where the performance bottleneck was. Clang-Doc was performing redundant work when it was processing each declaration. We settled on a caching/memoization strategy to minimize the redundant work.</p><p>For example, if we had the following project:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">//File: Base.h</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Base</span> {}</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">//File: A.cpp</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Base.h&#34;</span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span>...</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">//File: B.cpp</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Base.h&#34;</span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span></span></span><span style="display:flex;"><span>...</span></span></code></pre></div><p>In this case, the ASTVisitor invoked by Clang-Doc would visit the serialized Base class three times, once when it is parsing Base.h, another when its visiting A.cpp then B.cpp. The problem was that there was no mechanism to identify declarations that we had already seen. Using a simple dictionary which kept track of a list of declaration that Clang-Doc had visited as a basic form of memoization ended up being a surprisingly effective optimization.</p><p>Here is a plot of the benchmarking numbers:</p><div style="margin:0 auto;"> <img src="https://blog.llvm.org/img/clang-doc-benchmark-numbers.png"><br/></div><p>The benchmarking numbers were performed on a machine with a 6th gen Intel(R) Xeon(R) CPU @ 2.00GHz w/ 96 cores, and 180GB of ram. Clang-doc is able to run concurrently, however the benchmark here is with concurrency set to 2. This is because anything higher crashes the slow version of the tool with an out of memory error. It took around 6 hours to complete a full generation of LLVM documentation in the previous tool, where as current version took around 4 hours.</p><p>Here is a plot of the benchmark by number of threads:</p><div style="margin:0 auto;"> <img src="https://blog.llvm.org/img/clang-doc-concurrency.png"><br/></div><p>We notice a pretty dramatic dropoff as more and more threads are utilize, the original time t 6 hours was cut down to 13 minutes at 64 threads. Considering the previous versions of the tool could not use the higher thread count without crashing (even on a machine with 180GB of ram), the performance gains are even more dramatic.</p><h3 id="added-template-mustache-html-backend">Added Template Mustache HTML Backend</h3><p>Clang-Doc originally used an ad-hoc method of generating HTML. I introduced a templating language as a way of reducing project complexity and reducing the ease of development. Two RFCs were made before arriving at the idea of introducing Mustache as a library. Originally the idea was to introduce a custom templating language, however, upon further discussion, it was decided that the complexity of designing and implementing a new templating language was too much.An LLVM community member (<a href="https://discourse.llvm.org/u/cor3ntin/summary">@cor3ntin</a>) suggested using Mustache as a templating language.Mustache was the ideal choice since it was very simple to implement, and has a well defined spec that fit what was needed for Clang-Doc’s use case. The feedback on the <a href="https://discourse.llvm.org/t/rfc-add-template-mustache-language-to-the-support-library/82439/18">RFC</a> was generally positive. While there was some resistance regarding the inclusion of an HTML support library in LLVM, this concern stemmed partly from a lack of awareness that HTML generation already occurs in several parts of LLVM. Additionally, the introduction of Mustache has the potential to simplify other HTML-related use cases.In terms of engineering wins, this library was able to cut the direct down on the HTML backend significantly dropping 500 lines of code compared to the original Clang-Doc HTML backend. This library was also designed for general-purpose use around LLVM since there are numerous places in LLVM where various tools generate HTML in its way. Using the Mustache templating library would be a nice way to standardize the codebase.</p><h3 id="improve-clang-doc-html-output">Improve Clang-Doc HTML Output</h3><p>The previous version of Clang-Doc’s HTML output was a pretty minimal, bare bones implementation. It had a sidebar that contained every single declaration within the project which created a large unnavigable UI. Typedef documentation was missing, plus method documentation was missing details such as whether or not the method was a const or virtual. There was no linking between other declarations in the project and there was no syntax highlighting on any language construct.</p><p>With the new Mustache changes an additional backend was added using the specifier (&ndash;format=mhtml). That addresses these issues.</p><p>Below is a comparison of the same output between the two backends</p><div style="margin:0 auto;"> <img src="https://blog.llvm.org/img/Clang-Doc-old-html-output.png"><br/></div><div style="margin:0 auto;"> <img src="https://blog.llvm.org/img/Clang-Doc-new-output.png"><br/></div><p>You can also visit the output project on my github.io page link<a href="https://peterchou1.github.io/">here</a>.</p><p>Note: this output is still a work in progress.</p><h2 id="learning-insight">Learning Insight</h2><p>I&rsquo;ve learned a lot in the past few months, thanks to GSOC I now have a much better idea of what it’s like to participate in a large open-source project. I received a lot of feedback through PRs, making RFC, and collaborating with other GSOC members. I learned a lot about how to interact with the community and solicit feedback. I also learned a lot about instrumentation/profiling code having conducted many experiments to try to speed Clang-Doc up.</p><h2 id="future-work">Future Work</h2><p>As my work concluded I was named as one of the maintainers of the project. In the future I plan to work on Clang-Doc until an MVP product can be generated and evaluated for the LLVM project. My remaining tasks include landing the Mustache support library and Clang-Doc’s Mustache backend, as well as gathering feedback from the LLVM community regarding Clang-Doc’s current output. Additionally, I intend to add test cases for the Mustache HTML backend to ensure its robustness and functionality.</p><h2 id="conclusion">Conclusion</h2><p>Overall the current state of Clang-Doc is much healthier than it was before. It now has much better test coverage across all its output, markdown, html, yaml. Whereas previously there were no e2e test cases that were not as comprehensive. The tool is significantly faster especially for large scale projects like LLVM making documentation generation and development a much better experience.The tool also has a simplified HTML backend that will be much easier to work with compared to before leading to a faster velocity for development.</p><h2 id="acknowledgements">Acknowledgements</h2><p>I’d like to thank my mentors, Paul and Petr for their invaluable input when I encounter issues with the project. This year has been tough for me mentally, and I’d like to thank my mentors for being accommodating with me.</p> 

<br> 

<https://blog.llvm.org/posts/2024-12-04-improve-clang-doc/>

