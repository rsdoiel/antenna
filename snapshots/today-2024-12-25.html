<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>News gathered 2024-12-25</title>
  <style>
    body {
       padding: 1%;
       margin: 2%;
    }
    footer {
    	border-top: 0.24em solid green;
	text-align: center;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<section>
<h1 id="news-gathered-2024-12-25">News gathered 2024-12-25</h1>
<p>(date: 2024-12-25 07:09:02)</p>
<hr />
<h2
id="ayaneo-3-handheld-gaming-pc-features-module-controllers-and-up-to-ryzen-ai-9-hx-370">AYANEO
3 handheld gaming PC features module controllers and up to Ryzen AI 9 HX
370</h2>
<p>date: 2024-12-25, from: Liliputing</p>
<p>
The AYANEO 3 is an upcoming handheld gaming PC with a 7 inch display
that looks a lot like previous-generation devices from AYANEO at first
glance. But this all-new model has a few significant upgrades. First, it
will be available in several configurations, including entry-level
models with 7 inch IPS LCD displays and AMD Ryzen 7 8840U […]
</p>
<p>
The post
<a href="https://liliputing.com/ayaneo-3-handheld-gaming-pc-will-be-available-with-rdna-3-5-graphics-and-oled-display-options/">AYANEO
3 handheld gaming PC features module controllers and up to Ryzen AI 9 HX
370</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/ayaneo-3-handheld-gaming-pc-will-be-available-with-rdna-3-5-graphics-and-oled-display-options/"
class="uri">https://liliputing.com/ayaneo-3-handheld-gaming-pc-will-be-available-with-rdna-3-5-graphics-and-oled-display-options/</a></p>
<hr />
<h2 id="podcast-why-we-cover-ai-the-way-we-do">Podcast: Why We Cover AI
the Way We Do</h2>
<p>date: 2024-12-25, from: 404 Media Group</p>
<p>Here’s a holiday gift: an episode that was previously only available
to paying subscribers. Make sure to sign up for tons more bonus content
like this.</p>
<p><br></p>
<p><a
href="https://www.404media.co/podcast-why-we-cover-ai-the-way-we-do/"
class="uri">https://www.404media.co/podcast-why-we-cover-ai-the-way-we-do/</a></p>
<hr />
<h2
id="nothing-is-sacred-ai-generated-slop-has-come-for-christmas-music">Nothing
Is Sacred: AI Generated Slop Has Come for Christmas Music</h2>
<p>date: 2024-12-25, from: 404 Media Group</p>
<p>AI-generated Christmas classics that dwell in the uncanny valley are
giving listeners the creeps.</p>
<p><br></p>
<p><a href="https://www.404media.co/ai-generated-christmas-music/"
class="uri">https://www.404media.co/ai-generated-christmas-music/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-25, from: Dave Winer’s linkblog)</p>
<p>Toyota is donating $1 million to Trump's inauguration.</p>
<p><br></p>
<p><a
href="https://www.reuters.com/world/us/toyota-is-donating-1-million-trumps-inauguration-2024-12-24/"
class="uri">https://www.reuters.com/world/us/toyota-is-donating-1-million-trumps-inauguration-2024-12-24/</a></p>
<hr />
<h2 id="section"></h2>
<pre><code>            Advent of OS/2
        </code></pre>
<p>date: 2024-12-25, updated: 2024-12-25, from: Uninformative blog</p>
<p><br></p>
<p><a
href="https://www.uninformativ.de/blog/postings/2024-12-25/0/POSTING-en.html"
class="uri">https://www.uninformativ.de/blog/postings/2024-12-25/0/POSTING-en.html</a></p>
<hr />
<h2 id="did-you-dream-of-a-raspberry-pi-christmas">Did you dream of a
Raspberry Pi Christmas?</h2>
<p>date: 2024-12-25, from: Raspberry Pi News (.com)</p>
<p>
Did you unwrap something from Raspberry Pi for Christmas? This should
help you get started with your shiny new device.
</p>
<p>
The post
<a href="https://www.raspberrypi.com/news/did-you-dream-of-a-raspberry-pi-christmas/">Did
you dream of a Raspberry Pi Christmas?</a> appeared first on
<a href="https://www.raspberrypi.com">Raspberry Pi</a>.
</p>
<p><br></p>
<p><a
href="https://www.raspberrypi.com/news/did-you-dream-of-a-raspberry-pi-christmas/"
class="uri">https://www.raspberrypi.com/news/did-you-dream-of-a-raspberry-pi-christmas/</a></p>
<hr />
<h2 id="updates-2024q4">Updates 2024/Q4</h2>
<p>date: 2024-12-25, from: mrusme blog</p>
<p>Project updates from the current consecutive three-month period, with
info on the current status of my projects and next steps. You might find
this interesting in case you’re using any of my open source tools.</p>
<p><br></p>
<p><a href="https://xn--gckvb8fzb.com/updates-2024-q4/"
class="uri">https://xn--gckvb8fzb.com/updates-2024-q4/</a></p>
<hr />
<h2 id="a-cryptographically-secret-santa">A Cryptographically Secret
Santa</h2>
<p>date: 2024-12-25, from: Specbranch blog</p>
<p>
Twas about 4-6 weeks before Christmas, and all through the math
department, not a creature was stirring, not even a plucky young
undergrad. Cryptography professors Alice and Bob sat at the
elliptically-curved conference table to plan the department’s secret
Santa. Mallory, the department secretary, had been given the task of
organizing last year, and somehow managed to get three gifts while
leaving several people disappointed. This year’s math department thus
resolved to do their secret Santa without a trusted party.
</p>
<p><br></p>
<p><a href="http://localhost:1313/posts/cryptographic-santa/"
class="uri">http://localhost:1313/posts/cryptographic-santa/</a></p>
<hr />
<h2 id="trying-out-qvq---qwens-new-visual-reasoning-model">Trying out
QvQ - Qwen’s new visual reasoning model</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s
Weblog</p>
<p>
I thought we were done for major model releases in 2024, but apparently
not: Alibaba’s Qwen team just dropped the Apache 2.0 licensed
QvQ-72B-Preview, “an experimental research model focusing on enhancing
visual reasoning capabilities”.
</p>
<p>
Their blog post is titled
<a href="https://qwenlm.github.io/blog/qvq-72b-preview/">QvQ: To See the
World with Wisdom</a> - similar flowery language to their QwQ
announcement
<a href="https://qwenlm.github.io/blog/qwq-32b-preview/">QwQ: Reflect
Deeply on the Boundaries of the Unknown</a> a few weeks ago in November.
</p>
<p>
It’s a vision-focused follow-up to QwQ, which I
<a href="https://simonwillison.net/2024/Nov/27/qwq/">wrote about
previousy</a>. QwQ is an impressive openly licensed inference-scaling
model: give it a prompt and it will think out loud over many tokens
while trying to derive a good answer, similar to OpenAI’s o1 and o3
models.
</p>
<p>
The new QvQ adds vision to the mix. You can try it out
<a href="https://huggingface.co/spaces/Qwen/QVQ-72B-preview">on Hugging
Face Spaces</a> - it accepts an image and a single prompt and then
streams out a <em>very</em> long response where it thinks through the
problem you have posed it. There’s no option to send a follow-up prompt.
</p>
<p>
I’ve tried it out with a bunch of things, with mixed results - but it’s
really fun seeing how it works through a problem.
</p>
<p>
My most successful prompt was “Count the pelicans” with this image:
</p>
<p>
<img src="https://static.simonwillison.net/static/2024/count-pelicans-easy.jpg" alt="A photograph of four glorious young California Brown Pelicans perched on a dock. The back end of another pelican is visible at the front of the photo." style="max-width: 100%;" />
</p>
<p>
Here’s
<a href="https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response">the
full response</a>. Some highlights:
</p>
<blockquote>
<p>
So I’ve got this picture of some pelicans, and I need to count them. Let
me see… First off, I know that pelicans are those big birds with the
massive beaks, right? […]
</p>
<p>
Let me try counting again, more carefully this time. Starting from the
left:
</p>
<ol>
<li>
The first pelican is fully visible, standing at the front.
</li>
<li>
Behind it, slightly to the right, is the second pelican.
</li>
<li>
Then, further back, there’s a third pelican.
</li>
<li>
And next to that one, there’s a fourth pelican.
</li>
</ol>
<p>
So, that’s four pelicans in total. But, I notice that on the far right
of the image, there’s part of another bird, maybe a pelican’s back or
tail feathers, but it’s not fully in the frame, so I don’t think I
should count it as a whole pelican.
</p>
</blockquote>
<p>
Something about the tone of voice it uses, “First off, I know that
pelicans are those big birds with the massive beaks, right?”, tickles
me.
</p>
<p>
I also tried this on a
<a href="https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#image-2">much
harder image</a> - it counted 17 which looks about right to me but
honestly I have trouble with that image myself.
</p>
<p>
I fed in
<a href="https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#arc-agi-puzzle-0d87d2a6">one
of the ARC-AGI puzzles</a> that o3 had failed at.
</p>
<p>
<img alt="A puzzle with colored squares, where drawing a line between the single blue squares and turning any intersected rectangles blue is clearly the solution." src="https://static.simonwillison.net/static/2024/arc-agi-task-0d87d2a6.png" style="max-width: 100%;" />
</p>
<p>
It produced
<a href="https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response-1">a
very lengthy chain of thought</a> that was almost entirely incorrect,
but had some interesting “ideas” in it:
</p>
<blockquote>
<p>
[…] Let me try to think in terms of cellular automata rules, like
Conway’s Game of Life.
</p>
<p>
In Game of Life, each cell’s state in the next generation is determined
by its current state and the states of its eight neighbors.
</p>
<p>
Perhaps a similar rule applies here.
</p>
</blockquote>
<p>
Finally, I asked it to “Estimate the height of the dinosaur” against
this image (which, as it correctly noted, is actually an inflatable
dragon):
</p>
<p>
<img src="https://static.simonwillison.net/static/2024/inflatable-dragon.jpg" alt="A large inflatable Christmas decoration of a reindeer riding a dragon - both reindeer and dragon are wearing Christmas hats" style="max-width: 100%;" />
</p>
<p>
I’m not sure this task is even possible given the lack of items in the
frame of comparable known height. It
<a href="https://gist.github.com/simonw/6c296f4b9323736dc77978447b6368fc#response-3">had
a good go at it</a> though! I particularly enjoyed how it began:
</p>
<blockquote>
<p>
So I’ve got this inflatable Christmas decoration, and it’s a dragon! But
not just any dragon—it’s a green dragon wearing a Santa hat and a red
and white scarf. And get this, there’s a reindeer on its back, also
wearing a Santa hat and holding onto the dragon’s wings. It’s pretty
festive and colorful, with lots of vibrant greens, reds, and yellows.
</p>
<p>
The dragon has these big, friendly eyes and a smiling face, which makes
it look really cheerful. Its wings are spread out, and it has white
claws on its feet. The reindeer is brown with antlers and seems to be
enjoying the ride on the dragon’s back.
</p>
<p>
Now, the question is, how tall is this inflatable dragon? I need to
estimate its height. Let me see. I’m standing next to it, and I’m about
5 feet 10 inches tall. The dragon is definitely taller than me. Maybe
around 8 or 9 feet high? But I should try to be more precise.
</p>
</blockquote>
<p>
I wonder how it decided that its own height was 5 feet 10 inches!
</p>
<h4 id="running-qvq-locally">
Running QvQ locally
</h4>
<p>
All of my experiments so far have used the
<a href="https://huggingface.co/spaces/Qwen/QVQ-72B-preview">hosted
Hugging Face demo</a>. I’m hoping to try it out on my own laptop soon -
I think it should just fit in 64GB of M2, maybe with a smaller
quantization.
</p>
<p>
Right now the options are:
</p>
<ul>
<li>
<a href="https://huggingface.co/Qwen/QVQ-72B-Preview">Qwen/QVQ-72B-Preview</a>
on Hugging Face has the GPU model weights, for use with Hugging Face
Transformers and the
<a href="https://pypi.org/project/qwen-vl-utils/">qwen-vl-utils</a>
Python package.
</li>
<li>
Prince Canuma
<a href="https://twitter.com/Prince_Canuma/status/1871687356815716672">converted
the model for Apple’s MLX framework</a> as
<a href="https://huggingface.co/collections/mlx-community/qvq-72b-preview-676b345a6f93172ba980c0d5">mlx-community/qvq-72b-preview</a>
and got it working with his excellent
<a href="https://github.com/Blaizzy/mlx-vlm">mlx-vlm</a> package.
</li>
</ul>
<p>
As a happy user of Ollama’s <a href="https://ollama.com/library/qwq">qwq
port</a> I’m hoping they add a QvQ release at some point soon as well.
</p>
<h4 id="with-mlx-vlm">
Running QvQ on my Mac using mlx-vlm
</h4>
<p>
<strong>Update</strong>: I successfully ran QvQ on my 64GB M2 MacBook
Pro using Prince Canuma’s <code>mlx-vlm</code> via <code>uv run </code>
with the following command:
</p>
<div class="highlight highlight-source-shell">
<pre>uv run --with <span class="pl-s"><span class="pl-pds">'</span>numpy&lt;2.0<span class="pl-pds">'</span></span> --with mlx-vlm python \
  -m mlx_vlm.generate \
    --model mlx-community/QVQ-72B-Preview-4bit \
    --max-tokens 10000 \
    --temp 0.0 \
    --prompt <span class="pl-s"><span class="pl-pds">"</span>describe this<span class="pl-pds">"</span></span> \
    --image pelicans-on-bicycles-veo2.jpg</pre>
</div>
<p>
My image input here was taken from my experiment with Google’s
<a href="https://simonwillison.net/2024/Dec/16/veo-2/">Veo 2</a> video
generation model:
</p>
<p>
<img src="https://static.simonwillison.net/static/2024/pelicans-on-bicycles-veo2.jpg" alt="A screenshot of the Google VideoFX video generation tool" style="max-width: 100%;" />
</p>
<p>
Here’s the full output from running that command, after it first
downloaded 38GB to
<code>~/.cache/huggingface/hub/models–mlx-community–QVQ-72B-Preview-4bi</code>:
</p>
<pre><code>==========
Image: ['pelicans-on-bicycles-veo2.jpg'] 

Prompt: &lt;|im_start|&gt;system
You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
describe this&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant

Alright, I've got this "Text to Video" tool to explore. It seems pretty advanced, allowing me to turn text descriptions into actual videos. The interface looks user-friendly, with a dark theme that's easy on the eyes. On the left side, there's a panel where I can input my text prompt. It already has an example filled in: "A pelican riding a bicycle along a coastal path overlooking a harbor."

That sounds interesting! I can see four different video thumbnails on the right side, each showing variations of this scene. One of them even has the pelican wearing a helmet, which adds a humorous touch. Another shows the pelican standing next to a bicycle, almost as if it's ready to hop on and start pedaling.

Below the text input, there are some style options like "photorealistic," "35mm film," "minimal," "sketchy," "handmade," and "abstract." These probably let me choose the visual style of the video. For now, I'll stick with the default settings.

At the bottom of the left panel, there are two buttons: "Start over" and "Create videos." I guess "Create videos" is the one I need to press to generate the videos based on my text prompt.

Let me think about this. I want to see what kind of videos this tool can produce based on my description. The example prompt is already filled in, so maybe I should just go ahead and click "Create videos" to see what happens.

Wait a minute, the prompt mentions a coastal path overlooking a harbor. In the thumbnails, I can see a background with water and boats, which matches the description. It's cool how the tool has interpreted the text into visual elements.

I should also pay attention to the "Visual Prompt" section, which says "View only." That probably means I can't edit it directly, but I can view the generated videos based on this prompt.

Hmm, maybe I should try changing the prompt to see different results. For example, instead of a pelican riding a bicycle, maybe I could describe a different scene. But for now, I'll stick with the given example to understand how it works.

After clicking "Create videos," I expect to see the videos being generated. There might be a loading process, depending on the complexity of the prompt and the tool's processing speed.

Once the videos are ready, I can preview them to see how well they match my description. If I'm not satisfied, I can always go back and adjust the prompt or try different style options.

Overall, this seems like a powerful tool for creating custom videos based on textual descriptions. It could be really useful for storytelling, presentations, or even just for fun, experimenting with different scenes and styles.

I think I'll spend some time exploring the different style options to see how they affect the final videos. For instance, selecting "35mm film" might give the videos a vintage, cinematic look, while "abstract" could produce more artistic and less realistic visuals.

Also, I wonder if there's a way to customize more specific aspects, like the camera angle, lighting, or movement. Maybe after generating the initial videos, there are additional editing options available.

In summary, I'm excited to use this Text to Video tool and see what kind of creative possibilities it opens up. With a bit of practice, I could probably create some impressive visual content just by describing what I want to see.

**Final Answer**

To proceed with generating videos based on the provided text prompt, I need to click on the "Create videos" button. This action will initiate the process of converting the textual description into visual content, resulting in multiple video thumbnails that can be previewed for further use or adjustments.
==========
Prompt: 0.870 tokens-per-sec
Generation: 7.694 tokens-per-sec
</code></pre>
<pre><code>    &lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/python&quot;&gt;python&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/hugging-face&quot;&gt;hugging-face&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/vision-llms&quot;&gt;vision-llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/uv&quot;&gt;uv&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/qwen&quot;&gt;qwen&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/mlx&quot;&gt;mlx&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/inference-scaling&quot;&gt;inference-scaling&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a href="https://simonwillison.net/2024/Dec/24/qvq/#atom-everything"
class="uri">https://simonwillison.net/2024/Dec/24/qvq/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Insurers Are Dropping Homeowners as Climate Shocks Worsen.</p>
<p><br></p>
<p><a
href="https://www.nytimes.com/interactive/2024/12/18/climate/insurance-non-renewal-climate-crisis.html?unlocked_article_code=1.j04.2fww.8GfR7kiRvGwU&amp;smid=url-share"
class="uri">https://www.nytimes.com/interactive/2024/12/18/climate/insurance-non-renewal-climate-crisis.html?unlocked_article_code=1.j04.2fww.8GfR7kiRvGwU&amp;smid=url-share</a></p>
<hr />
<h2
id="gmk-nucbox-k11-mini-pc-features-ryzen-9-8945hs-oculink-2.5-gbe-lan-dual-fans-and-rgb-lighting">GMK
NucBox K11 mini PC features Ryzen 9 8945HS, OCuLink, 2.5 GbE LAN,
dual-fans, and RGB lighting</h2>
<p>date: 2024-12-24, from: Liliputing</p>
<p>
The GMK NucBox K11 is a 5.2″ x 5″ x 2.3″ desktop computer with an
unusual design feature: a clear panel on the top of the case that
reveals a large fan with RGB lighting effects. But while the RGB LED
lights are purely for looks, the fan is one of two inside the computer
that […]
</p>
<p>
The post
<a href="https://liliputing.com/gmk-nucbox-k11-mini-pc-features-ryzen-9-8945hs-oculink-2-5-gbe-lan-dual-fans-and-rgb-lighting/">GMK
NucBox K11 mini PC features Ryzen 9 8945HS, OCuLink, 2.5 GbE LAN,
dual-fans, and RGB lighting</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/gmk-nucbox-k11-mini-pc-features-ryzen-9-8945hs-oculink-2-5-gbe-lan-dual-fans-and-rgb-lighting/"
class="uri">https://liliputing.com/gmk-nucbox-k11-mini-pc-features-ryzen-9-8945hs-oculink-2-5-gbe-lan-dual-fans-and-rgb-lighting/</a></p>
<hr />
<h2
id="minisforum-mdsa156-is-a-231-portable-dual-screen-monitor">MINISFORUM
MDSA156 is a $231 portable dual-screen monitor</h2>
<p>date: 2024-12-24, from: Liliputing</p>
<p>
MINISFORUM is probably best known for making min PCs, but one of the
company’s latest products is a display… or rather two displays.
The MINISFORUM MDSA156 is a portable dual-screen monitor with two
displays connected by a laptop-like hinge, allowing you to fold the
device in half for easy transportation and storage and unfold it when
you […]
</p>
<p>
The post
<a href="https://liliputing.com/minisforum-mdsa156-is-a-231-portable-dual-screen-monitor/">MINISFORUM
MDSA156 is a $231 portable dual-screen monitor</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/minisforum-mdsa156-is-a-231-portable-dual-screen-monitor/"
class="uri">https://liliputing.com/minisforum-mdsa156-is-a-231-portable-dual-screen-monitor/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Denmark boosts Greenland defence after Trump repeats desire for US
control.</p>
<p><br></p>
<p><a href="https://www.bbc.com/news/articles/ckgzl19n9eko"
class="uri">https://www.bbc.com/news/articles/ckgzl19n9eko</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>#LETSDOIT25 rules and guidelines.</p>
<p><br></p>
<p><a
href="https://docs.google.com/document/u/0/d/1LLR-1Q9sp14bnwMpLkX8TNytbFLrPL1-4_z6-Wf0uNQ/mobilebasic"
class="uri">https://docs.google.com/document/u/0/d/1LLR-1Q9sp14bnwMpLkX8TNytbFLrPL1-4_z6-Wf0uNQ/mobilebasic</a></p>
<hr />
<h2
id="frore-airjet-pak-provides-up-to-25-watts-of-solid-state-cooling-for-the-nvidia-jetson-orin-nano-and-similar-som-based-pcs">Frore
AirJet PAK provides up to 25 watts of solid state cooling for the NVIDIA
Jetson Orin Nano (and similar SoM-based PCs)</h2>
<p>date: 2024-12-24, from: Liliputing</p>
<p>
The NVIDIA Jetson Orin Nano Super is a $249 AI development kit that
combines an NVIDIA system-on-a-module with a carrier board to create a
palm-sized computer with up to 67 TOPS of AI performance. It’s the
latest in a line of Jetson products from NVIDIA, and as NVIDIA noted
when launching the new model earlier this month, […]
</p>
<p>
The post
<a href="https://liliputing.com/frore-airjet-pak-provides-up-to-25-watts-of-solid-state-cooling-for-the-nvidia-jetson-orin-nano-and-similar-som-based-pcs/">Frore
AirJet PAK provides up to 25 watts of solid state cooling for the NVIDIA
Jetson Orin Nano (and similar SoM-based PCs)</a> appeared first on
<a href="https://liliputing.com">Liliputing</a>.
</p>
<p><br></p>
<p><a
href="https://liliputing.com/frore-airjet-pak-provides-up-to-25-watts-of-solid-state-cooling-for-the-nvidia-jetson-orin-nano-and-similar-som-based-pcs/"
class="uri">https://liliputing.com/frore-airjet-pak-provides-up-to-25-watts-of-solid-state-cooling-for-the-nvidia-jetson-orin-nano-and-similar-som-based-pcs/</a></p>
<hr />
<h2 id="quoting-paige-bailey">Quoting Paige Bailey</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s
Weblog</p>
<blockquote cite="https://twitter.com/dynamicwebpaige/status/1871606733170442615">
<p>
it’s <em>really hard</em> not to be obsessed with these tools. It’s like
having a bespoke, free, (usually) accurate curiosity-satisfier in your
pocket, no matter where you go - if you know how to ask questions, then
suddenly the world is an audiobook
</p>
</blockquote>
<p class="cite">
—
<a href="https://twitter.com/dynamicwebpaige/status/1871606733170442615">Paige
Bailey</a>
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/gemini&quot;&gt;gemini&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/llms&quot;&gt;llms&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/generative-ai&quot;&gt;generative-ai&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2024/Dec/24/paige-bailey/#atom-everything"
class="uri">https://simonwillison.net/2024/Dec/24/paige-bailey/#atom-everything</a></p>
<hr />
<h2 id="remembering-major-alton-glenn-miller-1904-1944">Remembering
Major Alton “Glenn” Miller (1904-1944)</h2>
<p>date: 2024-12-24, from: National Archives, Text Message blog</p>
<p>Today’s post is by Rachael Salyer, Subject Matter Expert for Modern
Military Records at the National Archives in College Park, MD. On
Christmas Eve 1944, news broke that popular musician and band leader
Glenn Miller had been reported missing in the European Theater of
Operations (ETO). Miller and the Army Air Forces Band (Special) that …
<a href="https://text-message.blogs.archives.gov/2024/12/24/remembering-major-alton-glenn-miller-1904-1944/" class="more-link">Continue
reading <span class="screen-reader-text">Remembering Major Alton “Glenn”
Miller (1904-1944)</span></a></p>
<p><br></p>
<p><a
href="https://text-message.blogs.archives.gov/2024/12/24/remembering-major-alton-glenn-miller-1904-1944/"
class="uri">https://text-message.blogs.archives.gov/2024/12/24/remembering-major-alton-glenn-miller-1904-1944/</a></p>
<hr />
<h2 id="an-extremely-questionable-therefore">An Extremely Questionable
“Therefore”</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: One Foot Tsunami</p>
<p><br></p>
<p><a
href="https://onefoottsunami.com/2024/12/24/an-extremely-questionable-therefore/"
class="uri">https://onefoottsunami.com/2024/12/24/an-extremely-questionable-therefore/</a></p>
<hr />
<h2 id="full-feed-rss">Full Feed RSS</h2>
<p>date: 2024-12-24, from: Chris Coyier blog</p>
<p>With Paul Kinlan’s Full Feed RSS I can now subscribe to Khoi’s site
where the RSS entries are more than one sentence. It does make me think…
would this annoy Khoi? Does he have it that way on purpose, perhaps
because he really wants the site to be read on the site? Do I feel
[…]</p>
<p><br></p>
<p><a href="https://chriscoyier.net/2024/12/24/full-feed-rss/"
class="uri">https://chriscoyier.net/2024/12/24/full-feed-rss/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Whatever forces you into copying and pasting into tiny little text
boxes, that's how you know you're in a silo.</p>
<p><br></p>
<p><a href="http://scripting.com/2024/12/24.html#a141952"
class="uri">http://scripting.com/2024/12/24.html#a141952</a></p>
<hr />
<h2
id="tipster-arrested-after-feds-find-ai-child-exploit-images-and-plans-to-make-vr-csam">Tipster
Arrested After Feds Find AI Child Exploit Images and Plans to Make VR
CSAM</h2>
<p>date: 2024-12-24, from: 404 Media Group</p>
<p>A review of a tipster’s phone revealed ‘real’ CSAM mixed in with AI
CSAM, and a darker involvement in the case.</p>
<p><br></p>
<p><a
href="https://www.404media.co/tipster-arrested-after-feds-find-ai-child-exploit-images-and-plans-to-make-vr-csam-2/"
class="uri">https://www.404media.co/tipster-arrested-after-feds-find-ai-child-exploit-images-and-plans-to-make-vr-csam-2/</a></p>
<hr />
<h2
id="ho-ho-ho---whats-so-funny-about-an-overweight-old-man-in-a-white-beard">HO,
HO, HO - WHAT’S SO FUNNY ABOUT AN OVERWEIGHT OLD MAN IN A WHITE
BEARD?</h2>
<p>date: 2024-12-24, from: Howard Jacobson blog</p>
<p>Robert Cenedella</p>
<p><br></p>
<p><a
href="https://jacobsonh.substack.com/p/ho-ho-ho-whats-so-funny-about-an"
class="uri">https://jacobsonh.substack.com/p/ho-ho-ho-whats-so-funny-about-an</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>My favorite news sources, pros and bloggers, podcasts, culture,
business, tech, politics.</p>
<p><br></p>
<p><a href="https://news.scripting.com/"
class="uri">https://news.scripting.com/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Ars Technica picks for TV of 2024.</p>
<p><br></p>
<p><a
href="https://arstechnica.com/culture/2024/12/tv-technica-2024-our-picks-for-the-best-of-tv/"
class="uri">https://arstechnica.com/culture/2024/12/tv-technica-2024-our-picks-for-the-best-of-tv/</a></p>
<hr />
<h2 id="quoting-jeremy-edberg">Quoting Jeremy Edberg</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s
Weblog</p>
<blockquote cite="https://news.ycombinator.com/item?id=42486610#42492484">
<p>
[On Reddit] we had to look up every single comment on the page to see if
you had voted on it […]
</p>
<p>
But with a bloom filter, we could very quickly look up all the comments
and get back a list of all the ones you voted on (with a couple of false
positives in there). Then we could go to the cache and see if your
actual vote was there (and if it was an upvote or a downvote). It was
only after a failed cache hit did we have to actually go to the
database.
</p>
<p>
But that bloom filter saved us from doing sometimes 1000s of cache
lookups.
</p>
</blockquote>
<p class="cite">
—
<a href="https://news.ycombinator.com/item?id=42486610#42492484">Jeremy
Edberg</a>
</p>
<pre><code>&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/reddit&quot;&gt;reddit&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/bloom-filters&quot;&gt;bloom-filters&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/scaling&quot;&gt;scaling&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2024/Dec/24/jeremy-edberg/#atom-everything"
class="uri">https://simonwillison.net/2024/Dec/24/jeremy-edberg/#atom-everything</a></p>
<hr />
<h2 id="finally-a-replacement-for-bert-introducing-modernbert">Finally,
a replacement for BERT: Introducing ModernBERT</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: Simon Willison’s
Weblog</p>
<p>
<strong><a href="https://www.answer.ai/posts/2024-12-19-modernbert.html">Finally,
a replacement for BERT: Introducing ModernBERT</a></strong>
</p>
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>
was an early language model released by Google in October 2018. Unlike
modern LLMs it wasn’t designed for generating text. BERT was trained for
masked token prediction and was generally applied to problems like Named
Entity Recognition or Sentiment Analysis. BERT also wasn’t very useful
on its own - most applications required you to fine-tune a model on top
of it.
</p>
<p>
In exploring BERT I decided to try out
<a href="https://huggingface.co/dslim/distilbert-NER">dslim/distilbert-NER</a>,
a popular Named Entity Recognition model fine-tuned on top of DistilBERT
(a smaller distilled version of the original BERT model).
<a href="https://til.simonwillison.net/llms/bert-ner">Here are my
notes</a> on running that using <code>uv run</code>.
</p>
<p>
Jeremy Howard’s <a href="https://www.answer.ai/">Answer.AI</a> research
group, <a href="https://www.lighton.ai/">LightOn</a> and friends
supported the development of ModernBERT, a brand new BERT-style model
that applies many enhancements from the past six years of advances in
this space.
</p>
<p>
While BERT was trained on 3.3 billion tokens, producing 110 million and
340 million parameter models, ModernBERT trained on 2 trillion tokens,
resulting in 140 million and 395 million parameter models. The parameter
count hasn’t increased much because it’s designed to run on lower-end
hardware. It has a 8192 token context length, a significant improvement
on BERT’s 512.
</p>
<p>
I was able to run one of the demos from the announcement post using
<code>uv run</code> like this (I’m not sure why I had to use
<code>numpy&lt;2.0</code> but without that I got an error about
<code>cannot import name ‘ComplexWarning’ from
‘numpy.core.numeric’</code>):
</p>
<div class="highlight highlight-source-shell">
<pre>uv run --with <span class="pl-s"><span class="pl-pds">'</span>numpy&lt;2.0<span class="pl-pds">'</span></span> --with torch --with <span class="pl-s"><span class="pl-pds">'</span>git+https://github.com/huggingface/transformers.git<span class="pl-pds">'</span></span> python</pre>
</div>
<p>
Then this Python:
</p>
<pre><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-s1">pipeline</span>
<span class="pl-k">from</span> <span class="pl-s1">pprint</span> <span class="pl-k">import</span> <span class="pl-s1">pprint</span>
<span class="pl-s1">pipe</span> <span class="pl-c1">=</span> <span class="pl-en">pipeline</span>(
    <span class="pl-s">"fill-mask"</span>,
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s">"answerdotai/ModernBERT-base"</span>,
    <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">bfloat16</span>,
)
<span class="pl-s1">input_text</span> <span class="pl-c1">=</span> <span class="pl-s">"He walked to the [MASK]."</span>
<span class="pl-s1">results</span> <span class="pl-c1">=</span> <span class="pl-en">pipe</span>(<span class="pl-s1">input_text</span>)
<span class="pl-en">pprint</span>(<span class="pl-s1">results</span>)</pre>
<p>
Which downloaded 573MB to
<code>~/.cache/huggingface/hub/models–answerdotai–ModernBERT-base</code>
and output:
</p>
<pre>[{<span class="pl-s">'score'</span>: <span class="pl-c1">0.11669921875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the door.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3369</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' door'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.037841796875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the office.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3906</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' office'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.0277099609375</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the library.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">6335</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' library'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.0216064453125</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the gate.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">7394</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' gate'</span>},
 {<span class="pl-s">'score'</span>: <span class="pl-c1">0.020263671875</span>,
  <span class="pl-s">'sequence'</span>: <span class="pl-s">'He walked to the window.'</span>,
  <span class="pl-s">'token'</span>: <span class="pl-c1">3497</span>,
  <span class="pl-s">'token_str'</span>: <span class="pl-s">' window'</span>}]</pre>
<p>
<p>I’m looking forward to trying out models that use ModernBERT as their
base. The model release is accompanied by a paper
(<a href="https://arxiv.org/abs/2412.13663">Smarter, Better, Faster,
Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and
Long Context Finetuning and Inference</a>) and
<a href="https://huggingface.co/docs/transformers/main/en/model_doc/modernbert">new
documentation</a> for using it with the Transformers library.</p>
<pre><code>&lt;p&gt;&lt;small&gt;&lt;/small&gt;Via &lt;a href=&quot;https://bsky.app/profile/benjaminwarner.dev/post/3ldur45oz322b&quot;&gt;@benjaminwarner.dev&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;


&lt;p&gt;Tags: &lt;a href=&quot;https://simonwillison.net/tags/ai&quot;&gt;ai&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/jeremy-howard&quot;&gt;jeremy-howard&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/transformers&quot;&gt;transformers&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/hugging-face&quot;&gt;hugging-face&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/uv&quot;&gt;uv&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/python&quot;&gt;python&lt;/a&gt;, &lt;a href=&quot;https://simonwillison.net/tags/nlp&quot;&gt;nlp&lt;/a&gt;&lt;/p&gt; </code></pre>
<p><br></p>
<p><a
href="https://simonwillison.net/2024/Dec/24/modernbert/#atom-everything"
class="uri">https://simonwillison.net/2024/Dec/24/modernbert/#atom-everything</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Tesla's 2024 deliveries growth might hinge on Musk's unorthodox
Cybertruck.</p>
<p><br></p>
<p><a
href="https://www.reuters.com/business/autos-transportation/teslas-2024-deliveries-growth-might-hinge-musks-unorthodox-cybertruck-2024-12-20/"
class="uri">https://www.reuters.com/business/autos-transportation/teslas-2024-deliveries-growth-might-hinge-musks-unorthodox-cybertruck-2024-12-20/</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Trump's Plan to Make America a Global Bully.</p>
<p><br></p>
<p><a
href="https://www.theatlantic.com/magazine/archive/2025/01/trump-foreign-policy-isolation/680754/?gift=f35zZN0v_gDFE8xNwlQAHQ8w2Hd1-0-zPQQVqe3_z0Y"
class="uri">https://www.theatlantic.com/magazine/archive/2025/01/trump-foreign-policy-isolation/680754/?gift=f35zZN0v_gDFE8xNwlQAHQ8w2Hd1-0-zPQQVqe3_z0Y</a></p>
<hr />
<p><strong><span class="citation" data-cites="Dave">@Dave</span> Winer’s
linkblog</strong> (date: 2024-12-24, from: Dave Winer’s linkblog)</p>
<p>Interesting way to sell an E-car. 🤪</p>
<p><br></p>
<p><a href="https://m.youtube.com/watch?v=LxtJ5yi_yps"
class="uri">https://m.youtube.com/watch?v=LxtJ5yi_yps</a></p>
<hr />
<h2 id="explainer-latent-space-experts">Explainer: Latent Space
Experts</h2>
<p>date: 2024-12-24, updated: 2024-12-24, from: Tom Kellog blog</p>
<p>A new paper just dropped from Google DeepMind, Deliberation in Latent
Space via Differentiable Cache Augmentation. I don’t think this paper is
very readable, but it also seems quite important so I wanted to take a
moment to break it down, as I understand it.</p>
<p><br></p>
<p><a href="http://timkellogg.me/blog/2024/12/24/latent-experts"
class="uri">http://timkellogg.me/blog/2024/12/24/latent-experts</a></p>
</section>
<footer>Feed items based on the feeds identified in <a href="news.txt">news.txt</a> harvested
with <a href="https://github.com/rsdoiel/skimmer">skimmer</a></footer>
</body>
</html>
